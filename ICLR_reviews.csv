sentence,lable,id
"This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.This is a very interesting and fairly easy to read paper.  The authors present a small, yet nifty approach to make Neural Programming Interpreters significantly more powerful. By allowing recursion, NPI generalizes better from fewer execution traces. It's an interesting example of how a small but non-trivial extension can make a machine learning method significantly more practical.  I also appreciate that the same notation was used in this paper and the original Deepmind paper. As a non-expert on this topic, it was easy to read the original paper in tandem.   My one point of critique is that the generalization proves are a bit vague. For the numerical examples in the paper, you can iterate over all possible execution paths until the next recursive call. However, how would this approach generalize a continuous input space (e.g. the 3D car example in the original paper). It seems that a prove of generalization will still be intractable in the continuous case?   Are you planning on releasing the source code?﻿This paper argues that being able to handle recursion is very important for neural programming architectures — that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data.  Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces — this paper trains NPI models on traces that have recursive calls.  The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems.    What I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see.  I’m actually not sure what the right take-away is — does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?).    For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest.  In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic — and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols.  What is a problem where we have access to execution traces but cannot infer it using the proposed method? ",1,304
"This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of ""rate"" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.  The authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific ""form"" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q. This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding. In addition, the paper discusses the approach’s relationship to variational autoencoders.  Given that the approach to rate-distortion optimization has already been published, the novelty of this submission is arguably not very high (correct me if I missed a new trick). In some ways, this paper even represents a step backward, since earlier work optimized for a perceptual metric where here MSE is used. However, the results are a visible improvement over JPEG 2000, and I don’t know of any other learned encoding which has been shown to achieve this level of performance. The paper is very well written.  Equation 10 appears to be wrong and I believe the partition function should depend on g_s(y; theta). This would mean that the approach is not equivalent to a VAE for non-Euclidean metrics.  What was the reason for optimizing MSE rather than a perceptual metric as in previous work? Given the author’s backgrounds, it is surprising that even the evaluation was only performed in terms of PSNR.  What is the contribution of adaptive entropy coding versus the effect of deeper encoders and decoders? This seems like an important piece of information, so it would be interesting to see the performance without adaptation as in the previous paper. More detail on the adaptive coder and its effects should be provided, and I will be happy to give a higher score when the authors do.This is the most convincing paper on image compression with deep neural networks that I have read so far. The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework. The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG). I would expect this paper to have a very good impact.   Yes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions. I think it is important to clearly state how NN compare to best previous methods. From the submitted version, I still don't know how both categories of methods are positioned. ",0,305
"In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.  -----  This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each ""example"" includes a sequence of batches of ""training"" pairs, followed by a final ""test"" batch. The inputs at each ""step"" include the outputs of a ""base learner"" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final ""test"" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.  Strengths: - It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning. - The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments. - The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work. - As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless. - The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.  Weaknesses: - The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience. - Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question). - The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.  This is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.  Pros:  - An interesting and feasible approach to meta-learning - Competitive results and proper comparison to state-of-the-art - Good recommendations for practical systems  Cons:  - The analogy would be closer to GRUs than LSTMs - The description of the data separation in meta sets is hard to follow and could be visualized - The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest - Fig 2 doesn't have much value  Remarks:  - Small typo in 3.2: ""This means each coordinate has it"" -> its  > We plan on releasing the code used in our evaluation experiments.  This would certainly be a major plus.This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN). The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing.   Several tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated.  The experiments are convincing. This is a strong paper. My only concerns/questions are the following:  1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough. 2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this? 3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:      - Samy Bengio PhD thesis (1989) is all about this ;-)      - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)      - I am convince Schmidhuber has done something, make sure you find it and update related work section.    Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.   ",1,306
"This paper presents a new, public dataset and tasks for goal-oriented dialogue applications. The dataset and tasks are constructed artificially using rule-based programs, in such a way that different aspects of dialogue system performance can be evaluated ranging from issuing API calls to displaying options, as well as full-fledged dialogue.  This is a welcome contribution to the dialogue literature, which will help facilitate future research into developing and understanding dialogue systems. Still, there are pitfalls in taking this approach. First, it is not clear how suitable Deep Learning models are for these tasks compared to traditional methods (rule-based systems or shallow models), since Deep Learning models are known to require many training examples and therefore performance difference between different neural networks may simply boil down to regularization techniques. The tasks 1-5 are also completely deterministic, which means evaluating performance on these tasks won't measure the ability of the models to handle noisy and ambiguous interactions (e.g. inferring a distribution over user goals, or executing dialogue repair strategies), which is a very important aspect in dialogue applications. Overall, I still believe this is an interesting direction to explore.  As discussed in the comments below, the paper does not have any baseline model with word order information. I think this is a strong weakness of the paper, because it makes the neural networks appear unreasonably strong, yet simpler baselines could very likely be be competitive (or better) than the proposed neural networks. To maintain a fair evaluation and correctly assess the power of representation learning for this task, I think it's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information. This would more convincly demonstrate the utility of Deep Learning models for this task. For example, the one could experiment with a logistic regression model which takes as input 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features. If such a baseline is included, I will increase my rating to 8.    Final minor comment: in the conclusion, the paper states ""the existing work has no well defined measures of performances"". This is not really true. End-to-end trainable models for task-oriented dialogue have well-defined performance measures. See, for example ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al. On the other hand, non-goal-oriented dialogue are generally harder to evaluate, but given human subjects these can also be evaluated. In fact, this is what Liu et al (2016) do for Twitter. See also, ""Strategy and Policy Learning for Non-Task-Oriented Conversational Systems"" by Yu et al.  ----  I've updated my score following the new results added in the paper.SYNOPSIS: This paper introduces a new dataset for evaluating end-to-end goal-oriented dialog systems.  All data is generated in the restaurant setting, where the goal is to find availability and eventually book a table based on parameters provided by the user to the bot as part of a dialog.  Data is generated by running a simulation using an underlying knowledge base to generate samples for the different parameters (cuisine, price range, etc), and then applying rule-based transformations to render natural language descriptions. The objective is to rank a set of candidate responses for each next turn of the dialog, and evaluation is reported in terms of per-response accuracy and per-dialog accuracy. The authors show that Memory Networks are able to improve over basic bag-of-words baselines.  THOUGHTS: I want to thank the authors for an interesting contribution.  Having said that, I am skeptical about the utility of end-to-end trained systems in the narrow-domain setting. In the open-domain setting, there is a strong argument to be made that hand-coding all states and responses would not scale, and hence end-to-end trained methods make a lot of sense. However, in the narrow-domain setting, we usually know and understand the domain quite well, and the goal is to obtain high user satisfaction. Doesn't it then make sense in these cases to use the domain knowledge to engineer the best system possible?  Given that the domain is already restricted, I'm also a bit disappointed that the goal is to RANK instead of GENERATE responses, although I understand that this makes evaluation much easier. I'm also unsure how these candidate responses would actually be obtained in practice? It seems that the models rank the set of all responses in train/val/test (last sentence before Sec 3.2). Since a key argument for the end-to-end training approach is ease of scaling to new domains without having to manually re-engineer the system, where is this information obtained for a new domain in practice?  Generating responses would allow much better generalization to new domains, as opposed to simply ranking some list of hand-collected generic responses, and in my mind this is the weakest part of this work.  Finally, as data is generated using a simulation by expanding (cuisine, price, ...) tuples using NL-generation rules, it necessarily constrains the variability in the training responses. Of course, this is traded off with the ability to generate unlimited data using the simulator. But I was unable to see the list of rules that was used. It would be good to publish this as well.  Overall, despite my skepticism, I think it is an interesting contribution worthy of publication at the conference.   ------  I've updated my score following the clarifications and new results.Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of  high significance.  Two important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper: 1)	The performance of end2end ML approaches is still insufficient for goal oriented dialogs. 2)	When comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions.  While its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops.  While this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than  for revision.  First they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact.  Would the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one.  For the concierge data, what would happen if ‘correct’ meant being the best, not among the 5-best?   While I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator  Another issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code ‘Indian’ as ‘type of cuisine’, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them. ",0,307
"SUMMARY  This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach.   PROS  Theory, good questions, nice answers.  Makes an interesting use of concepts form analysis and differential topology.  Proposes avenues to avoid instability in GANs.   CONS  A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work).   MINOR COMMENTS  - Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix.   - Section 3 provides a nice, intuitive, simple solution.   - On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero.   - On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.    - Lemma 1 would also hold in more generality.   - Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof).   - In Theorem 2.4, it would be good to remind the reader about p(z).   - Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof).  Specify the domain of the random variables.   - relly - > rely   - Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)  - Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3.   - Theorem 2.5 ``Therefore'' -> `Then'?   - Theorem 2.6 ``Is a... '' -> `is a' ?   - The number of the theorems is confusing.  This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor. This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure.   With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction.   I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison? ",1,308
"This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features. They propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance. Such agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations.   This work contrasts with traditional ""passive"" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.  To me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the ""possibility"" of control that an agent has over the environment. The proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL. The methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found. I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.  This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy's optimization problem with terms corresponding to (domain-independent) auxiliary tasks. These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network). Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.  The paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance. At high level I only have few things to say, none being of major concern: - I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components? - If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent ""also maximises many other pseudo-reward functions simultaneously by reinforcement learning"", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1. - The ""feature control"" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here. - Since as you mentioned, ""the performance of our agents is still steadily improving"", why not keep them going to see how far they go? (at least the best ones) - Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice? - Please mention the fact that auxiliary tasks are not trained with ""true"" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)  Minor stuff: - ""Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -..."" => that's actually a loss to be minimized - In eq. 1 lambda_c should be within the sum - Just below eq. 1 r_t^(c) should be r_t+k^(c) - Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3 - ""the features discovered in this manner is shared"" => are shared - The text around eq. 2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq. 2 - Please explain what ""Clip"" means for dueling networks in the legend of Figure 3 - I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed - In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text. - In 4.1.2: ""Figure 3 (right) shows..."" => it is actually the top left plot of the figure. Also later ""This is shown in Figure 3 Top"" should be Figure 3 Top Right. - ""Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels"" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing) - In 4.2: ""The left side shows the average performance curves of the top 5 agents for all three methods the right half shows..."" => missing a comma or something after ""methods"" - Appendix: ""Further details are included in the supplementary materials."" => where are they? - What is the value of lambda_PC? (=1 I guess?)  [Edit] I know some of my questions were already answered in Comments, no need to re-answer themThis paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the conference. However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping. This paper is a proof of concept for this idea.  The paper is well written and easy to follow by any reader with deep RL expertise.  Can authors comment about the computational resources needed to train the UNREAL agent?  The overall architecture is quite complicated. Are the authors willing to release the source code for their model?  -------------------------------------------------------- After rebuttal: No change in the review.",1,309
"In this paper, a referential game is proposed between two agents. Both agents observe two images. The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target. The agents both get a reward, if the receiver agent can predict the target. The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE. In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts. In addition to this, the paper presents experiments on a variant of the game grounded on different image classes. In this setting, the agents appear to learn even more meaningful concepts. Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images). Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.  Learning shared representations for communication in a multi-agent setup is an interesting research direction to explore. This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task. To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel. As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects. I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work. For this reason, I think the paper should be accepted.    Other comments: - How is the target (t) incorporated into the sender networks? Please clarify this. - Table 1 and Table 2 use percentage (%) values differently. In the first, percentages seem to be written in the interval [0, 100], and in the second in the interval [0, 1]. Please correct this. Perhaps related to this, in Table 1, the column ""obs-chance purity"" seems to have extremely small values. I assume this was mistake? - ""assest"" -> ""assess"" - ""usufal"" -> ""usual""To train natural language systems by putting multiple agents within an interactive referential communication game is very nice. As the authors mention, there has been some (although seemingly not much) previous work on using multi-agent games to teach communication, and it certainly seems like a direction worth pursuing. Moreover, the approach of switching between these games and some supervised learning, as in the experiment described in Section 5 and suggested in Section 6, seems particularly fruitful.   Note: For “clarity”, I believe some of the network connections in Fig 1 have been omitted. However, given the rather highly-customized architecture and the slightly hard-to-follow description in Section 3, the shorthand diagram only adds to the confusion. The diagram probably needs to be fine-tuned, but at the very least (especially if I am misunderstanding it!), a caption must [still] be added to help the reader interpret the figure.   Overall, the framework (Section 2) is great and seems quite effective/useful in various ways, the results are reasonable, and I expect there will be some interesting future variations on this work as well.  Caveat: While I am quite confident I understood the paper (as per confidence score below), I do not feel I am sufficiently familiar with the most closely related literature to accurately assess the place of this work within that context. Thank you for an interesting read.  Pros - This paper tackles a very crucial problem of understanding communications between 2 agents. As more and more applications of reinforcement learning are being explored, this approach brings us back to a basic question. Is the problem solving approach of machines similar to that of humans.  - The task is simple enough to make the post learning analysis intuitive.  - It was interesting to see how informed agents made use of multiple symbols to transmit the message, where as agnostic agents relied only on 2 symbols.   Cons - The task effectively boils down to image classification, if the 2 images sent are from different categories. The symbols used are effectively the image class which the second agent learns to assign to either of the images. By all means, this approach boils down to a transfer learning problem which could probably be trained much faster than a reinforcement learning algorithm.",1,310
"This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid. The pros of the paper are: 1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising. 2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.  The cons of the paper are: 1. The training time of the network is long, even with a lot of computing resources.  2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.  Overall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.This paper presents search for optimal neural-net architectures based on actor-critic framework. The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor. The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward. A auto-regressive two-layer LSTM is used as a controller and critic. The method is evaluated on two different problems, and each compared with number of other human-created architectures.  This is very exciting paper! Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are. The presented method is  novel. The authors do an excellent job of describing it in detail, with all the improvements that needed to be done. The tested data represents well the capability of the method. It is very interesting to see the differences between the generated architectures and human generated ones. The paper is written very clearly, and is very accessible. The coverage and contrast with the related literature is done well.  It would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model. It would also be interesting to see how human bootstrapped models perform and involve.  Overall, an excellent and interesting paper.This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.  The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.  This is a well written paper on an interesting topic with strong results. I recommend it be accepted.",0,312
"The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.  The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:  - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).  - There is an ablation study that supports the thesis that all the ""added complexity"" of the paper's model is useful.  Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive.   A few comments (nitpicks) on the form:  - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.  - The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).  - The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.  - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).  I think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published.This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables.   - The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract/intro  - I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past.   - Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)  Overall, I think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agentsDeep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.  The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.  There have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’ for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work.  In addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal.  The paper is well-communicated and the empirical results compelling and will be of significant interest.  Some minor potential improvements: There is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy). This should be discussed. The algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not. I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.  ",0,314
"I think that the paper is quite interesting and useful.  It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability.   Pros and Cons: Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks.   Significance: I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.  Comments: Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.    Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well",1,315
"Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.  One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.  Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.  Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.  Other comments:  Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.  G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.  Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results.   The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers.   The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.  The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.",1,316
"The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation. Results are nicely demonstrated on several datasets.  I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? Sincere apologies for the late review.  This paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested.   Summary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain.   Manuscript should be proof-read once more, there were some very few typos that may be worth fixing.The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:  1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.  2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection?  3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work.  4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size.  5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector?  Overall, this paper provides a new framework for SR with solid theoretical analysis. The idea is novel and the author explore many methods. Though there still exist questions like the necessity and more experiments are needed. I think this work will will provide good inspiration to the community.",1,317
"The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.  The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.  The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.  This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. ",1,318
"This paper proposes to investigate attention transfers between a teacher and a student network.   Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term. Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs).   They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers  does help improving the student network test performance.  However, the student networks performs worst than the teacher, even with attention.  Few remarks/questions: - in section 3 authors  claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well. - how did you choose the hyperparameter values, it would be nice to see what is the impact of $\beta$. - it would be nice to report teacher train and validation loss in Figure 7 b) - from the experiments, it is not clear what at the pros/cons of the different attention maps - AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?  In summary: Pros: - Clearly written and well motivated. - Consistent improvement of the student with attention compared to the student alone. Cons: - Students have worst performances than the teacher models. - It is not clear which attention to use in which case? - Somewhat incremental novelty relatively to Fitnet The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline.   Pros: + The author evaluated the proposed methods on various computer vision dataset  + The paper is in general well-written  Cons:   - The method seems to be limited to the convolutional architecture - The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of  activations in a hidden feature map. - The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the ""attention-based"" transfer. It seems like a parallel idea added to the paper that does not seem to add much value. - It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \in \mathbb{R}^{H \times W}  whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm?  Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.   The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.",1,319
"1) Summary  This paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.  2) Contributions  + Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing. + Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes. + Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors. + Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics. + Open source virtual city environment to benchmark visual servoing.  3) Suggestions for improvement  - More complex benchmark: Although the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.  - End-to-end and representation learning: Although the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).  - Reproducibility: The formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.  - Typos: p.2: ""learning is a relative[ly] recent addition"" p.2: ""be applied [to] directly learn""  4) Conclusion  In spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper.This paper investigates the benefits of visual servoing using a learned visual representation. The authors  propose to first learn an action-conditional bilinear model of the visual features (obtained from a pre-trained VGG net) from which a policy can be derived using a linearization of the dynamics. A multi-scale, multi-channel and locally-connected variant of the bilinear model is presented. Since the bilinear model only predicts the dynamics one step ahead, the paper proposes a weighted objective which incorporates the long-term values of the current policy. The evaluation problem is addressed using a fitted-value approach.  The paper is well written, mathematically solid, and conceptually exhaustive. The experiments also demonstrate the benefits of using a value-weighted objective and is an important contribution of this paper. This paper also seems to be the first to outline a trust-region fitted-q iteration algorithm. The use of pre-trained visual features is also shown to help, empirically, for generalization.  Overall, I recommend this paper as it would benefit many researchers in robotics. However, in the context of this conference, I find the contribution specifically on the ""representation"" problem to be limited. It shows that a pre-trained VGG representation is useful, but does not consider learning it end-to-end. This is not to say that it should be end-to-end, but proportionally speaking, the paper spends more time on the control problem than the representation learning one. Also, the policy representation is fixed and the values are approximated in linear form using problem-specific features. This doesn't make the paper less valuable, but perhaps less aligned with what I think ICLR should be about. The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are:  1. Bilinear dynamics model for predicting next frame (features) based on action and current frame 2. Formulation of servoing with a Q-function that learns weights for different feature channels 3. An elegant method for optimizing the Bellman error to learn the Q-function  Pros: + The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability.   Cons: - While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics.  Pros:  + The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently. + Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + \gamma min_u Q_{t+1}) fixed.   Cons: - However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed.  Experimental results: - Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized. - Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version.  Overall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims. ",0,320
"I like the setting presented in this paper but I have several criticism/questions:  (1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.  (2) Looking at Sec 5.3 -- "" let X be a random variable denoting the grid in which the agent is currently situated"" -- is the space discretized? And if so why and what happens if it isn't.   (3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach  (4) Authors claim that ""Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.""  I don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks. *Edited the score 6->7.  The paper presents a method for hierarchical RL using stochastic neural networks. The paper has introduced using information-theoretic measure of option identifiability as an additional reward for learning a diverse mixture of sub-policies. One nice result in the paper is the comparison with strong baseline which directly combines the intrinsic rewards with sparse rewards and shows that this supposedly smooth reward can’t solve tasks. Besides the argument made from the authors on difficulty on long-term credit assignment/benefits from hierarchical abstraction, one possible explanation for this might be the diversity requirement imposed in sub-policy training, which is assumed to be off in the baseline case. Wonder if this can shed insights into improving the baseline and proposing new end-to-end hierarchical policy learning as hierarchical REPS/option-critic etc. papers do. Nice visualizations.  The paper presents a promising direction, and it may be strengthened further by possibly addressing some of the following points.   1) Limited diversification of sub-policies: Both concatenation and bilinear integration allow only minimal differentiations in sub-policies through first hidden weight, which is not a problem in the tested tasks because they essentially require same locomotion policies with minimal diversification, but such limitation can be more obvious in other tasks where ideal sub-policies are more diverse. Thus it is interesting to see it apply on harder, non-locomotion domains, where ideal sub-policies are not that similar, e.g. for manipulation, solving some task from one state can be very different from solving it from another state.   2) Limitation on hierarchical policies: Manager network is trained while the sub-policies are fixed. Furthermore, the time steps for sub-policies are fixed. This requires “intrinsic” rewards and their learned sub-policies to be very good for solving down-stream tasks. It would be nice to see some more discussions/results on handling such cases, ideally connecting to end-to-end hierarchical policy learning.  3) Intrinsic/unsupervised rewards seem domain-specific/supervised rewards: Because of (2), this seems unavoidable.  Interesting work on hierarchical control, similar to the work of Heess et al.  Experiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side.  (1) Like other reviewers, I find the use of the term ‘intrinsic’ motivation somewhat inappropriate (mostly because of its current meaning in RL).  Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar.   (2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result?  (3) The factorization between S_agent and S_rest should be clearly detailed in the paper. Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it?  (4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome.",1,321
"This paper addresses the problem of allowing networks to change the number of units that are used during training.  This is done in a simple but elegant and well-motivated way.  Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero.  The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units.  In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case.  One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters.   One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem.  The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero.  It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches.  In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case.  The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically.  This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular.  However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks.  Another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time.  Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy.  This means that the cost of grid search is not always paid, but the slowness of the authors’ approach may be endemic.  The authors do not discuss how this issue will scale as much larger networks are trained.  It is a concern that this approach may not be practical for large-scale networks, because training will be very slow.  In general, the experiments are helpful and encouraging, but not comprehensive or totally convincing.  I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful.    Overall, I found this to be an interesting and clearly written paper that makes a potentially useful point.  The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision.  But the current results remain pretty speculative. I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge.  The authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data. That reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights. I would  have like more intuition to be given for this theorem. It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper. For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results. Please also see below (point 2) for some suggestions.  I have a few other comments to make:  1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory?  I understand that experiments along this line are given in Figure 2, with mixed results. The Figure i must say, is not very clear, but it is possible to interpret under careful inspection.  In some the non-parametric nets are doing better and others are doing worse than the parametric ones. Even in such case i could see the usefulness of the method as it helps discovering the structure.   What i don't fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance? Could you give more insight?  2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight?   As i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up. For example, can regularizer parameter lambda  be predicted given the data - is there a property in the data that can help guessing the right lambda? My feeling is that lambda is the key factor for determining the final net structure. Is this true?   How much does the structure of the final net depend on the initialization? Do you get different nets if you start from different random weights? How different are they?  What happens when fan in and fan out regularizers are combined? Do you still have the same theoretical result?  I have a few additional questions:  1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values?  2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think.  I changed my rating to 7, while hoping that the authors will address my comments above.     This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below:  What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments.  It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?",0,322
"This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.  Pros: - Impressively retains accuracy on popular models on ImageNet and Cifar10  Cons: - There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set. - There is no direct comparison to the multitude of other pruning and speedup methods. - While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.  This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines.  The results are a 10% improvements in ResNet-like and ImageNet, which may be also achieved with better design of networks. New networks should have been also compared, but this we know it is time-consuming. A good paper with some useful results.The idea of ""pruning where it matters"" is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too.  Extra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (\mathbb symbol - two thumbs up!).  The experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case?  My question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the ""min sum of weights"" criterion compares to other approaches. How does it compare to other pruning criteria? Is it better than ""pruning at random""?  Overall, I liked the paper.",0,324
"Summary: This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.  Review: The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.  I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)  Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.  Minor: – I am missing citations for “ordered visible dimension sampling” – Typos and frequent incorrect use of \citet and \citepThe paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, ""better"", samples from the blending process.  This is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper. The proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models. I think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as: - convergence and mode coverage problems as in generative adversarial networks - problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model  That being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.  Other major points (good and bad): - Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here. - No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at. - The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well! - Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.  Minor points: - The second reference seems broken - Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ? - The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the ""Improved GANs"" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ? - The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ? - footnote 1 contains errors: ""This allow to"" -> ""allows to"",  ""few informations"" -> ""little information"". ""This force the network"" -> ""forces"" - Page 1 error: etc... - Page 4 error: ""operator should to learn""  [1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015   >>> Update <<<< Copied here from my response below:   I believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.  I am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall ""inflated"" review scores). This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach: - It uses only a small number of denoising steps, and is thus far more computationally efficient. - Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.) - There is no tractable variational bound on the log likelihood.  I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.  Detailed comments follow:  Sec. 2: ""theta(0) the"" -> ""theta(0) be the"" ""theta(t) the"" -> ""theta(t) be the"" ""what we will be using"" -> ""which we will be doing"" I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks. ""q*. Having learned"" -> ""q*. [paragraph break] Having learned"" Sec 3.3: ""learn to inverse"" -> ""learn to reverse"" Sec. 4: ""For each experiments"" -> ""For each experiment"" How sensitive are your results to infusion rate? Sec. 5: ""appears to provide more accurate models"" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper. Fig 4. -- neat! ",1,325
"This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.  Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it.   In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM.   It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further.  I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)? This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.  On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.  On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.  I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.  I suspect followup work building on this proof of concept will be increasingly exciting.  Minor comments: Sec 3.2: I didn't understand the role of the 0.833 ms bins. Use ""epoch"" throughout, rather than alternating between ""epoch"" and ""pass through data"".  Fig. 4 would be better with the x-axis on a log scale.This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.  This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.  I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?     I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative. ",1,328
"The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks.  My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.  In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme.This paper is about using denoising autoencoders to improve performance in GANs. In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well. I think it's an interesting idea to use this ""extra information"" -- namely the feature representations learned by the discriminator. It seems very much in the spirit of ICLR! My main concern, though, is that I'm not wholly convinced on the nature of the improvement. This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results. In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?). One thing I'd be curious to know is, how hard is it to get this thing to actually work?   Also, I view GANs as a means to an end -- while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems. So it would have been nice to see these improvements in inception score translate into improvements in a more useful task. But this criticism could probably apply to many GAN papers and so perhaps isn't fair here.   I do think the idea of exploiting ""extra information"" (like discriminator features) is interesting both inside and outside the context of this paper. This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN.     Extra comment: Please add more discussion with EBGAN in next version.  ",0,329
"This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.  Joint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '“The Sum of Its Parts”: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.  On the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.  Overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.This paper presents a framework for creating document representations.  The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0.  Experiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods.   While I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions. Most of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method.  For this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.   For RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model? Did you use the final hidden state as the representation, or the average of all hidden states? One of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation.  I think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.  The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?  While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.",0,330
"The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.  Compared to previous work (Ammar et al. 2015), it seems the main contribution here is to “assume that good correspondences in episodic tasks can be extracted through time alignment” (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don’t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.  In general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.  Overall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated. This paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent. The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA.    The paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links. For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA. A three layer neural net is used to construct the latent feature spaces.   The problem of transfer in RL is extremely important, and receives less attention than it should. This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent. This is a promising approach. However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic. There is little by way of theoretical development of the ideas using MDP theory.  This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.  One limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).  General remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for ""f"" and ""g"" (the embedding functions on the two domains), to check that the bad performance of the ""no transfer"" version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal \alpha.  The experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no ""CCA"" nor ""direct mapping"" results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?  Overall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.",1,331
"On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well.   I think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- ""we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]"". I am skeptical of any claimed connections bigger than that.  I think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think ""Remarkably"" is justified in the statement ""Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet""  I'm not an expert on human vision, but from browsing online and from what I've learned before it seems that ""object persistence"" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming.  This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the ""Tenenbaum objects” is shown.  Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.  Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.  More details:  The “image purification” paper is very related to this work:  [A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.  There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online (",1,332
"The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.  Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.  The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.  The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.  Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.    This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough.  I do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps.  This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ",0,333
"This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's.   The main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.  Another drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice.The paper presented an extension to the current visual attention model that learns a deformable sampling lattice.  Comparing to the fixed sampling lattice from previous works, the proposed method shows different sampling strategy can emerge depending on the visual classification tasks. The authors empirically demonstrated the learnt sampling lattice outperforms the fixed strategies. More interestingly, when the attention mechanism is constrained  to be translation only, the proposed model learns a sampling lattice resembles the retina found in the primate retina.     Pros: + The paper is generally well organized and written  + The qualitative analysis in the experimental section is very comprehensive.  Cons: -  The paper could benefit substantially from additional experiments on different datasets. -  It is not clear from the tables the proposed learnt sampling  lattice offer any computation benefit when comparing to  a fixed sampling strategy with zooming capability, e.g. the one used in DRAW model.  Overall, I really like the paper. I think the experimental section can be improved by additional experiments and more quantitative analysis with other baselines. Because the current revision of the paper only shows experiments on digit dataset with black background, it is hard to generalize the finding or even to verify the claims in the paper, e.g.  linear relationship between eccentricity and sampling interval leads to the primate retina, from the results on a single dataset.This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina.  The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim.  The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy.  Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way we’d like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective.  Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim.  Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.",1,334
"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.  This seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.  Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges. This paper proposes a hierarchical infomax method. My comments are as follows:   (1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding. Therefore, it is not easy for readers to follow the paper. The authors should make this paper as compact as possible while maintaining the important message.   (2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R). However, it is unclear why maximizing I(X;\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\breve{Y}) is an “upper” bound of I(X;R) (When it is difficult to directly maximize a function, people often maximize some tractable “lower” bound of it).  Minor comments: (1) If (2.11) is approximation of (2.8), “\approx” should be used.   (2) Why K_1 instead of N in Eq.(2.11)?  (3) In Eq.(2.12), H(X) should disappear?  (4) Can you divide Section 3 into subsections?  This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form. Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.  While the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve. For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy. The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.  A few questions that the authors may want to clarify: 1. Page 4, last paragraph: ""from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)"". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true? 2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that? 3. At the end of page 9: ""we will discuss how to get optimal solution of C for two specific cases"". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly. ",0,335
"Summary: This paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.  Review: This is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.  My main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).  The main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has “proved to be a problem for earlier models based on continuous distributions”. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).  60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content – and this can serve as a good example to demonstrate the usefulness of dropout – why not use “80 million tiny images” (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model’s likelihood is tractable), so this data could even be used in the class-conditional case.  It would be interesting to know how fast the different models are at test time (i.e., when generating images).# Review This paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10. Improving generative models, especially for images, is an active research area and this paper definitely contributes to it.   # Pros The authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.  The authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).  The authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.   # Cons It is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.   # Minor Comments In Fig.2 it is written ""Sequence of 6 layers"" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer? In Fig.2 what does the first ""green square -> blue square"" which isn't in the white rectangle represents? Is there any reason why the mixture indicator is shared across all three channels?Apologies for the late submission of this review, and thank you for the author’s responses to earlier questions.  This submission proposes an improved implementation of the PixelCNN generative model. Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis. The submission demonstrates state-of-the-art likelihood results on CIFAR-10.  My summary of the main contribution: Autoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form. A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:  - In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM) the conditional distribution is modelled as a continuous density over real numbers. This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods. Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood. - In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities. This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.  The authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids. This approach makes sense to me, and is new in a way, but it doesn’t appear to be very revolutionary or significant to me.  The second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions). The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model. The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling. Again, I feel that this modification not particularly revolutionary. Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.  Overall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea.",0,336
"This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.  The pros of the paper are: 1. A novel perspective to understand the recent progress of neural network is proposed. 2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.  The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption.   Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.  The paper describes an alternative view on hierarchical feature representations in deep neural networks. The viewpoint of refining representations is well motivated and is in agreement with the success of recent model structures like ResNets.  Pros:  - Good motivation for the effectiveness of ResNets and Highway networks - Convincing analysis and evaluation  Cons:  - The effect of this finding of the interpretation of batch-normalization is only captured briefly but seems to be significant - Explanation of findings in (Zeiler & Fergus (2014)) using UIE viewpoint missing  Remarks:  - Missing word in line 223: ""that it *is* valid""Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                                                                                                                                                                                                                             Pros                                                                                                                                                                                                       - the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                  - Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                                                                                                                                                                                                                                Cons                                                                                                                                                                                                       - Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                          - Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   ",1,338
"This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.  They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously. I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.  I recommend this interesting and well analyzed paper be accepted.This paper proposes a simple extension to a neural network language model by adding a cache component.  The model stores The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines.  The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.  The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.  My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.",1,339
"Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.  This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.  Pros: 1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.  2. The proposed method produces visually appealing results on several datasets  3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task  4. The paper is well-written and easy to read  Cons: 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)  2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.  3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.  I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly.    The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks.   The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work.   It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity.   Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?  Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style.  +The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts.  +This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper.  +The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree.  -It will be more interesting to show results in other domains such as texts and images.  -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ",0,340
"The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).  The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.  I would have expected to see comparison to the following methods added to Figure 3: 1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective. 2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance. 3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).  Including these results would in my view significantly enhance the impact of the paper.The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.  I believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable. There are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order) - The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: ""Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills""; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.   There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: ""we find that this simple approach has been able to solve the problems"" - The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why? - While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ? - Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here. - The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step? - The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?  UPDATE: I updated the score. Please see my response to the rebuttal below. This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.  While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.  Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.",0,341
"This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.  Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.  I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:  - As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.  - One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.Update: I thank the authors for their comments! After reading them, I still think the paper is not novel enough so I'm leaving the rating untouched.  This paper proposes a domain adaptation technique for time series. The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step).  Pros:  1. The authors consider a very important application of domain adaptation.  2. The paper is well-written and relatively easy to read.  3. Solid empirical evaluation. The authors compare their method against several recent domain adaptation techniques on a number of datasets.  Cons:  1. The novelty of the approach is relatively low: it’s just a straightforward fusion of the existing techniques.  2. The paper lacks any motivation for use of the particular combination (VRNN and RevGrad). I still believe comparable results can be obtained by polishing R-DANN (e.g. carefully penalizing domain discrepancy at every step)  Additional comments:  1. I’m not convinced by the discussion presented in Section 4.4. I don’t think the visualization of firing patterns can be used to support the efficiency of the proposed method.  2. Figure 1(c) looks very suspicious. I can hardly believe t-SNE could produce this _very_ regular structure for non-degenerate (non-synthetic, real-world) data.  Overall, it’s a solid paper but I’m not sure if it is up to the ICLR standard.The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings.  The new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4.    Detailed comments: 1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA.  What do you think are the two dominant latent factors encoded in figure 1 (c)?   2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this?  3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left.  ",0,342
"This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.  Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.  It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.  *Pros* 1. Novel approach. 2. Good results.  *Cons* 1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.  *Comments* 1. Please include n-gram results in the table for Wikipedia results.The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided.  Cons/suggestions: - The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions. - The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context. - More compact/convincing examples of human interpretability would be helpful.  Other comments - Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc. This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it.  Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities.  Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved.  Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)",0,343
"This work introduces some StarCraft micro-management tasks (controlling individual units during a battle). These tasks are difficult for recent DeepRL methods due to high-dimensional, variable action spaces (the action space is the task of each unit, the number of units may vary). In such large action spaces, simple exploration strategies (such as epsilon-greedy) perform poorly.  They introduce a novel algorithm ZO to tackle this problem. This algorithm combines ideas from policy gradient, deep networks trained with backpropagation for state embedding and gradient free optimization. The algorithm is well explained and is compared to some existing baselines. Due to the gradient free optimization providing for much better structured exploration, it performs far better.  This is a well-written paper and a novel algorithm which is applied to a very relevant problem. After the success of DeepRL approaches at learning in large state spaces such as visual environment, there is significant interest in applying RL to more structured state and action spaces. The tasks introduced here are interesting environments for these sorts of problems.  It would be helpful if the authors were able to share the source code / specifications for their tasks, to allow other groups to compare against this work.  I found section 5 (the details of the raw inputs and feature encodings) somewhat difficult to understand. In addition to clarifying, the authors might wish to consider whether they could provide the source code to their algorithm or at least the encoder to allow careful comparisons by other work.  Although discussed, there is no baseline comparison with valued based approaches with attempt to do better exploration by modeling uncertainty (such as Bootstrapped DQN). It would useful to understand how such approaches, which also promise better exploration, compare.  It would also be interesting to discuss whether action embedding models such as energy-based approaches (e.g. This is a very interesting and timely paper, with multiple contributions.  - it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units, - it establishes some deep RL baseline results on a collection of Starcraft subdomains, - it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.   As mentioned in an earlier comment, I don’t see why the “gradient of the average cumulative reward” is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they “did not observe a large difference in preliminary experiments” -- so if that is the case, then why not choose the correct objective?  DPQ is characterized incorrectly: despite its name, it does not “collect traces by following deterministic policies”, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this.   Gradient-free optimization is also characterized incorrectly (“it only scales to few parameters”), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your “preliminary experiments with direct exploration in the parameter space” may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?  On the specific results, I’m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.  The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.  There are a few seemingly arbitrary choices that are justified only by ""it worked in practice"". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: ""Also we neglected the argmax operation that chooses the actions"". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as ""We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important"" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?  The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: ""because we neglected that a single u is sampled for an entire episode""; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.  ""perturbated"" -> ""perturbed""  --- After response period:   No rebuttal entered, therefore review remains unchanged.",0,344
"This paper revives a classic idea involving regularization for purposes of compression for modern CNN models on resource constrained devices. Model compression is hot and we're in the midst of lots of people rediscovering old ideas in this area so it is nice to have a paper that explicitly draws upon classic approaches from the early 90s to obtain competitive results on standard benchmarks.  There's not too much to say here: this study is an instance of a simple idea applied effectively to an important problem, written up in an illuminating manner with appropriate references to classic approaches. The addition of the filter visualizations enhances the contribution.The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training.  Quality: Of course it is a serious drawback that the method doesn't seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved). I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition. In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case? Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well? This might be helpful for other people trying to apply this method. I really like Figure 7 in it's latest version.  Clarity: Especially section 2 on MDL is written very well and gives a nice theoretic introduction. Sections 4, 5 and 6 are very short but seem to contain most relevant information. It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters). In 6.1 the authors claim ""Even though most variances seem to be reasonable small there are some that are large"". From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component. It might be helpful to have either a log histogram or separate histograms for each componenent. What are the large points in Figure 2 as opposed to the smaller ones? They seem to have a very good compression/accuracy loss ratio, is that it? Some other points are listed below  originality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches  significance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models. This obviously is a drawback for the practical usability of the methods and therefor it's significance. If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance.  Minor issues:  page 1: There seems to be a space in front of the first author's name page 3: ""in this scenario, pi_0 may be fixed..."". Missing backslash in TeX? page 6: 6.2: two wrong blanks in ""the number of components_, \tau_."" page 6, 6.3: ""in experiences with VGG"": In experiments? page 12: ""Figure C"": Figure 7? This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors. A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks).  This clustering effect can exploited for parameter quantisation and compression of the network parameters. The authors show that this leads to compression rates and predictive accuracy comparable to related approaches.   Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.  A first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero.  In particular a compression rate of 64.2 is obtained on the LeNet300-100 model. In section 6.1 the text refers to figure C, I suppose this should be figure 1.  Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.  Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016). Comparable results are obtained in terms of compression rate and accuracy.  The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.  The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training. This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques. The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets. Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.  ",1,345
"This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called “steerable fibers”. The architecture is strongly inspired by the author’s previous work, as well as that of “capsules” (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn’t action-recognition in videos, for example, not be a better illustrative dataset?  The authors propose a parameterization of CNNs that guarantees equivariance wrt a large family of geometric transformations.  The mathematical analysis is rigorous and the material is very interesting and novel. The paper overall reads well; there is a real effort to explain the math accessibly, though some small improvements could be made.  The theory is general enough to include continuous transformations, although the experiments are restricted to discrete ones. While this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on CIFAR.  Another form of intertwiner has been studied recently by Lenc & Vedaldi [1]; they have studied equivariance empirically in CNNs, which offers an orthogonal view.  In addition to the recent references on scale/rotation deep networks suggested below, geometric equivariance has been studied extensively in the 2000's; mentioning at least one work would be appropriate. The one that probably comes closest to the proposed method is the work by Reisert [2], who studied steerable filters for invariance and equivariance, using Lie group theory. The difference, of course, is that the focus at the time was on kernel machines rather than CNNs, but many of the tools and theorems are relatable.   Some of the notation could be simplified, to make the formulas easier to grasp on a first read:  Working over a lattice Z^d is unnecessarily abstract -- since the inputs are always images, Z^2 would make much of the later math easier to parse. Generalization is straightforward, so I don't think the results lose anything by it; and the authors go back to 2D latices later anyway.  It could be more natural to do away with the layer index l which appears throughout the paper, and have notation for current/next layer instead (e.g. pi and pi'; K and D instead of K_{l+1} and K_l).  In any case I leave it up to the authors to decide whether to include these suggestions on notation, but I urge them to consider them (or other ways to unburden notation).   A few minor issues: Some statements would be better supported with an accompanying reference (e.g. ""Explicit formulas exist"" on page 5, the introduction of intertwiners on page 3). Finally, there is a tiny mistake in the Balduzzi & Ghifary reference (some extra information was included as an author name).  [1] Lenc & Vedaldi, ""Understanding image representations by measuring their equivariance and equivalence"", 2015 [2] Reisert, ""Group integration techniques in pattern analysis: a kernel view"", 2008  This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters.  The basic idea of steerability makes huge sense and seems like a very important idea to develop.  It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s.  This paper approaches it through a formal treatment of group theory.  But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image.  Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill.  I'm not sure what this buys us in the end.  it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful.  Also the description of the experiments is fairly opaque.  I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.    ",1,348
"This paper investigates a set of tasks that augment the basic bAbI problems. In particular, some of the people and objects in the scenarios are replaced with unknown variables. Some of these variables must be known to solve the question, thus the agent must learn to query for the values of these variables. Interestingly, one can now measure both the performance of the agent in correctly answering the question, and its efficiency in asking for the values of the correct unknown variables (and not variables that are unnecessary to answer the question). This inferring of unknown variables goes beyond what is required for the vanilla version of the bAbI tasks, which are now more or less solved.  The paper is well-written, and the contributions are clear. Due to the very limited vocabulary and structure of the bAbI problems in general, I think these tasks (and variants on them) should be viewed more as basic reasoning tasks than natural language understanding. I’m not convinced by the claim of the paper that this really tests the ‘interaction’ capabilities of agents – while the task is phrased as a kind of interaction, I think it’s more aptly described by simply ‘inferring important unknown variables’, which (while important) is more related to reasoning. I’m not sure whether the connection of this ability to ‘interaction’ is more a superficial one.  That being said, it is certainly true that conversational agents will need basic reasoning abilities to converse meaningfully with humans. I sympathise with the general goal of the bAbI tasks, which is to test these reasoning abilities in synthetic environments, that are just complicated enough (but not more) to drive the construction of interesting models. I am convinced by the authors that their extension to these tasks are interesting and worthy of future investigation, and thus I recommend the acceptance of the paper. This paper introduces a nice dataset/data generator that creates bAbI like tasks, except where the questioning answering agent is required to clarify the values of some variables in order to succeed.  I think the baselines the authors use to test the tasks are appropriate.   I am a bit worried that the tasks may be too easy (as the bAbI tasks have been), but still, I think locally these will be useful.  If the generation code is well written, and the tasks are extensible, they may be useful for some time.This paper proposed an integration of memory network with reinforcement learning. The experimental data is simple, but the model is very interesting and relatively novel. There are some questions about the model:  1. how does the model extend to the case with multiple variables in a single sentence?  2. If the answer is out of vocabulary, how would the model handle it?  3. I hope the authors can provide more analysis about the curriculum learning part, since it is very important for the RL model training.  4. In the training, in each iteration, how the data samples were selected, by random or from simple one depth to multiple depth?  ",1,349
"Paper Summary This paper proposes an unsupervised learning model in which the network predicts what its state would look like at the next time step (at input layer and potentially other layers).  When these states are observed, an error signal is computed by comparing the predictions and the observations. This error signal is fed back into the model. The authors show that this model is able to make good predictions on a toy dataset of rotating 3D faces as well as on natural videos. They also show that these features help perform supervised tasks.  Strengths - The model is an interesting embodiment of the idea of predictive coding   implemented using a end-to-end backpropable recurrent neural network architecture. - The idea of feeding forward an error signal is perhaps not used as widely as it could   be, and this work shows a compelling example of using it.  - Strong empirical results and relevant comparisons show that the model works well. - The authors present a detailed ablative analysis of the proposed model.  Weaknesses - The model (esp. in Fig 1) is presented as a generalized predictive model   where next step predictions are made at each layer. However, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers. While the authors intend to address this in future work, I think this point merits some more discussion in the current work, given the way this model is presented. - The network currently lacks stochasticity and does not model the future as a   multimodal distribution (However, this is mentioned as potential future work).  Quality The experiments are well-designed and a detailed analysis is provided in the appendix.  Clarity The paper is well-written and easy to follow.  Originality Some deep models have previously been proposed that use predictive coding. However, the proposed model is most probably novel in the way it feds back the error signal and implements the entire model as a single differentiable network.  Significance This paper will be of wide interest to the growing set of researchers working in unsupervised learning of time series. This helps draw attention to predictive coding as an important learning paradigm.  Overall Good paper with detailed and well-designed experiments. The idea of feeding forward the error signal is not being used as much as it could be in our community. This work helps to draw the community's attention to this idea.Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning. In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))  I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images. Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.  The weaknesses: - the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model. - any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.  Minor comment: Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.  Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.   ""Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).""   It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.  ",1,350
"The problem addressed here is practically important (supervised learning with n<The paper addresses the important problem (d>>n) in deep learning.  The proposed approach, based on lower-dimensional feature embeddings, is reasonable and makes applying deep learning methods to data with large d possible. The paper is well written and the results show improvements over reasonable baselines. The paper presents an application of deep learning to genomic SNP data with a comparison of possible approaches for dealing with the very high data dimensionality. The approach looks very interesting but the experiments are too limited to draw firm conclusions about the strengths of different approaches. The presentation would benefit from more precise math.   Quality:  The basic idea of the paper is interesting and the applied deep learning methodology appears reasonable. The experimental evaluation is rather weak as it only covers a single data set and a very limited number of cross validation folds. Given the significant variation in the performances of all the methods, it seems the differences between the better-performing methods are probably not statistically significant. More comprehensive empirical validation could clearly strengthen the paper.   Clarity:  The writing is generally good both in terms of the biology and ML, but more mathematical rigour would make it easier to understand precisely what was done. The different architectures are explained on an intuitive level and might benefit from a clear mathematical definition. I was ultimately left unsure of what the ""raw end2end"" model is - given so few parameters it cannot work on raw 300k dimensional input but I could not figure out what kind of embedding was used.  The results in Fig. 3 might be clearer if scaled so that maximum for each class is 1 to avoid confounding from different numbers of subjects in different classes. In the text, please use the standard italics math font for all symbols such as N, N_d, ...   Originality:  The application and the approach appear quite novel.   Significance:  There is clearly strong interest for deep learning in the genomics area and the paper seeks to address some of the major bottlenecks here. It is too early to tell whether the specific techniques proposed in the paper will be the ultimate solution, but at the very least the paper provides interesting new ideas for others to work on.   Other comments:  I think releasing the code as promised would be a must. ",0,351
"All in all this is a nice paper.  I think the model is quite clever, attempting to get the best of latent variable models and auto-regressive models. The implementation and specific architecture choices (as discussed in the pre-review) also seem reasonable. On the experimental side, I would have liked to see something more than NLL measurements and samples - maybe show this is useful for other tasks such as classification?  Though I don't think this is a huge leap forward this is certainly a nice paper and I recoemmend acceptance.The paper combines a hierarchical Variational Autoencoder with PixelCNNs to model the distribution of natural images.  They report good (although not state of the art) likelihoods on natural images and briefly start to explore what information is encoded by the latent representations in the hierarchical VAE.  I believe that combining the PixelCNN with a VAE, as was already suggested in the PixelCNN paper, is an important and interesting contribution.  The encoding of high-, mid- and low-level variations at the different latent stages is interesting but seems not terribly surprising, since the size of the image regions the latent variables model is also at the corresponding scale. Showing that the PixelCNN improves the latent representation of the VAE with regard to some interesting task would be a much stronger result.  Also, while the paper claims, that combining the PixelCNN with the VAE reduces the number of computationally expensive autoregressive layers, it remains unclear how much more efficient their whole model is than an PixelCNN with comparable likelihood.  In general, I find the clarity of the presentation wanting. For example, I agree with reviewer1 that the exact structure of their model remains unclear from the paper and would be difficult to reproduce.  UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance. -------------- This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder. I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.  I have the following suggestions/concerns about the paper:  1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.  2) A similar idea is also presented in a concurrent ICLR submission ""Variational Lossy Autoencoder"". It would be interesting to have a discussion included in the paper and compare these works.  3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.  4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.  I will gladly raise the score if the authors address my concerns.",0,353
"The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.   Positives:  1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper. 2. Well written paper, with clear description of the method and thorough experiments.   Suggestions for improvement / other comments:  1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with ""true ensembles"" (i.e., ensembles of networks trained independently) should be provided. Specificially, Table 4 should be augmented with results from ""true ensembles"".  2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of ""true ensemble"" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: ""[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].""  3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) ""true ensembles"", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).I don't have much to add to my pre-review questions. The main thing I'd like to see that would strengthen my review further is a larger scale evaluation, more discussion of the hyperparameters, etc. Where test error are reported for snapshot ensembles it would be useful to report statistics about the performance of individual ensemble members for comparison (mean and standard deviation, maybe best single member's error rate).This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time. The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima's well of attraction. The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost).   This paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets. I especially liked the analysis in Section 4.4.  The publicly available code to ensure reproducibility is also greatly appreciated.  I would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles.  Preliminary rating: This is an interesting work with convincing experiments and clear writing.   Minor note: Why is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.",1,354
"This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.  Two of my concerns have remained unanswered (see AnonReviewer2, below).   In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.This paper basically applies A3C to 3D spatial navigation tasks.   - This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper  -  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust  - Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.  The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.  If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.  I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.  --- Added after rebuttal:  I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted. ",1,355
"This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).  There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.  Other miscellaneous comments: * Too many e’s in the expansion probability expression — might be better just to write “Softmax”. * There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see). * The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good. * It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible. * In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?) * There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs). This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.  Questions/Comments:  - The dataset is a good choice, because it is simple and easy to understand. What is the effect of the ""rule based strategy"" for computing well formed input strings?  - Clarify what ""backtracking search"" is? I assume it is the same as trying to generate the latent function?   - In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.  The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.  Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.  More comments:  I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?  Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?  Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.",0,356
"1) Summary  This paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.  2) Contributions  + The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition. + The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.  3) Suggestions for improvement  Static dataset bias: In response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the ""pixel-copying"" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.  Additional recognition experiments: As mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.  4) Conclusion  Overall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.  5) Post-rebuttal final decision  The authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.  The paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.  Details :  The paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.  Example : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are ""additional"" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.  Equation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.  The residual nature of the layer should be made more apparent in equation (3).  There are several typos, absence of articles and prepositions (""of"" etc.). The paper should be reread carefully. This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.  The idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.  While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset.  The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question.  Overall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.",0,357
"The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community.   Comments:  - It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".  - The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following:   (a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior?   (b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.  This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to ""learn to learn"" by acquiring the ability to learn distributions from small numbers of examples.  Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.  The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that ""someone who thinks up statistics"".   The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.  The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.)   Will the authors release the code? This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a ""double"" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.    Hierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.    Pros:   -The few-shot learning results look good, but I'm not an expert in this area.     -The idea of using a ""double"" variational bound in a hierarchical generative model is well presented and seems widely applicable.    Questions:    -When training the statistic network, are minibatches (i.e. subsets of the examples) used?     -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.    Suggestions:    -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ",1,358
"The paper proposes to study the problem of semi-supervised RL where one has to distinguish between labelled MDPs that provide rewards, and unlabelled MDPs that are not associated with any reward signal. The underlying is very simple since it aims at simultaneously learning a policy based on the REINFORCE+entropy regularization technique, and also a model of the reward that will be used (as in inverse reinforcement learning) as a feedback over unlabelled MDPs. The experiments are made on different continous domains and show interesting results  The paper is well written, and easy to understand. It is based on a simple but efficient idea of simultaneously learning the policy and a model of the reward and the resulting algorithm exhibit interesting properties. The proposed idea is quite obvious, but the authors are the first ones to propose to test such a model. The experiments could be made stronger by mixing continuous and discrete problems but are convincing. This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward. The authors name this setting ""semi-supervised reinforcement learning"" (SSRL), as a reference to semi-supervised learning (where one only has access to labels for a subset of the dataset). They provide an approach for solving SSRL named semi-supervised skill generalization (S3G), which builds on the framework of maximum entropy control. The whole approach is straightforward and amounts to an EM algorithm with partial labels (: they alternate iteratively between estimating a reward function (parametrized) and fitting a control policy using this reward function. They provide experiments on 4 tasks (obstacle, 2-link reacher, 2-link reacher with vision, half-cheetah) in MuJoCo.  The paper is well-written, and is overall clear. The appendix provides some more context, I think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.  The link to inverse reinforcement learning seems to be done correctly. However, there is no reference to off-policy policy learning, and, for instance, it seems to me that the \tau \in D_{samp} term of equation (3) could benefit from variance reduction as in e.g. TB(\lambda) [Precup et al. 2000] or Retrace(\lambda) [Munos et al. 2016].  The experimental section is convincing, but I would appreciate a precision (and small discussion) of this sentence ""To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs"" with numbers for the different scenarios (or the replacement of superset by ""union"" if this is the case). It may explain better the poor results of ""oracle"" on ""obstacle"" and ""2-link reacher"", and reinforce* the further sentences ""In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs. Hence, the reward regression and oracle methods perform poorly"".  Correction on page 4: ""5-tuple M_i = (S, A, T, R)"" is a 4-tuple.  Overall, I think that this is a good and sound paper. I am personally unsure as to if all the parallels and/or references to previous work are complete, thus my confidence score of 3.  (* pun intended)In supervised learning, a significant advance occurred when the framework of semi-supervised learning was  adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically.   This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term ""labeled MDP"" to mean the typical MDP framework where the reward function is unknown. They use the confusing term ""unlabeled MDP"" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process).   In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source ""labeled"" MDP to a target ""labeled"" MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an ""unlabeled"" CMP, and the source is both a ""labeled"" MDP and an ""unlabeled"" CMP. The basic approach is to use inverse RL to infer the unknown ""labels"" and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator.   The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study. ",1,360
"This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models. This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictions. This builds on previous work in Domhan, 2015, however in this work the model incorporates information from all tested hyperparameter settings rather than just extrapolating from a single learning curve. This paper also explores two MCMC methods for inference: SGLD and SGHMC, but I couldn’t tell if either of these were tested in Domhan, 2015 as well.  The performance seems overall positive, particularly in the initial phase of each curve where there is very little information. In this case, as expected, sharing knowledge across curves helps. One regime which did not seem to be tested, but might be very informative, is when some curves in the training set have been mostly, or fully observed. This might be a case where sharing information really helps.  Something that concerns me about this approach is the timing. The authors stated that to train the network takes about 20-60 seconds. In the worst case, with 100 epochs, this results in a little over 1.5 hours spent training the Bayesian network. This is a non-trivial fraction of the several hours it takes to train the model being tuned.  The Bayesian network makes many separate predictions, as shown in Figure 2. It would be interesting to see how accurate some of these individual pieces are. For example, did you bound the asymptotic value of the learning curve, since you mostly predicted accuracy? If not, did the value tend to lie in [0,1]?  Below are some minor questions/comments.  Figure 1 axes should read “validation accuracy” Figure 6 can you describe LastSeenValue (although it seems self-explanatory, it’s good to be explicit) in the bottom left figure, and why isn’t it used anywhere else as a baseline? Figure 7 and Table 1 are you predicting just the final value of the curves? Or every value along each curve, conditioned on the previous values? Why do you only use 5 basis functions? Does this sufficiently capture all of the flexibility of these learning curves? Would more basis functions help or hurt? This paper is about using Bayesian neural networks to model learning curves (that arise from training ML algorithms). The application is hyper-parameter optimization: if we can model the learning curve, we can terminate bad runs early and save time. The paper builds on existing work that used parametric learning curves. Here, the parameters of these learning curves form the last layer of a Bayesian neural network. This seems like a totally sensible idea.   I think the main strength of this paper is that it addresses an actual need. Based on my personal experience, there is high demand for a working system to do early termination in hyperparameter optimization. What I'd like to know, which I wish I'd asked during pre-review questions, is whether the authors plan to release their code. Do you? I sincerely hope so, because I think the code would be a significant part of the paper's contribution, since the nature of the paper is more practical than conceptual.  The experiments in the paper seem thorough but the results are a bit underwhelming. I'm less interested in the part about whether the learning curves are actually modeled well, and more interested in the impact on hyperparameter optimization. I was hoping to see BIG speedups as a result of using this method, but I am left feeling unsure how big the speedup really is. Instead of ""objective function vs. iterations"" I would be more interested in the inverse plot: number of iterations needed to get to a fixed objective function value. Since what I'm really interested in is how much time I can save. Ideally there would also be some mention of real time as sometimes these hyperparameter optimization methods are themselves so slow that they end up being unusable.   Finally, one figure that I feel is missing is a histogram of termination times over different runs. This would provide me with more intuition than all the other figures. Because it would tell me, what fraction of runs are being terminated early. And, how early? Right now I have no sense of this, except that at least *some* runs are clearly being terminated early, since this is neccessary for the proposed method to outperform other methods.  Overall, I think this paper merits acceptance because it is a solid effort on an interesting problem. The progress is fairly incremental but I can live with that.The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC.  The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I'd love to see an experiment that evaluates the relative advantage of this proposed method :)  Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that?  I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run?  Minor comments:  Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures.  Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?",0,361
"This paper proposes an approach to learning a custom optimizer for a given class optimization problems. I think in the case of training machine learning algorithms, a class would represent a model like “logistic regression”. The authors cleverly cast this as a reinforcement learning problem and use guided policy search to train a neural network to map the current location and history onto a step direction/magnitude. Overall I think this is a great idea and a very nice contribution to the fast growing meta-learning literature. However, I think that there are some aspects that could be touched on to make this a stronger paper.  My first thought is that the authors claim to train the method to learn the regularities of an entire class of optimization problems, rather than learning to exploit regularities in a given set of tasks. The distinction here is not terribly clear to me. For example, in learning an optimizer for logistic regression, the authors seem to claim that learning on a randomly sampled set of logistic regression problems will allow the model to learn about logistic regression itself. I am not convinced of this, because there is bias in the randomly sampled data itself. From the paper in this case, “The instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each.” It seems the optimizer is then trained to optimize instances of logistic regression *given this specific family of training inputs* and not logistic regression problems in general. A simple experiment to prove the method works more generally would be to repeat the existing experiments, but where the test instances are drawn from a completely different distribution. It would be even more interesting to see how this changes as the test distribution deviates further from the training distribution.  Can the authors comment on the choice of architecture used here? Why one layer with 50 hidden units and softplus activations specifically? Why not e.g., 100 units, 2 layers and ReLUs? Presumably this is to prevent overfitting, but given the limited capacity of the network, how do these results look when the dimensionality of the input space increases beyond 2 or 3?  I would love to see what kind of policy the network learns on e.g., a 2D function using a contour plot. What do the steps look like on a random problem instance when compared to other hand-engineered optimizers?  Overall I think this a really interesting paper with a great methodological contribution. My main concern is that the results may be oversold as the problems are still relatively simple and constrained. However, if the authors can demonstrate that this approach produces robust policies for a very general set of problems then that would be truly spectacular.  Minor notes below.  Section 3.1 should you be using \pi_T^* to denote the optimal policy? You use \pi_t^* and \pi^* currently.  Are the problems here considered noiseless? That is, is the state transition given an action deterministic? It would be very interesting to see this on noisy problems. The current version of the paper is improved w.r.t. the original arXiv version from June. While the results are exactly the same, the text does not oversell them as much as before. You may also consider to avoid words like ""mantra"", etc.  I believe that my criticism given in my comment from 3 Dec 2016 about ""randomly generated task"" is valid and you answer is not. This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks.  As pointed below, this is a useful addition.  However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain.  In summary, the idea is a good one, but the experiments are weak.  ",1,362
"The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.  This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.  Detail:  - The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections. - If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy). - Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)? - You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite. - Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly. This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.  The paper is well written overall.  A few detailed comments: * page 4, line5: including a some -> including some * What's the benefit of the preprocessing and attention step? Can you provide the results without it? * Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality. This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs.  The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.     ",1,363
"The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.  The approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?  I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression.   A quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?  Preliminary rating: I think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.  Minor notes: Please resize equation 4 to fit within the margins (\resizebox{\columnwidth}{!}{ blah } works well in latex for this)The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.  This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.  Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.  As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.  The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.  The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.   The paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.  The result tables are a bit confusing unfortunately.  minor issues:  p1 english mistake: “while networks *that* consist of convolutional layers”.  p6-p7 Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:  In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better. I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.",0,365
"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.  Minor comments: Please add citation to [1] or [2] for neural variational inference, and [2] for VAE.  A typo in “This approximation to the Dirichlet prior p(θ|α) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(θ|α) results in the distribution”  In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?  In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?  How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?  It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).  Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions.   [1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14 [2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)  In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work.   Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?  Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.  Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?  None of the numbers include error bars. Are the results statistically significant?   Minor comments:  Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?  The idea of using an inference network is much older, cf. Helmholtz machine.  ",1,366
"The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data.  The paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.   Given the weights of the ‘min-max’decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.  The paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.   clarity:  -The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2).    significance  Overall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.  - what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.  - what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?  - Have you tried on more complex datasets such as cifar?The author attacks the problem of shallow binary autoencoders using a minmax game approach. The algorithm, though simple, appears to be very effective. The paper is well written and has sound analyses. Although the work does not extend to deep networks immediately, its connections with other popular minmax approaches (eg GANs) could be fruitful in the future.The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach.   The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing. ",0,367
"The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: “Measuring the reliability of MCMC inference with bidirectional Monte Carlo” is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models.  In section 2.3 the authors seem to make a mistake. They write E[p’(x)] <= p(x) but I think they mean: E[log p’(x)] <= log E[p’(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can’t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise. On page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ? I am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50? The experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great.  Summary: This paper describes how to estimate log-likelihoods of currently popular decoder-based generative models using annealed importance sampling (AIS) and HMC. It validates the method using bidirectional Monte Carlo on the example of MNIST, and compares the performance of GANs and VAEs.   Review: Although this seems like a fairly straight-forward application of AIS to me (correct me if I missed an important trick to make this work), I very much appreciate the educational value and empirical contributions of this paper. It should lead to clarity in debates around the density estimation performance of GANs, and should enable more people to use AIS.  Space permitting, it might be a good idea to try to expand the description of AIS. All the components of AIS are mentioned and a basic description of the algorithm is given, but the paper doesn’t explain well “why” the algorithm does what it does/why it works.  I was initially confused by the widely different numbers in Figure 2. On first glance my expectation was that this Figure is comparing GAN, GMMN and IWAE (because of the labeling at the bottom and because of the leading words in the caption’s descriptions). Perhaps mention in the caption that (a) and (b) use continuous MNIST and (c) uses discrete MNIST. “GMMN-50” should probably be “GMMN-10”.   Using reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model. Depending on the likelihood, a posterior sample might have very low density under the prior, for example. It would be great if the authors could point out and discuss the limitations of this test a bit more.   Minor:  Perhaps add a reference to MacKay’s density networks (MacKay, 1995) for decoder-based generative models.  In Section 2.2, the authors write “the prior over z can be drastically different than the true posterior p(z|x), especially in high dimension”. I think the flow of the paper could be improved here, especially for people less familiar with importance sampling/AIS. I don’t think the relevance of the posterior for importance sampling is clear at this point in the paper.  In Section 2.3 the authors claim that is often more “meaningful” to estimate p(x) in log-space because of underflow problems. “Meaningful” seems like the wrong word here. Perhaps revise to say that it’s more practical to estimate log p(x) because of underflow problems, or to say that it’s more meaningful to estimate log p(x) because of its connection to compression/surprise/entropy.# Review This paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)).  The authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model.   # Pros Their evaluation framework is public and is definitely a nice contribution to the community.  This paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution.   # Cons/Questions It is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set?  It is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both?  16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2?  I would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others?   # Minor comments Table 1 is not referenced in the text and lacks description of what the different columns represent. Figure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2). Figure 2(c), I'm guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder? Are the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ? Typo in caption of Figure 3: ""(c) GMMN-10"" but actually showing GMMN-50 according to the graph title and subcaption.",0,368
"This work presents a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values. Unlike past work, these quantized values are separate and learned stochastically alongside all other network parameters. This approach achieves impressive quantization results while retaining or surpassing corresponding full-precision networks on CIFAR10 and ImageNet.  Strengths:  - Overall well written and algorithm is presented clearly. - Approach appears to work well in the experiments, resulting in good compression without loss (and sometimes gain!) of performance. - I enjoyed the analysis of sparsity (and how it changes) over the course of training, though it is uncertain if any useful conclusion can be drawn from it.  Some points:  - The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. Can the authors provide average activation sparsity for each network to help verify this assumption. Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.  - In section 5.1.1, the authors suggest having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity (owed to the relative magnitude of different layer weights with respect to the maximum). In Section 5.1.2 paragraph 2, this is further developed by suggesting additional sparsity can be achieved by allowing each layer a different values of t. How are these values set? Does this multiple threshold style network appear in any of the tables or figures? Can it be added?  - The authors claim ""ii) Quantized weights play the role of ""learning rate multipliers"" during back propagation."" as a benefit of using trained quantization factors. Why is this a benefit?   - Figure and table captions are not very descriptive.  Preliminary Rating: I think this is an interesting paper with convincing results but is somewhat lacking in novelty.   Minor notes: - Table 3 lists FLOPS rather than Energy for the full precision model. Why? - Section 5 'speeding up' - 5.1.1 figure reference error last line This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values. The group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts.  I also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc. This data is important to decide if a compression is worth the effort.The paper shows a different approach to a ternary quantization of weights. Strengths: 1. The paper shows performance improvements over existing solutions 2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.  Weaknesses: 1. The paper is very incremental. 2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is ""what is new in the topic"" update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience. 3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation ""it is related to mobile, therefore it is cool"" sufficient.  This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience.  Also - the code was not released is my understanding.",1,369
"This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.  Finally, all the weights are again jointly trained.  Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results.  The proposed idea is novel and interesting.  In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different.  The main advantage of the proposed method is its simplicity.  Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase.  Given these, it can be plugged in to training a range of networks, as shown in the experiments.  The concern I have is regarding the current empirical evaluation.  As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method.  Standard tricks, such as dropping the learning rate upon ""convergence"" and continuing to learn, can be employed.  The response seems to indicate that these approaches can be effective.  I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper.  An exploration regarding the sparsity hyper-parameter would also be interesting. Summary:  The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations.  Pro: The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that there’s significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used.   Cons & Questions: The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs).  Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD… approach?   Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training.  The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community.  The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs. ",1,370
"- summary  The paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.  - novelty  The differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.  - citations   This work includes all relevant citations.  - clarity  The article is well written and easy to understand.  - experiments   Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio).   Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening.   The authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.  - conclusion / recommendation  The main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.  Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other ""bottom-up"" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in ""common-sense"" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear.Paper proposes a neural physics engine (NPE). NPE provides a factorization of physical scene into composable object-based representations. NPE predicts a future state of the given object as a function composition of the pairwise interactions between itself and near-by objects. This has a nice physical interpretation of forces being additive. In the paper NPE is investigated in the context of 2D worlds with balls and obstacles.   Overall the approach is interesting and has an interesting flavor of combining neural networks with basic properties of physics. Overall, it seems like it may lead to interesting and significant follow up work in the field. The concerns with the paper is mainly with evaluation, which in places appears to be weak (see below).   > Significance & Originality:  The approach is interesting. While other methods have tried to build models that can deal with physical predictions, the idea of summing over pair-wise terms, to the best of my knowledge, is novel and much more in-line with the underlying principles of mechanics. As such, while relatively simple, it seems to be an important contribution.   > Clarity:  The paper is generally well written. However, large portion of the early introduction is rather abstract and it is difficult to parse until one gets to 5th paragraph. I would suggest editing the early part of introduction to include more specifics about the approach or even examples ... to make text more tangible.  > Experiments  Generally there are two issues with experiments in my opinion: (1) the added indirect comparison with Fragkiadaki et al (2015) does not appears to be quantitatively flattering with respect to the proposed approach, and (2) quantitative experiments on the role the size of the mask has on performance should really be added. Authors mention that they observe that mask is helpful, but it is not clear how helpful or how sensitive the overall performance is to this parameter. This experiment should really be added.  I do feel that despite few mentioned shortcomings that would make the paper stronger, this is an interesting paper and should be published.Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls.  The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object's state to predict the focus object's change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object's neighbors in a sequence of LSTM states.  NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines.   Comments ===  * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors?   * Section 2.1 begins with ""First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).""  I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation.   * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled ""Interaction Networks for Learning about Objects, Relations and Physics."" This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared.   Overall Evaluation ===  This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.",0,371
"The paper proposes a new memory module to be used as an addition to existing neural network models.  Pros: * Clearly written and original idea. * Useful memory module, shows nice improvements. * Tested on some big tasks.  Cons: * No comparisons to other memory modules such as associative LSTMs etc. A new memory module based on k-NN is presented. The paper is very well written and the results are convincing.   Omniglot is a good sanity test and the performance is surprisingly good. The artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain. And the translation task eventually makes a very strong point on practical usefulness of the proposed model.  I am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.  Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.  Authors have addressed all my pre-review questions and I am ok with their response.  Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?  References:  [R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016) ",1,372
"This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.  It'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is.Authors' response well answered my questions. Thanks! Evaluation not changed.  ###  This paper proposes a hierarchical framework of transfer learning for sequence tagging, which is expected to help the target task with the source task, by sharing as many levels of representation as possible. It is a general framework for various neural models. The paper has extensive and solid experiments, and the performance is competitive with the state of the art on multiple benchmark datasets. The framework is clear by itself, except that more details about training procedure, i.e. sec-3.3, need to be added.   The experimental results show that for some task pairs {s,t}, this framework can help low-resource target task t, and the improvement increases with more levels of representations can be shared. Firstly, I suggest that the terms *source* and *target* should be more precisely defined in the current framework, because, due to Sec-3.3, the s and t in each pair are sort of interchangeable. That is, either of them can be the *source* or *target* task, especially when p(X=s)=p(X=t)=0.5 is used in the task sampling. The difference is: one is low-resourced and the other is not. Thus it could be thought of as multi-tasking between tasks with imbalanced resource. So one question is: does this framework simultaneously help both tasks in the pair, by learning more generalizable representations for different domains/applications/languages? Or is it mostly likely to only help the low-resourced one? Does it come with sacrifice on the high-resourced side?   Secondly, as the paper shows that the low-resourced tasks are improved for the selected task pairs, it would also be interesting and helpful to know how often this could happen. That is, when the tasks are randomly paired (one chosen from a low-resource pool and the other from a high resource pool), how often could this framework help the low-resourced one?  Moreover, the choice of T-A/T-B/T-C lies intuitively in how many levels of representation *could* be shared as possible. This implicitly assumes share more, help more. Although I tend to believe so, it would be interesting to have some empirical comparison. For example, one could perhaps select some cross-domain pair, and see if T-A > T-B > T-C on such pairs, as mentioned in the author’s answer to the pre-review question.   In general, I think this is a solid paper, and more exploration could be done in this direction. So I tend to accept this paper. The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks.  The field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared ""layers"" being dependent of the task of interest.  The novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks.  The experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale.  Figure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results.  Overall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.",0,373
"SUMMARY.  The paper proposes a gating mechanism to combine word embeddings with character-level word representations. The gating mechanism uses features associated to a word to decided which word representation is the most useful. The fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction. For the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed. In both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.   ----------  OVERALL JUDGMENT  This paper proposes a clever fine-grained extension of a scalar gate for combining word representation. It is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.  I liked the ablation study that shows quite clearly the impact of individual contributions. And I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way.  It would be interesting to see if syntactic features can be helpful.  I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.  I am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model.This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task.   Intuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice!   In sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.",1,374
"This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.  Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.  Here are some comments on technical details:  - The word ""discourse"" is confusing. I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning. - Is there any justification about $c_0$ related to syntac? - Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings."" in section ""Computing the sentence embedding"" - Is there any explanation about the results on sentiment in Table 2?This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger.  Though it is sad that for sufficiently large datasets, NB-SVM still works better.  In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.  Minor comments:  ""The capturing the similarities"" -- typo in line 2 of intro. ""Recently, (Wieting et al.,2016) learned"" -- use citet instead of parenthesized citation  ",1,375
"The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.  The experimental setups look sound. To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups.   The descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c. It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.  Novelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I’d argue that the paper is original enough for that reason alone.  The paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.  Pros: * Thorough analysis. * Seemingly proper experiments. * The way of quantifying capacity in neural networks adds to the novelty of the paper. * The results have some practical value and suggest similar analysis of other architectures. * The results provide useful insights into the relative merits of different RNN architectures.  Cons: * It’s hard to isolate the most important findings (some plots seem redundant). * Some relevant experimental details are missing.CONTRIBUTIONS Large-scale experiments are used to measure the capacity and trainability of different RNN architectures. Capacity experiments suggest that across all architectures, RNNs can store between three and six bits of information per parameter, with ungated RNNs having the highest per-parameter capacity. All architectures are able to store approximately one floating point number per hidden unit. Trainability experiments show that ungated architectures (RNN, IRNN) are much harder to train than gated architectures (GRU, LSTM, UGRNN, +RNN). The paper also proposes two novel RNN architectures (UGRNN and +RNN); experiments suggest that the UGRNN has similar per-parameter capacity as the ungated RNN but is much easier to train, and that deep (8-layer) +RNN models are easier to train than existing architectures.  CLARITY The paper is well-written and easy to follow.  NOVELTY This paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter. The idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup.  The proposed UGRNN is similar but not identical to the minimal gated unit proposed by Zhou et al, “Minimal Gated Unit for Recurrent Neural Networks”, International Journal of Automation and Computing, 2016.  SIGNIFICANCE I have mixed feelings about the significance of this paper. I found the experiments interesting, but I don’t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that I think about RNNs, or the way that I will use them in my own future work. On the other hand it is valuable to see intuitive results about RNNs confirmed by rigorous experiments, especially since few researchers have the computational resources to perform such large-scale experiments.  The capacity experiments (both per-parameter capacity and per-unit capacity) essentially force the network to model random data. For most applications of RNNs, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of real-world tasks, we hope that RNNs can learn to model data that is anything but random. It is not clear to me that an architecture’s ability to model random data should be beneficial in modeling real-world data; indeed, the experiments in Section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in Section 3 show that these same architectures do not significantly vary in their capacity to model real-world data.  I do not think that the experimental results in the paper are sufficient to prove the significance of the proposed UGRNN and +RNN architectures. It is interesting that the UGRNN can achieve comparable bits per parameter as the ungated RNN and that the deep +RNNs are more easily trainable than other architectures, but the only experiments on a real-world task (language modeling on text8) do not show these architectures to be significantly better than GRU or LSTM.  SUMMARY I wish that the experiments had revealed more surprising insights about RNNs, though there is certainly value in experimentally verifying intuitive results. The proposed UGRNN and +RNN architectures show some promising results on synthetic tasks, but I wish that they showed more convincing performance on real-world tasks. Overall I think that the good outweighs the bad, and that the ideas of this paper are of value to the community.  PROS - The paper is the first of my knowledge to explicitly measure the bits per parameter that RNNs can store - The paper experimentally confirms several intuitive ideas about RNNs:     - RNNs of any architecture can store about one number per hidden unit from the input     - Different RNN architectures should be compared by their parameter count, not their hidden unit count     - With very careful hyperparameter tuning, all RNN architectures perform about the same on text8 language modeling     - Gated architectures are easier to train than non-gated RNNs  CONS - Experiments do not reveal anything particularly surprising or unexpected - The UGRNN and +RNN architectures do not feel well-motivated - The utility of the UGRNN and +RNN architectures is not well-establishedThis paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.     Pros:  * This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it.   This is a nice use of Google's resources to help the community.   * The work appears to have been done carefully so that the results can be believed.  * The basic answer arrived at (that, in the ""typical training environment"" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.   Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.  * The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.  In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity.  * The point about the near-equivalence of capacity at equal numbers of parameters is very useful.     * The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.  * The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed.   * The paper text is very clearly written.  Cons:  * The work on the UGRNNs and the +RNNs seems a bit preliminary.  I don't think that the authors have clearly shown that the +RNN should be ""recommended"" with the same generality as the GRU.   I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel).   In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.   I don't really mind having them in the paper though.   I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don't know what the ICLR area chairs will think.   * The paper gives short shrift to the details of the HP algorithm itself.  They do say:        ""Our setting of the tuner’s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a         Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs""    and give some good references, but I expect that actually trying to replicate this involves a lot of missing details.     * I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.     * The neuroscience reference (""4.7 bits per synapse"") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.  I guess it's just in the discussion, but it seems gratuitous.   Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be ""in agreement"" here between computational architectures and neuroscience, but perhaps they could say something like -- ""We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience."")    ",1,376
"This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.  As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks.   The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”. The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.  One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.  To discern the level of contribution of the paper, one must ask the following questions:   1)	how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and 2)	how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects?   It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks).   Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.  Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.    --------------- EDIT: score updated, see comments below This paper investigates the question of gathering information (answering question) through direct interaction with the environment. In that sense, it is closely related to ""active learning"" in supervised learning, or to the fundamental problem of exploration-exploitation in RL. The authors consider a specific  instance of this problem in a physics domain and learn information-seeking policies using recent deep RL methods.  The paper is mostly empirical and explores the effect of changing the cost of information (via the discount factor) on the structure of the learned policies. It also shows that general-purpose deep policy gradient methods are sufficient powerful to learn such tasks. The proposed environment is, to my knowledge, novel as well the task formulation in section 2. (And it would be very valuable to the the community if the environment would be open-sourced)  The expression ""latent structure/dynamics"" is used throughout the text and the connection with bandits is mentioned in section 4. It therefore seems that authors aspire for more generality with their approach but the paper doesn't quite fully ground the proposed approach formally in any existing framework nor does it provide a new one completely.  For example: how does your approach formalize the concept of ""questions"" and ""answers"" ? What makes a question ""difficult"" ? How do you quantify ""difficulty"" ? How do you define the ""cost of information""? What are its units (bits, scalar reward), its semantics ? Do you you have an MDP or a POMDP ? What kind of MDP do you consider ? How do you define your discounted MDP ? What is the state and action spaces ? Some important problem structure under the ""interaction/labeling/reward"" paragraph of section 2 would be worth expressing directly in your definition of the MDP: labeling actions can only occur during the ""labeling phase"" and that the transition and reward functions have a specific structure (positive/negative, lead to absorbing state). The notion of ""phase"" could perhaps be implemented by considering an augmented state space : $\tilde s = (s, phase)$ This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.  The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios.  While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.  I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea.    The followings are some detailed questions (not directly impacting my overall rating): (1) Page 2 ""we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties."": why does one ""must"" interact with objects in order to learn about the properties? Can't we also learn through observation?  (2) Figure 1right is missing a Y-axis label.  (3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.  (4) Page 5 ""which makes distinguishing between the two heaviest blocks very difficult"": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?  (5) Page 5 ""Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train."": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion.  (6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?  (7) Any baseline approach?",0,377
"The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.  The model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.   overview: This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch. That is, when an action sequence under-appreciates its reward, its log-probability is increased. This method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \tau, and intuitively provides us with a better exploration mechanism than \epsilon-greedy or random exploration. The method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.   remarks: - the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method. - in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6. - approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice. - an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.  opinion: - An interesting approach to policy-gradient, to be sure. It tackles the very important question of ""how should agents explore?"" - I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here) - I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6). - It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015) - At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.   The methodology and reasoning is clearly explained and I think this paper communicates its message very well. That message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL. The experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.  I realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration. Reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.  This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation.  Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.  -------------------------- After rebuttal: I missed the action sequences argument when I pointed about small action space issue.  For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014.  I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline. ",0,378
"The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.   Pros:  - significant speed improvements through dynamic batching - source code provided   Cons:  - the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context - presentation/vizualisation can be improved The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.  The presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (""A combinator library for neural networks"") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the ""Stanford Sentiment Treebank"" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble ""[...] variant sets a new state-of-the-art on both subtasks"" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.  Update on Jan. 17th: after the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it.  They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN.  In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1.  The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.",1,379
"The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).  The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.  The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.  To this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?  Overall, a clearly written paper. I vote for acceptance.  As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?  The authors present a method for changing the objective of generative adversarial networks such that the discriminator accurately recovers density information about the underlying data distribution. In the course of deriving the changed objective they prove that stability of the discriminator is not guaranteed in the standard GAN setup but can be recovered via an additional entropy regularization term.  The paper is clearly written, including the theoretical derivation. The derivation of the additional regularization term seems valid and is well explained. The experiments also empirically seem to support the claim that the proposed changed objective results in a ""better"" discriminator. There are only a few issues with the paper in its current form: - The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN. As a result it took me several passes through the paper to understand why the results don't hold for a standard GAN. I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below). - The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training. You also don't make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment. It might be worth-while mentioning this early in the paper. - The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets. I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance. For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data). Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories). - Main complaint: It is completely unclear what the generator and discriminators look like for the experiments. You mention that code will be available soon but I feel like a short description at least of the form of the energy used should also appear in the paper somewhere (perhaps in the appendix). This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data.  I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature.  Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 ""Finally, let's whose discriminative power"". I'm not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.",1,380
"Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.  Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.  It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).  Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.  [1] Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7).   (A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion: 1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone.  2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion.  It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation. It is possible that a modified criterion - eqn (7) + \lambda feature activation, (where \lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning.     (B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don't think this missing comparison is justified.   (C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary).  (D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets.   (E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations?   Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning: (1) Based on taylor expansion (2) Feature normalization to reduce parameter tuning efforts.  (3) Iterative finetuning.  However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept.         ",0,381
"Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.The paper looks solid and the idea is natural. Results seem promising as well.  I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.  I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.  If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.  Minor:  - ResNets should be mentioned in Table This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices.  Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10).  Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.  Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.",0,383
"SUMMARY. This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage. The proposed model combines two well-know neural network architectures match-lstm and pointer nets. First the passage and the questions are encoded with a unidirectional LSTM. Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question. For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm. The same process is done in the opposite direction with a backward lstm. The final representation is a concatenation of the two lstms. As a decoded a pointer network is used. The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.  The proposed model is tested on the Stanford Question Answering Dataset. An ensemble of the proposed model achieves performance close to state-of-the-art models.   ----------  OVERALL JUDGMENT  I think the model is interesting mainly because of the use of pointer networks as a decoder. One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm. The analysis of the model is interesting and insightful. The sharing of the code is good.Summary: The paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.  Strengths: 1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features. 2. Significant performance boost over the baseline presented in the SQuAD paper. 3. Some insightful analyses of the results such as performance is better when answers are short, ""why"" questions are difficult to answer.  Weaknesses/Questions/Suggestions: 1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer. 2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer. 3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required. 4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension?  5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1. 6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?  Review Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.  Strength: -	The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results) -	The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.   Weaknesses: 1.	It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text. 2.	Experimental evaluation 2.1.	It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance. 2.2.	It would be interested if this approach generalizes to other datasets.   Other (minor/discussion points) -	The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction. -	I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process. -	Page 6, last paragraph: missing “.”: “… searching This…”    Summary: While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task. ",0,384
"This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).  The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.  My only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.  All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.   The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.  The idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.  An important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation. The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps.  The paper claims that this  a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method b) improves performance on texture inpainting tasks compared to the Gatys et al. method c) improves results in season transfer when combined with the style transfer method by Gatys et al.  Furthermore the paper shows that d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.  I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (",0,387
"This paper proposed a dynamic coattention network for the question answering task with long contextual documents.  The model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer.   Overall, this is a well-written paper.  Although the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented.  Also the performance on the SQuAD dataset is good.  I would recommend this paper to be accepted.  Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.  Strengths:  1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.  2. The proposed model architecture is novel and the design choices made seem reasonable.  3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.  4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.  Weaknesses/Questions/Suggestions:  1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.  2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?  3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).  4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.  5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn’t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang’s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.  6. In section 2.1, “n” and “m” are swapped when explaining the Document and Question encoding matrix. Please fix it.  Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).  Paper Summary:  The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models.  Paper Strengths:  -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described.  Paper Weaknesses / Future Thoughts:  -- The paper provides model's performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc?  Preliminary Evaluation:  Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.",0,388
"Pros: The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time. RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples. Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.  Cons: The paper is lacking equations that detail the model. This can be remedied in the camera-ready version. The paper is lacking detailed explanations of the modeling choices: - It's not clear why an MLP is used in the bottom layer instead of (another) RNN. - It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation.  As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable.   Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution.  The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.  It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.  The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.  The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.  Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.  Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.  Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.    Other remarks:  - upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise?  - The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?  - Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation. The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests.   As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version.  The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better. ",1,389
"This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory « experts » is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations (« pondering ») even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.  The paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.  Here are a few specific comments, questions and suggestions:  1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style — do these mean different things?   2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the « bottom row » part. This makes understanding this figure difficult.  3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2).   4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?  5) the supplementary materials are very helpful. Thank you for all these details. Pros (quality, clarity, originality, significance:):  This paper presents a novel metacontroller optimization system that learns the best action for a one-shot learning task, but as a framework has the potential for wider application. The metacontroller is a model-free reinforcement learning agent that selects how many optimization iterations and what function or “expert” to consult from a fixed set (such as an action-value or state transition function). Experimental results are presented from simulation experiments where a spacecraft must fire its thruster once to reach a target location, in the presence of between 1 and 5 heavy bodies.  The metacontroller system has a similar performance loss on the one-shot learning task as an iterative (standard) optimization procedure. However, by taking into account the computational complexity of running a classical, iterative optimization procedure as a second “resource loss” term, the metacontroller is shown to be more efficient. Moreover, the metacontroller agent successfully selects the optimal expert to consult, rather than relying on an informed choice by a domain-expert model designer. The experimental performance is a contribution that merits publication, and it also exhibits the use of an interaction network for learning the dynamics of a simulated physical system. The dataset that has been developed for this task also has the potential to act as a new baseline for future work on one-shot physical control systems. The dataset constitutes an ancillary contribution which could positively impact future research in this area.  Cons: It's not clear how this approach could be applied more broadly to other types of optimization. Moreover, the REINFORCE gradient estimation method is known to suffer from very high variance, yielding poor estimates. I'm curious what methods were used to ameliorate these problems and if any other performance tricks were necessary to train well. Content of this type this could form a useful additional appendix.  A few critiques on the communication of results:  - The formal explication of the paper’s content is clear, but Fig.’s 1A and 3 could be improved. Fig. 1A is missing a clear visual demarcation of what exactly the metacontroller agent is. Have you considered a plate or bounding box around the corresponding components? This would likely speed the uptake of the formal description.  - Fig. 3 is generally clear, but the lack of x-axis tick marks on any subplots makes it more challenging than necessary to compare among the experts. Also, the overlap among the points and confidence intervals in the upper-left subplot interferes with the quantitative meaning of those symbols. Perhaps thinner bars of different colors would help here. Moreover, this figure lacks a legend and so the different lines are impossible to compare with each other.  - Lastly, the second sentence in Appendix B. 2 is a typo and terminates without completion.A well written paper and an interesting construction - I thoroughly enjoyed reading it.   I found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well.  In Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework.  Experimental results were presented clearly and well illustrated the usefulness of the metacontroller. I'm curious to see the results of using more metaexperts.",0,390
"Updated review: 18 Jan. 2017  Thanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012.  The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the ""RNN Sparse 1760"" result in Table 3.  I have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.  This paper has three main contributions.  (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training.  (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model.  (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.  Pros + The paper is mostly clear and easy to understand. + The paper tackles an important, practical problem in deep learning:  how to successfully deploy models at the lowest possible computational and memory cost.  Cons - As a second baseline, this paper should compare to ""distillation"" approaches (e.g., Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate.   Pros: Proposes a pruning method that doesn’t need re-training and doesn’t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.  Cons & Questions: Judiciously choosing hyper parameters for different models and different applications wouldn’t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)  In table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can’t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)  Why sparsity for table3 and table5 are different? In text: “average sparsity of 88%” but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed) The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.  The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.  Overall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn’t motivate the details of the method very well. It’s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.  Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.  Improving scalability is an important driving force of the progress in neural network research. While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.   Pros: The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance. The experiments are done using a state-of-the-art system for a practical application.  Cons: The proposed method is very similar to earlier work and barely novel. There is no comparison with other pruning methods. The data is private and this prevents others from replicating the results.   [1] Jaeger, H. (2001). The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.   [2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.",0,391
"The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.  Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---  1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?  2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?  3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.  Pros: + The paper is clear and well-written. + The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution. + The proposed approaches to quantization and rate estimation are sensible and well-justified.  Cons: - The experimental baselines do not appear to be entirely complete.  The task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.  I have no further specific comments at this time as they were answered sufficiently in the pre-review questions.This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field.  Pros: + Very clear paper. It should be possible to replicate these results should one be inclined to do so. + The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression. It's definitely better than other neural network approaches to compression, though.  Cons: - The training procedure seems clunky. It requires multiple training stages, freezing weights, etc. - The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)",1,392
"The authors propose to extend the “standard” attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.  The paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the “structured attention” models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the “simple” attention models, but the techniques are nonetheless interesting.  Main comments: 1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I’m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric. 2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?  Minor comments: 1. Typographical error: Equation 1: “p(z | x, q” → “p(z | x, q)” 2. Section 3.3: “Past work has demonstrated that the techniques necessary for this approach, … ” →  “Past work has demonstrated the techniques necessary for this approach, … ” This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.  The paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them.   This seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?  Minor comments/typos: - last paragraph of sec 1: ""standard attention attention"" - third paragraph of sec 3.2: ""the on log-potentials"" - sec 4.1, Results: ""... as it has no information about the source ordering"" -- what do you mean here?This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network.  Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.  The experiments of the paper prove the usefulness of the model in various level — seq2seq and tree structure etc. I think it’s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model.  In sum, I think this is a solid contribution and the approach will benefit the research in other problems. ",1,393
"This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.  This is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.Paper Summary This paper proposes a variant of dropout, applicable to RNNs, in which the state of a unit is randomly retained, as opposed to being set to zero. This provides noise which gives the regularization effect, but also prevents loss of information over time, in fact making it easier to send gradients back because they can flow right through the identity connections without attenuation. Experiments show that this model works quite well. It is still worse that variational dropout on Penn Tree bank language modeling task, but given the simplicity of the idea it is likely to become widely useful.  Strengths - Simple idea that works well. - Detailed experiments help understand the effects of the zoneout probabilities   and validate its applicability to different tasks/domains.  Weaknesses - Does not beat variational dropout (but maybe better hyper-parameter tuning   will help).  Quality The experimental design and writeup is high quality.  Clarity The paper clear and well written, experimental details seem adequate.  Originality The proposed idea is novel.  Significance This paper will be of interest to anyone working with RNNs (which is a large group of people!).  Minor suggestion- - As the authors mention - Zoneout has two things working for it - the noise and   the ability to pass gradients back without decay. It might help to tease apart the contribution from these two factors. For example, if we use a fixed mask over the unrolled network (different at each time step) instead of resampling it again for every training case, it would tell us how much help comes from the identity connections alone.The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.  Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.  The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.  Overall, the paper bears great potential. However, I do see some points.  1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.  I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here. Zoneout does not seem to improve that much in the other tasks.  2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.  3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.   An extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)  Consequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”. ",1,394
"Thank you for an interesting read.  I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.  The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?The paper introduces Edward, a probabilistic programming language built over TensorFlow and Python, and supporting a broad range of most popular contemporary methods in probabilistic machine learning.   Quality:  The Edward library provides an extremely impressive collection of modern probabilistic inference methods in an easily usable form. The paper provides a brief review of the most important techniques especially from a representation learning perspective, combined with two experiments on implementing various modern variational inference methods and GPU-accelerated HMC.  The first experiment (variational inference) would be more valuable if there was a clear link to complete code to reproduce the results provided. The HMC experiment looks OK, except the characterising Stan as a hand-optimised implementation seems unfair as the code is clearly not hand-optimised for this specific model and hardware configuration. I do not think anyone doubts the quality of your implementation, so please do not ruin the picture by unsubstantiated sensationalist claims. Instead of current drama, I would suggest comparing head-to-head against Stan on single core and separately reporting the extra speedups you gain from parallelisation and GPU. These numbers would also help the readers to estimate the performance of the method for other hardware configurations.   Clarity:  The paper is in general clearly written and easy to read. The numerous code examples are helpful, but also difficult as it is sometimes unclear what is missing. It would be very helpful if the authors could provide and clearly link to a machine-readable companion (a Jupyter notebook would be great, but even text or HTML would be easier to copy-paste from than a pdf like the paper) with complete runnable code for all the examples.   Originality:  The Edward library is clearly a unique collection of probabilistic inference methods. In terms of the paper, the main threat to novelty comes from previous publications of the same group. The main paper refers to Tran et al. (2016a) which covers a lot of similar material, although from a different perspective. It is unclear if the other paper has been published or submitted somewhere and if so, where.   Significance:  It seems very likely Edward will have a profound impact on the field of Bayesian machine learning and deep learning.   Other comments:  In Sec. 2 you draw a clear distinction between specialised languages (including Stan) and Turing-complete languages such as Edward. This seems unfair as I believe Stan is also Turing complete. Additionally no proof is provided to support the Turing-completeness of Edward. The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling.   However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF.  Presentation: * Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter. * I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example. It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper.   Experiments: * Why is the run time not reported in table 1? * What are the ""difficulties around convergence"" encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox?  * Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported.  * How difficult is it to get the inference to work (eg HMC) when we don't have full control over the computational graph structure and sampler? * It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models. * What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example?  Minor comments: * Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I'm not sure why alpha=-1 was chosen here.  * How do you handle discrete distributions (eg Figure 5)? * x_real is not defined in Figure 7. * I would suggest highlighting M in Figure 8. * Comma instead of period after ""rized), In"" on page 8.  In conclusion I would say that the software developments presented here are quite exciting, and I'm glad the authors are pushing towards practical and accessible ""inference for all"". In its current form though I am forced to give the submission itself a score of 5.",1,395
"The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.  I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary.  - Is the mask a binary mask or a alpha blending mask? - I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.  The proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).  Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.  Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.  - What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as ""advertised"".  Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.  I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs. The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.  The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit.  It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image. This is obviously a reasonable approach there is clearly a foreground object. However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer. How would the proposed method deal with such situations?  Overall, this is a reasonable work that approaches an important problem from a new angle. Yet, I think sizable efforts remain needed to make it a generic methodology. ",1,396
"This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel. The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though. The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent. This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary.This paper introduces the notion of a ""variational lossy autoencoder"", where a powerful autoregressive conditional distribution on the inputs x given the latent code z is crippled in a way that forces it to use z in a meaningful way. Its three main contributions are:  (1) It gives an interesting information-theoretical insight as to why VAE-type models don't tend to take advantage of their latent representation when the conditional distribution on x given z is powerful enough.  (2) It shows that this insight can be used to efficiently train VAEs with powerful autoregressive conditional distributions such that they make use of the latent code.  (3) It presents a powerful way to parametrize the prior in the form of an autoregressive flow transformation which is equivalent to using an inverse autoregressive flow transformation on the approximate posterior.  By itself, I think the information-theoretical explanation of why VAEs do not use their latent code when the conditional distribution on x given z is powerful enough constitutes an excellent addition to our understanding of VAE-related approaches.  However, the way this intuition is empirically evaluated is a bit weak. The ""crippling"" method used feels hand-crafted and very task-dependent, and the qualitative evaluation of the ""lossyness"" of the learned representation is carried out on three datasets (MNIST, OMNIGLOT and Caltech-101 Silhouettes) which feature black-and-white images with little-to-no texture. Figures 1a and 2a do show that reconstructions discard low-level information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets. Have the authors tried applying VLAE to such datasets?  I think the Caltech101 Silhouettes benchmark should be treated with caution, as no comparison is made against other competitive approaches like IAF VAE, PixelRNN and Conv DRAW. This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined.  A question which is very relevant to this paper is ""Does a latent representation on top of an autoregressive model help improve the density modeling performance?"" The paper touches this question, but very briefly: the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin.  The proposal to transform the latent code with an autoregressive flow which is equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation is also interesting. There is, however, one important distinction to be made between the two approaches: in the former, the prior over the latent code can potentially be very complex whereas in the latter the prior is limited to be a simple, factorized distribution.  It is not clear to me that having a very powerful prior is necessarily a good thing from a representation learning point of view: oftentimes we are interested in learning a representation of the data distribution which is untangled and composed of roughly independent factors of variation. The degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a high-level representation certainly warrants some thought. I would be interested in hearing the authors' opinion on this.  Overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance.  UPDATE: The rating has been revised to a 7 following the authors' reply.This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data. This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder. To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure.  The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored. As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used. Then, they propose two complementary approaches to force the latent variables to be used by the decoder. The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency. The second is to parametrize the prior distribution over the latent code with an autoregressive model.  They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes.  Review: The bits-Back interpretation of VAE is a nice contribution to the community. Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved.  Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications. For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have.  However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2). However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used?  Also, it is not clear if WindowAround(i) represents only a subset of x_{",1,397
"This paper poses an interesting idea: removing chaotic behavior or RNNs. While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.  Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour.   Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?  It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?  Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.  The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.  This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.  The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.",0,398
"This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.  Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.  An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.  Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.    Small comment: I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines.Paper Strengths:  -- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner  -- The effective batch size for training the MoE drastically increased also  -- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.   Paper Weaknesses:  --- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors. This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load).  Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs. Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.  I have the several comments on the paper: - I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1.  - The paper is missing some important references in conditional computation (e.g. ",0,399
"Authors' response well answered my questions. Thanks.  Evaluation not changed.  ###  This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset.   There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence.   Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding.   On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?  The paper is well written, except for minor typo as mentioned in my pre-review questions.   In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it. The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches — (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).  The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.  I think the recovering synthetic tree task is not very satisfying for two reasons — (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can’t show its full potentials since the length of the information flow in the model won’t be very long.  I think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.  One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture.  A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice.  I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.",0,400
"EDIT: Updated score. See additional comment.  I quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.  However the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:  - The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.  - A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.  - An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.  - I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.  - The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does ""didn't work"" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.  - A question about jump points:  The I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.  Since the main idea is very interesting, I will be happy to update my score if the above concerns are addressed. The paper reads well and the idea is new. Sadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing.  The training of the introspection network could have been described in more detail.  Also, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments. That would have made the results more convincing. Due to the novelty of the idea, I recommend the paper. I would increase the rating if an updated draft addresses the mentioned issues.In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.    Pros: + The organization is generally very clear + Novel meta-learning approach that is different than the previous learning to learn approach  Cons:  - The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.   - Neither MNIST nor CIFAR experimental section explained the architectural details - Mini-batch size for the experiments were not included in the paper - Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method.   Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. ",1,401
"This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme. The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time. Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources. In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model. The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.  Overall I think this paper is a good contribution to the hyperparameter optimization literature. It’s relatively simple to implement, and seems to be quite effective for many problems. It seems like a natural extension of the random search methodology to the case of early stopping. To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient. I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).  I’m not sure I agree with the use of random2x as a baseline. I can see why it’s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions. I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc. I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines. This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.  Having read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method? I hope to get a response by the authors and see this made clearer in an updated version of the paper.  In terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.)  The experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: ""Multi-Task Bayesian Optimization"" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups.   Given that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says ""configuration evaluation"" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.  As another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: ""Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation"" (This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.  Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I'm excited to try it. I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (",1,402
"The paper proposes a new memory access scheme based on Lie group actions for NTMs.  Pros: * Well written * Novel addressing scheme as an extension to NTM. * Seems to work slightly better than normal NTMs. * Some interesting theory about the novel addressing scheme based on Lie groups.  Cons: * In the results, the LANTM only seems to be slightly better than the normal NTM. * The result tables are a bit confusing. * No source code available. * The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme. * It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here. * No tests on real-world tasks, only some toy tasks. * No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.  The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences. Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.  The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.  This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.  Overall, the paper is well communicated and a novel idea.  The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.  The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.  Minor issues: Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix. Figures on page 8 are difficult to follow. The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups.  This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes.  Pros: - novel and interesting idea for memory access - nicely written   Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory)                                  - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks) ",0,403
" This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.  - I would encourage the authors to improve the explanation of the model.  - Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from.  - Otherwise the experiments seem adequate and I enjoyed this paper.  This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings. This paper introduces a novel RNN architecture named QRNN.  QNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input. Consequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.  Various extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.  Authors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation).   Overall the paper is an enjoyable read and the proposed approach is interesting, Pros: - Address an important problem - Nice empirical evaluation showing the benefit of their approach - Demonstrate up to 16x speed-up relatively to a LSTM Cons: - Somewhat incremental novelty compared to (Balduzizi et al., 2016)  Few specific questions: - Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?   - How does the i-fo-ifo pooling perform comparatively?  - How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step).   As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.  The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others.   While I feel that the contribution is somewhat incremental, I recommend acceptance.  ",0,404
"The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art. The original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future, and their inability to jump directly to a future prediction without iterating through all intermediate states. The authors have provided an extensive experimental evaluation on several benchmarks with promising results. In general the paper is well written and quite clear in its explanations. Demonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.  # Minor comments: `jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper.  # Typos Section 3.1 - `this configuration is all experiments'The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. The paper claims three main contributions:  1. modification to model architecture (used in Oh et al.) by using action at time t-1 to directly predict hidden state at t 2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames) 3. exploring different training schemes (trade-off between observation and prediction frames for training LSTM)  1. modification to model architecture + The motivation seems good that in past work (Oh et al.) the action at t-1 influences x_t, but not the state h_t of the LSTM. This could be fixed by making the LSTM state h_t dependent on a_{t-1} - However, this is of minor technical novelty. Also, as pointed in reviewer questions, a similar effect could be achieved by adding a_t-1 as an input to the LSTM at time t. This could be done without modifying the LSTM architecture as stated in the paper. While the authors claim that combining a_t-1 with h_t-1 and s_t-1 performs worse than the current method which combines a_t-1 only with h_t-1, I would have liked to see the empirical difference in combining a_t-1 only with s_t-1 or only with h_t-1. Also, a stronger motivation is required to support the current formulation. - Further, the benefits of this change in architecture is not well analyzed in experiments. Fig. 5(a) provides the difference between Oh et al. (with traditional LSTM) and current method. However, the performance difference is composed of 2 components (difference in training scheme and architecture). This contribution of the architecture to the performance is not clear from this experiment. The authors did claim in the pre-review phase that Fig. 12 (a) shows the difference in performance only due to architecture for ""Seaquest"". However, from this plot it appears that the gain at 100-steps (~15)  is only a small fraction of the overall gain in Fig. 5 (a) (~90). It is difficult to judge the significance of the architecture modification from this result for one game.  2. Exploring the idea of jumpy predictions: + As stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations. + The results in Fig. 5(b) present some interesting observations that omitting intermediate frames does not lead to significant error-increase for at least a few games. - However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al. - While, the observations themselves are interesting, it would have been better to provide a more detailed analysis for more games. Also, the novelty in dropping intermediate frames for speedup is marginal.  3. Exploring different training schemes + This is perhaps the most interesting observation presented in the paper. The authors present the difference in performance for different training schemes in Fig. 2(a). The training schemes are varied based on the fraction of training phase which only uses observation frames and the fraction that uses only prediction frames. + The results show that this change in training can significantly affect prediction results and is the biggest contributor to performance improvement compared to Oh et al. - While this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.  Clarity of presentation: - The exact experimental setup is not clearly stated for some of the results. For instance, the paper does not say that Fig. 2(a) uses the same architecture as Oh et al. However, this is stated in the response to reviewer questions. - Fig. 4 is difficult to interpret. The qualitative difference between Oh et al. and current method could be highlighted explicitly.  - Minor: The qualitative analysis section requires the reader to navigate to various video-links in order to understand the section. This leads to a discontinuity in reading and is particularly difficult while reading a printed-copy.  Overall, the paper presents some interesting experimental observations. However, the technical novelty and contribution of the proposed architecture and training scheme is not clear.[UPDATE] After going through the response from the author and the revision, I increased my review score for two reasons. 1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it. This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions. It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).  2. The revised paper contains more comprehensive results than before. The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.  - Summary This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.  - Novelty The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area.   - Experiment The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.  - Clarity The paper is well-written and easy to follow.  - Overall  Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.  [Reference] Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.",0,405
"The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.  Overall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don’t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers. Paper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain.   > Significance  Paper addresses and important and significant problem. The approach taken in addressing it is also interesting   > Clarity  Paper is well written, but does require domain knowledge to understand.   My main concerns were well addressed by the rebuttal and corresponding revisions to the paper. This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.  That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.  ",0,406
"The paper tackles important problems in multi-task reinforcement learning: avoid negative transfer and allow finer selective transfer. The method is based on soft attention mechanism, very general, and demonstrated to be applicable in both policy gradient and value iteration methods. The introduction of base network allows learning new policy if the prior policies aren't directly applicable. State-dependent sub policy selection allows finer control and can be thought of assigning state space to different sub policies/experts. The tasks are relatively simplistic but sufficient to demonstrate the benefits. One limitation is that the method is simple and the results/claims are mostly empirical. It would be interesting to see extensions to option-based framework, stochastic hard attention mechanism, sub-policy pruning, progressive networks.   In figure 6, the read curve seems to perform worse than the rest in terms of final performance. Perhaps alternative information to put with figures is the attention mask activation statistics during learning, so that we may observe that it learns to turn off adversarial sub-policies and rely on newly learned base policy mostly. This is also generally good to check to see if any weird co-adaptation is happening. This paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer. The proposed approach is based on a convex combination of existing solutions and the being-learned solution to the novel task. The non-negative weight of each solution implies that the solution of negative effect is ignored and more weights are allocated to more relevant solution in each state. This paper derives this so-called ""A2T"" learning algorithm for policy transfer and value transfer for REINFORCE and ACTOR-CRITIC algorithms and experiments with synthetic Chain World and Puddle World simulation and Atari 2600 game Pong.  +This paper presents a novel approach for transfer reinforcement learning. +The experiments are cleverly designed to demonstrate the ability of the proposed method. -An important aspect of transfer learning is that the algorithm can automatically figure out if the existing solutions to known tasks are sufficient to solve the novel task so that it can save the time and energy of learning-from-scratch. This issue is not studied in this paper as most of experiments have a learning-from-scratch solution as base network. It will be interesting to see how well the algorithm performs without base network. In addition, from Figure 3, 5 and 6, the proposed algorithm seems to accelerate the learning speed, but the overall network seems not better than the solo base network. It will be more convincing to show some example that existing solutions are complementary to the base network. -If ignoring the base network, the proposed network can be considered as ensemble reinforcement learning that take advantages of learned agents with different expertise to solve the novel task. In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably. One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.   Pros: The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work. The experiments are good proofs of concept, but do not go beyond that i.m.h.o.  Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).  Cons: As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell. The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016). Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task. Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.  ",0,407
"this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.  although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions: - investigating the use of fairly known architecture on a new domain. - providing novel objectives specific to the domain - setting up new benchmarks designed for evaluating multi-view models  I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption. The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.  There's a lot of discussion of character edit distance relative to acoustic span similarity. It seems very natural to also include phoneme string edit distance in this discussion and experiments. This is especially true of the word similarity test. Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances. Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings. I believe the last function could remain identical it's just swapping out characters for phones as the symbol set.  finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.   the vocabulary size and training data amount make this really a toy problem. although there are many pairs constructed most of those pairs will be easy distinctions. the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.   it seems this approach is unable to address the task of keyword spotting in longer spoken utterances. If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations. The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand Pros:   Interesting training criterion. Cons:   Missing proper ASR technique based baselines.  Comments:   The dataset is quite small.   ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.   More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection   performance of out-of-vocabulary words.   It would be interesting to show scatter plots for embedding vs. orthographic distances. ",0,408
"The paper introduces a new dataset called MusicNet (presumably analogous to ImageNet), featuring dense ground truth labels for 30+ hours of classical music, which is provided as raw audio. Such a dataset is extremely valuable for music information retrieval (MIR) research and a dataset of this size has never before been publicly available. It has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough. The paper is clear and well-written.  The paper also features some ""example"" experiments using the dataset, which I am somewhat less excited about. The authors decided to focus on one single task that is not particularly challenging: identifying pitches in isolated segments of audio. Pitch information is a fairly low-level characteristic of music. Considering that isolated fragments are used as input, this is a relatively simple problem that probably doesn't even require machine learning to solve adequately, e.g. peak picking on a spectral representation could already get you pretty far. It's not clear what value the machine learning component in the proposed approach actually adds, if any. I could be wrong about this as I haven't done the comparison myself, but I think the burden is on the authors to demonstrate that using ML here is actually useful.  I would argue that one of the strenghts of the dataset is the variety of label information it provides, so a much more convincing setup would have been to demonstrate many different prediction tasks for both low-level (e.g. pitch, onsets) and high-level (e.g. composer) characteristics, perhaps with fewer and simpler models -- maybe even sticking to spectrogram input and forgoing raw audio input for the time being, as this comparison seems orthogonal to the introduction of the dataset itself. As it stands, I feel that the fact that the experiments are relatively uninteresting detracts from the main point of the paper, which is to introduce a new public dataset that is truly unique in terms of its scale and scope.  That said, the experiments seem to have been conducted in a rigorous fashion and the evaluation and analysis of the resulting models is properly executed.  Re: Section 4.5, it is rather unsurprising to me that a pitch detector would learn filters that resemble pitches (i.e. sinusoids), although the observation that this requires a relatively large amount of data is interesting. However, it would be more interesting to demonstrate that this is also the case for higher-level tasks. The authors favourably compare the features learnt by their model with prior work on end-to-end learning from raw audio, but neglect that the tasks considered in this work were much more high-level.  Some might also question whether ICLR is the appropriate venue to introduce a new dataset, but personally I think it's a great idea to submit it here, seeing as it will reach the right people. I suppose this is up to the organisers and the program committee, but I thought it important to mention this, because I don't think this paper merits acceptance based on its experimental results alone.This paper describes the creation of a corpus of freely-licensed classical music recordings along with corresponding MIDI-scores aligned to the audio.  It also describes experiments in polyphonic transcription using various deep learning approaches, which show promising results.  The paper is a little disorganised and somewhat contradictory in parts. For example, I find the first sentence in section 2 (MusicNet) would better be pushed one paragraph below so that the section be allowed to begin with a survey of the tools available to researchers in music. Also, the description for Table 3 should probably appear somewhere in the Methods section. Last example: the abstract/intro says the purpose is note prediction; later (4th paragraph of intro) there's a claim that the focus is ""learning low-level features of music...."" I find this slightly disorienting.  Although others (Uehara et al., 2016, for example) have discussed collection platforms and corpora, this work is interesting because of its size and the approach for generating features. I'm interested in what the authors will to do expand the offerings in the corpus, both in terms of volume and diversity. This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough ""hours"" of music, and where genre has been well constrained in order to allow for sufficient homogeneity in the data to help ensure usefulness. This is great for the community.  The description of the validation of the dataset is interesting, and indicates a careful process was followed.  The authors provide just enough basic experiments to show that this dataset is big enough that good low-level features (i.e. expected sinusoidal variations) can indeed be learned in an end-to-end context.  One might argue that in terms of learning representations, the work presented here contributes more in the dataset than in the experiments or techniques used. However, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, I feel that this contribution carries substantial weight in terms of expected future rewards. (If research groups were making great new datasets available on a regular basis, that would place this in a different context. But so far, that is not the case.) In otherwords, while the experiments/techniques are not necessarily in the top 50% of accepted papers (per the review criteria), I am guessing that the dataset is in the top 15% or better.",0,409
"The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty.   My comments appear below: 1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold).  2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases).  3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?)  4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required?  5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.  It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.  It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.  In section 4, the description of the auxiliary decoder setup might benefit from more detail.  There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below.  1. Ogawa, Tetsuji, et al. ""Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation."" Proceedings of ICASSP. 2015.  2. Hermansky, Hynek, et al. ""Towards machines that know when they do not know."" Proceedings of ICASSP, 2015.  3. Variani, Ehsan et al. ""Multi-stream recognition of noisy speech with performance monitoring."" INTERSPEECH. 2013.The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.",1,410
"Two things I really liked about this paper: 1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.  2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.  The argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.  Nonetheless, I give this paper an ""accept"", because I learned something valuable and the results are very good. This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input. The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.  The writing is clear and results highlight the efficacy of the method.  comments / questions: - Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned. The rest is still effectively the stoke MCMC scheme? Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )  - Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).  - In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion. The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms.  This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines.  The significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work.  The proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough.  It is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance.",0,411
"This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.  Overall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.  Of course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).  The experiment is a bit weak. 1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.  2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place.  A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.  Summary: ——— I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.  Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is reasonable albeit heuristics are required. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.  Details: ———— 1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.  2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing.   Pros:  - To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis. - The paper is well-written and easy to follow.   Cons:  - Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling.  	 - The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, ",0,412
"This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community.   As an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbrück, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.  I guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and  Most of my questions have already been answers in the pre-review period. My only question remaining is on page 3, “It should be noted that when we refer to “temporal differences”, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.”  This does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.  Figure 1 should be made bigger.  An improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience? The paper presents a method to improve the efficiency of CNNs that encode sequential inputs in a ‘slow’ fashion such that there is only a small change between the representation of adjacent steps in the sequence. It demonstrates theoretical performance improvements for toy video data (temporal mnist) and natural movies with a powerful Deep CNN (VGG).   The improvement is naturally limited by the ‘slowness’ of the CNN representation that is transformed into a sigma-delta network: CNNs that are specifically designed to have ‘slow’ representations will benefit most. Also, it is likely that only specialised hardware can fully harness the improved efficiency achieved by the proposed method. Thus as of now, the full potential of the method cannot be thoroughly evaluated. However, since the processing of sequential data seems to be a broad and general area of application, it is conceivable that this work will be useful in the design and application of future CNNs.  All in all, this paper introduces an interesting idea to address an important topic. It shows promising initial results, but the demonstration of the actual usefulness and relevance of the presented method relies on future work. This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs. The paper was clearly written, the approach is clever, and it's neat to see a practical algorithm driven by what is essentially a spiking network. The benefits of this approach are still more theoretical than practical -- it seems unlikely to be worthwhile to do this on current hardware. I strongly suspect that if deep networks were trained with an appropriate sparse slowness penalty, the reduction in computation would be much larger.",1,413
"Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation.   Orthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.  Although the improvement in performances is not significant the direction of research and the observations made are promising.The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as ""local"" and ""global"" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.  The experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.  One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as ""features"". However, the authors have already said they will address this.  The paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.  In response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the ""multi-bias"" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for ""multi-bias"", seeing as the weights are shared across sets of correlated features.  The dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).  Overall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ",0,415
"The authors propose a new approach for estimating maximum entropy distributions subject to expectation constraints. Their approach is based on using normalizing flow networks to non-linearly transform samples from a tractable density function using invertible transformations. This allows access to the density of the resulting distribution. The parameters of the normalizing flow network are learned by maximizing a stochastic estimate of the entropy obtained by sampling and evaluating the log-density on the obtained samples. This stochastic optimization problem includes constraints on expectations with respect to samples from the normalizing flow network. These constraints are approximated in practice by sampling and are therefore stochastic. The optimization problem is solved by using the augmented Lagrangian method. The proposed method is validated on a toy problem with a Dirichlet distribution and on a financial problem involving the estimation of price changes from option price data.  Quality:  The paper seems to be technically sound. My only concern would the the approach followed to apply the augmented Lagrangian method when the objective and the constraints are stochastic. The authors propose their own solution to this problem, based on a hypothesis test, but I think it is likely that this has already been addressed before in the literature. It would be good if the authors could comment on this.  The experiments performed show that the proposed approach can outperform Gibbs sampling from the exact optimal distribution or at least be equivalent, with the advantage of having a closed form solution for the density.  I am concern about the difficulty of he problems considered. The Dirichlet distributions are relatively smooth and the distribution in the financial problem is one-dimensional (in this case you can use numerical methods to compute the normalization constant and plot the exact density). They seem to be very easy and do not show how the method would perform in more challenging settings: high-dimensions, more complicated non-linear constraints, etc...  Clarity:  The paper is clearly written and easy to follow.  Originality:  The proposed method is not very original since it is based on applying an existing technique (normalizing flow networks) to a specific problem: that of finding a maximum entropy distribution. The methodological contributions are almost non-existing. One could only mention the combination of the normalizing flow networks with the augmented Lagrangian method.   Significance:  The results seem to be significant in the sense that the authors are able to find densities of maximum entropy distributions, something which did not seem to be possible before. However, it is not clearly how useful this can be in practice. The problem that they address with real-world data (financial data) could have been solved as well by using 1-dimensional quadrature. The authors should consider more challenging problems which have a clear practical interest.  Minor comments:  More details should be given about how the plot in the bottom right of Figure 2 has been obtained.  ""a Dirichlet whose KL to the true p∗ is small"": what do you mean by this? Can you give more details on how you choose that Dirichlet?  I changed updated my review score after having a look at the last version of the paper submitted by the authors, which includes new experiments.Much existing deep learning literature focuses on likelihood based models. However maximum entropy approaches are an equally valid modelling scenario, where information is given in terms of constraints rather than data. That there is limited work in flexible maximum entropy neural models is surprising, but  is due to the fact that optimizing a maximum entropy model requires (a) establishing the effect of the constraints on some distribution, and formulating the entropy of that complex distribution. There is no unbiased estimator of entropy from samples alone, and so an explicit model for the density is needed. This challenge limits approaches. The authors have identified that invertible neural models provide a powerful class of models for solving the maximum entropy network problem, and this paper goes on to establish this approach. The contributions of this paper are (a) recognising that, because normalising flows provide an explicit model for the density, they can be used to provide unbiased estimators for the entropy (b) that the resulting Lagrangian can be implemented as a relaxation of a augmented Lagrangian (c) establishing the practical issues in doing the augmented Lagrangian optimization. As far as the reviewer is aware this work is novel – this approach is natural and sensible, and is demonstrated on an number of models where clear evaluation can be done. Enough experiments have been done to establish this is an appropriate method, though not that it is entirely necessary – it would be good to have an example where the benefits of the flexible flow transformation were much clearer. Further discussion of the computational and scaling aspects would be valuable. I am guessing this approach is probably appropriate for model learning, but less appropriate for inferential settings where a known model is then conditioned on particular instance based constraints? Some discussion of appropriate use cases would be good. The issue of match to the theory via the regularity conditions has been brought up, but it is clear that this can be described well, and exceeds most of the theoretical discussions that occur regarding the numerical methods in other papers in this field.  Quality: Good sound paper providing a novel basis for flexible maximum entropy models. Clarity: Good. Originality: Refreshing. Significance: Significant in model development terms. Whether it will be an oft-used method is not clear at this stage.  Minor issues  Please label all equations. Others might wish to refer to them even if you don’t. Top of page 4: algorithm 1→ Algorithm 1. The update for c to overcome stability appears slightly opaque and is mildly worrying.  I assume there are still residual stability issues? Can you comment on why this solves all the problems? The issue of the support of p is glossed over a little. Is the support in 5 an additional condition on the support of p? If so, that seems hard to encode, and indeed does not turn up in (6). I guess for a Gaussian p0 and invertible unbounded transformations, if the support happens to be R^d, then this is trivial, but for more general settings this seems to be an issue that you have not dealt? Indeed in your Dirichlet example, you explicitly map to the required support, but for more complex constraints this may be non trivial to do with invertible models with known Jacobian? It would be nice to include this in the more general treatment rather than just relegating it to the specific example.  Overall I am very pleased to see someone tackling this question with a very natural approach.This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.  The paper is clearly written and is easy to follow.  Novelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers. Applying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p_{\phi};f], where f is the unormalized likelihood f \propto exp \sum_k( - \lambda_k T - c_k ||T_k||^2  ). This type of derivation is typical in all previous works using NFs for variational inference. A few experiments on more complex data would strengthen the paper's results. The two experiments provided show good results but both of them are toy problems.   Minor point:  Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.",0,416
"The paper introduces a technique for stabilizing the training of Generative Adversrial Networks by unrolling the inner (discriminator) optimization in the GAN loss function several steps and optimizing the generator with respect to the final state of this optimization process. The experimental evidence that this actually helps is very compelling: the 2d example shows a toy problem where this technique helps substantially, the LSTM MNIST generator example shows that the procedure helps with stabilizing the training of an unusual architecture of generator, and the image generation experiment, while not being definitive, is very convincing. For future work it would be interesting to see whether a method with smaller memory requirements could be devised based on similar principles. I strongly recommend to accept this paper.The paper presents an approach for tackling the instability problem that is present in generative adversarial networks. The general idea is to allow the generator to ""peek ahead"" at how the discriminator will evolve its decision boundary over-time with the premise that this information should prevent the generator from collapsing to produce only samples from a single mode of the data distribution.  This is a very well written paper that clearly motivates its attack on an important open issue. The experiments are well carried out and strongly support the presented idea. The pursued approach is substantially more elegant than current existing ""hacks"" that are commonly used to make GANs work in practice. I however have three main issues that let me partly doubt the success of the method. If these can be resolved this paper is a clear candidate for acceptance.  1) I am not entirely convinced that the same effect cannot be obtained by the following procedure: simply train the discriminator for an extended number of K steps when updating the generator (say a number equivalent to the unrolling steps used in the current experiments) then, after the generator was updated undo the K updates to the discriminator and do 1 new update step instead. I only briefly glanced at your response to Reviewer2 which seems to imply you now tried something similar to this setup by stopping gradient flow at an appropriate point (although I think this is not exactly equivalent). 2) I tried to reproduce the simple MNIST example but using a fully connected network instead of an RNN generator without much success. Even when unrolling the discriminator for 30-40 steps the generator still engages in mode seeking behavior or does not train at all. This could either be because of a bug in my implementation or because of some peculiarities of the RNN generator or because I did not use batch normalization anywhere. If it is one of the latter two this would entail a dependence of the proposed approach on specific forms of the discriminator and generator and should be discussed. My code can be found here This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.  The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we're able to find such a z, then the generator can generate this particular training example. It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.  The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric.  There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: ",1,418
"This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.  The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.  Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).  Some questions: How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?  It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?  It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?  Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.  Minor comments: Below figure 2: GHz -> GB \Gamma is not defined.This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word.  Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD.  The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.  Some questions and comments: - In Table 2, how do you use LDA features for RNN (RNN LDA features)?  - I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM. - The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic ""trading""? What about the IMDB one? - How scalable is the proposed method for large vocabulary size (>10K)? - What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines. This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address:  1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper.   2 -  The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine)     Figure 2 colors very difficult to distinguish. ",1,419
"The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.  The experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it’s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn’t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this.   While memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I’m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies.   The exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.  The results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices).    Pros: Impressive and also interesting results. Good comparison with earlier work. The n-gram RNN is an interesting baseline.   Cons: The relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat. The model descriptions are not entirely clear. I would have liked to have seen what happens when the attention is applied to a much larger context size. This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.  The authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named ""George Abbot"", yet ""Abbot"" isn't mentioned again until the next sentence 40 tokens later, and then the next ""Abbot"" is 15 tokens from there. Most gaps between occurrences of ""Abbot"" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.  This is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions.This paper focusses on attention for neural language modeling and has two major contributions:  1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well. 2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.  The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.  I am convinced with authors’ responses for my pre-review questions.  Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well. ",1,420
"This paper presents a novel model for unsupervised segmentation and classification of time series data.  A recurrent hidden semi-markov model is proposed.  This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.  Each latent class has its own RNN for modeling observations for that category.  Further, an efficient training procedure based on a variational approximation.  Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.  This is an interesting and novel paper.  The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.  The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.  The method is technically sound and demonstrated to be effective.  It would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).  CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.  Regardless, I think the merits of using RNNs for the class-specific generative models are clear. Putting the score for now, will post the full review tomorrow.This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea.   That being said, I think two aspects may need further improvement: (1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification). (2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.",0,421
"The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables. The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ? After rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a.  Then paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.  The proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I’d recommend acceptance.  The SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows.   Is there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?  Since the SVAE code is publicly available, one could probably compare against it in the experiments.   I’m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn’t what’s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn’t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)? This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.  My compliments for the amount of detail put in the paper and appendix.  The experiments are on toy examples, but show promise.  - Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).  - Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”. This seems rather trivial to achieve.  - Eq 9: “This interpretation implies the factorization of the recognition model:..” The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)",0,422
"In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator.  My main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a ’soft-discriminator’. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier.  Overall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.The paper extends the GAN framework to accommodate multiple discriminators. The authors motivate this from two points of view:  (1) Having multiple discriminators tackle the task is equivalent to optimizing the value function using random restarts, which can potentially help optimization given the nonconvexity of the value function.  (2) Having multiple discriminators can help overcome the optimization problems arising when a discriminator is too harsh a critic. A generator receiving signal from multiple discriminators is less likely to be receiving poor gradient signal from all discriminators.  The paper's main idea looks straightforward to implement in practice and makes for a good addition to the GAN training toolbelt.  I am not very convinced by the GAM (and by extension the GMAM) evaluation metric. Without evidence that the GAN game is converging (even approximately), it is hard to make the case that the discriminators tell something meaningful about the generators with respect to the data distribution. In particular, it does not inform on mode coverage or probability mass misallocation.  The learning curves (Figure 3) look more convincing to me: they provide good evidence that increasing the number of discriminators has a stabilizing effect on the learning dynamics. However, it seems like this figure along with Figure 4 also show that the unmodified generator objective is more stable even with only one discriminator. In that case, is it even necessary to have more than one discriminator to train the generator using an unmodified objective?  Overall, I think the ideas presented in this paper show good potential, but I would like to see an extended analysis in the line of Figures 3 and 4 for more datasets before I think it is ready for publication.  UPDATE: The rating has been revised to a 7 following discussion with the authors.This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing.   The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect.  I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.",0,423
"The authors show that the idea of smoothing a highly non-convex loss function can make deep neural networks easier to train.  The paper is well-written, the idea is carefully analyzed, and the experiments are convincing, so we recommend acceptance. For a stronger recommendation, it would be valuable to perform more experiments. In particular, how does your smoothing technique compare to inserting probes in various layers of the network? Another interesting question would be how it performs on hard-to-optimize tasks such as algorithm learning. For example, in the ""Neural GPU Learns Algorithms"" paper the authors had to relax the weights of different layers of their RNN to make it optimize -- could this be avoided with your smoothing technique?This paper first discusses a general framework for improving optimization of a complicated function using a series of approximations. If the series of approximations are well-behaved compared to the original function, the optimization can in principle be sped up. This is then connected to a particular formulation in which a neural network can behave as a simpler network at high noise levels but regain full capacity as training proceeds and noise lowers.  The idea and motivation of this paper are interesting and sound. As mentioned in my pre-review question, I was wondering about the relationship with shaping methods in RL. I agree with the authors that this paper differs from how shaping typically works (by modifying the problem itself) because in their implementation the architecture is what is ""shaped"". Nevertheless, the central idea in both cases is to solve a series of optimization problems of increasing difficulty. Therefore, I strongly suggest including a discussion of the differences between shaping, curriculum learning (I'm also not sure how this is different from shaping), and the present approach.  The presentation of the method for neural networks lacks clarity in presentation. Improving this presentation will make this paper much easier to digest. In particular: - Alg. 1 can not be understood at the point that it is referenced.  - Please explain the steps to Eq. 25 more clearly and connect to steps 1-6 in Alg. 1. - Define u(x) clearly before defining u*(x)  There are several concerns with the experimental evaluations. There should be a discussion about why doesn't the method work for solving much more challenging network training problems, such as thin and deep networks. Some specific concerns:  - The MLPs trained (Parity and Pentomino) are not very deep at all. An experiment of training thin networks with systematically increasing depth would be a better fit to test this method. Network depth is well known to pose optimization challenges. Instead, it is stated without reference that ""Learning the mapping from sequences of characters to the word-embeddings is a difficult problem.""  - For cases where the gain is primarily due to the regularization effect, this method should be compared to other weight noise regularization methods.  - I also suggest comparing to highway networks, since there are thematic similarities in Eq. 22, and it is possible that they can automatically anneal their behavior from simple to complex nets during training, considering that they are typically initialized with a bias towards copying behavior.  - For CIFAR-10 experiment, does the mollified model also use Residual connections? If so, why? In either case, why does the mollified net actually train slower than the residual and stochastic depth networks? This is inconsistent with the MLP results.  Overall, the ideas and developments in this paper are promising, but it needs more work to be a clear accept for me.The paper shows the relation between stochastically perturbing the parameter of a model at training time, and considering a mollified objective function for optimization. Aside from Eqs. 4-7 where I found hard to understand what the weak gradient g exactly represents, Eq. 8 is intuitive and the subsequent Section 2.3 clearly establishes for a given class of mollifiers the equivalence between minimizing the mollified loss and training under Gaussian parameter noise.  The authors then introduce generalized mollifiers to achieve a more sophisticated annealing effect applicable to state-of-the-art neural network architectures (e.g. deep ReLU nets and LSTM recurrent networks). The resulting annealing effect can be counterintuitive: In Section 4, the Binomial (Bernoulli?) parameter grows from 0 (deterministic identity layers) to 1 (deterministic ReLU layers), meaning that the network goes initially through a phase of adding noise. This might effectively have the reverse effect of annealing.  Annealing schemes used in practice seem very engineered (e.g. Algorithm 1 that determines how units are activated at a given layer consists of 9 successive steps).  Due to the more conceptual nature of the authors contribution (various annealing schemes have been proposed, but the application of the mollifying framework is original), it could have been useful to reserve a portion of the paper to analyze simpler models with more basic (non-generalized) mollifiers. For example, I would have liked to see simple cases, where the perturbation schemes derived from the mollifier framework would be demonstrably more suitable for optimization than a standard heuristically defined perturbation scheme.",0,424
"This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.  In this paper, the authors propose two changes: ""CCA"" and ""inverted softmax"".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.  Overall, I wonder which aspect of this paper is really new. You mention:  - Faruqui & Dyer 2014 already used CCA and dimensionality reduction  - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal  Could you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?  Using cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)  Also, it seems to me that in linguistics the term ""cognate"" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts. The paper focuses on bilingual word representation learning with the following setting:  1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations. 2. There is no direct word to word alignments available between the source and target language.  This is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:  1. Theory for offline bilingual representation learning. 2. Inverted softmax. 3. Using cognate words for languages that share similar scripts. 4. Showing that this method also works at sentence level (to some extent).  Authors have addressed all my pre-review questions and I am ok with their response. I have few more comments:  1. Header for table 3 which says “word frequency” is misleading. “word frequency” could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5. 2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix. 3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate. 4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary. This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.  The inverted Softmax idea is very nice.  A few minor issues that ought to be addressed in a published version of this paper:  1) There is no mention of Haghighi et al (2008) ""Learning Bilingual Lexicons from Monolingual Corpora."", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed. 2) Likewise, Hermann & Blunsom (2013) ""Multilingual distributed representations without word alignment."" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data. 3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages 4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion 5) I don't have a better suggestion, but is there an alternative to using the term ""translation (performance/etc.)"" when discussing word alignment across languages? Translation implies something more complex than this in my mind. 6) The Mikolov citation in the abstract is messed up",0,426
"The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network. The manuscript is well written is provides good insight into the problem. I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn't interesting enough. I do have some questions and comments. 1.  As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn't affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In ""Objects in Contexts"", authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as ""ParseNet"" also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change.  2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here?  4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive. The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly. They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it's analysis and making it usable for images and DNNs.  The authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked. It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet). One thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones).   They paper is very clearly written, all necessary details are given and the paper is very nice to read.  Alltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot. The author's method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from). I can think of several potential applications of the method and therefore consider it of high significance.  Update: The authors did a great job of adopting all of my suggestions. Therefore I improve the rating from 8 to 9.The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-Šikonja & Kononenko (2008).  The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper.   My one concern with the paper is – Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified.  I would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution.   ",0,427
"This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,  I. McGraw, I. Badr, and J. Glass, ""Learning lexicons form speech using a pronunciation mixture model,"" in IEEE Transactions on Audio, Speech, and Language Processing, 2013  L. Lu, A. Ghoshal, S. Renals, ""Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition"", in Proc. ASRU   R. Singh, B. Raj, and R. Stern, ""Automatic generation of subword units for speech recognition systems,""  in IEEE Transactions on Speech and Audio Processing, 2002  It would be interesting to put this work in the context by linking it to some previous works in the HMM framework.  Overall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, ""O(5) days to converge"" sounds a bit odd to me. Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.  Although the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.  The authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.  Obviously, only text based segmentation would result in two (but simpler) independent tasks. In order to extract such segmentation several publicly open tools are available and should be cited. Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.  It looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models. In another way, less tokens are more probable due to less multiplication of probabilities. As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all? (WSJ is a closed vocabulary task). It would be good to show that the sub word model could outperform even a full-word model (no segmentation). Your model estimates p(z_t|x,zThis paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes.   Minor points - Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it's hard to tell if the model is learning phonemes or just most frequent character n-grams. - Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now). ",1,430
"This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.  In terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.  In the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.  UPDATED: code is now available. Revised review accordingly.In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.  The ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.  More experiments were added, so I'm updating my score.This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies.  The paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature. Furthermore, their code is open-source and the live demo is looking good.  The authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful. It would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.  ",1,431
"This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.  I think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.  That being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular: - The notation \tilde{Q}^pi is introduced in a way that is not very clear, as ""an estimate of the Q-values"" while eq. 5 is an exact equality (no estimation) - It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations. - The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the ""variant of asynchronous deep Q-learning"" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods - Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The ""mu"" distribution also comes somewhat out of nowhere.  I hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).  Minor remarks: - The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined) - The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over ""a"" of a quantity that does not depend (clearly) on ""a"" - I believe 4.3 is for the tabular case but this is not clearly stated - Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.  Typos: - ""we refer to the classic text Sutton & Barto (1998)"" => missing ""by""? - ""Online policy gradient typically require an estimate of the action-values function"" => requires & value - ""the agent generates experience from interacting the environment"" => with the environment - in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi - ""allowing us the spread the influence of a reward"" => to spread - ""in the off-policy case tabular case"" => remove the first case - ""The green line is Q-learning where at the step an update is performed"" => at each step - In Fig. 2 it says A2C instead of A3C  NB: I did not have time to carefully read Appendix ANice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.    Presentation:  Although that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network – for me that would be a more intuitive exposition of the new algorithm and findings.   Small concern in general case derivation:  Section 3.2: Eq. (7) the expectation (s,a) is wrt to \pi, which is a function of \theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.  Results: A comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)  Overall: strong paper, good theoretical insights. This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper.  Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.",0,432
"Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.  Given that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.  The authors claim that “unlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.” Where is the evidence for this claim? I didn’t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.  I still think the author’s intuitions around the “fixed reconstruction cost of L2” are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \int p(z) N(x | f(z), \sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points. The authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.  Generative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.  While not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.  The authors clearly mention the difference and similarities with other types of generative models that are being actively researched. Compared to autoregressive models, the proposed approach offers fast sampling. Compared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation. Compared to variational autoencoders, the inference is exact. Compared to deep Boltzmann machines, the learning of the proposed method is tractable. It is clear that Real NVP goal is to bridge the gap between existing and popular generative models.  The paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?  Typo: (Section 3.7) We also ""use apply"" batch normalizationThis paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible.  The paper nicely presents this core idea and a way to achieve this - by choosing special ""routings"" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation.   On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures.  I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.  In summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance.  ",0,433
"Contributions The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.  Novelty+Significance Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics.   Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.  As RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn’t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization?   Clarity The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.  Summary Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.  Pros - Shows batch normalization to work for RNNs where previous works have not succeeded - Good empirical analysis of hyper-parameter choices and of the activations - Experiments on multiple tasks - Clarity  Cons - Relatively incremental - Several ‘hacks’ for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization) - No mention of computational overhead - Only character or pixel-level tasks, what about word-level?This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN question-answering task), and pixel level (MNIST and pMNIST). The supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration.  The experiment on pMNIST also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies. I additionally also appreciated the gradient flow insight, specifically the impact of unit variance on tanh derivatives. Showing it not just for batch normalization but additionally the ""toy task"" (Figure 1b) was hugely useful.  Overall I find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting.The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence.  The paper is well written and the idea well presented.   i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM.   Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out. ",1,434
"This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm. The relevance of these results goes beyond image classification.   Pros:  - Simple and effective method to improve convergence - Good evaluation on well known database   Cons:  - Connection of introduction and topic of the paper is a bit unclear - Fig 2, 4 and 5 are hard to read. Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer. The baseline also doesn't seem to converge  Remarks: An loss surface for T_0 against T_mult would be very helpful. Also understanding the relationship of network depth and the performance of this method would add value to this analysis. This an interesting investigation into learning rate schedules, bringing in the idea of restarts, often overlooked in deep learning. The paper does a thorough study on non-trivial datasets, and while the outcomes are not fully conclusive, the results are very good and the approach is novel enough to warrant publication.   I thank the authors for revising the paper based on my concerns.  Typos: - “flesh” -> “flush”This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks  Working on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another.  Revision: -  the authors substantially improved the contents of the paper, including experiments on another set than Cifar -  the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate I have therefore improved my rating",0,435
"This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role.  It'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What's the headroom and oracle number with the current approach? This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task.  As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language.  By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation.  On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods.   Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding.  The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design.   The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art.   I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks).   I do have a few objections though.   -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the ""row selector"", the ""scalar answer"" and the ""lookup answer"" etc.   -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well. ",0,436
"This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive.   One caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.  I applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.  A disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.This paper introduce a variant of A3C model where while agents run on multiple cores on CPU the model computations which is the computationally intensive part is passed to the GPU. And they perform various analysis to show the gained speedup.  Thanks the authors for the replying to the questions and adjusting the paper to make it more clear. It's an interesting modification the the original algorithm. And section 5 does a through analysis of gpu utilization on different configurations. The main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities. Stability is a very important issue in RL, and also the most successful algorithms should be able to achieve good results in various domains. I do understand the computational resource limitation, especially in academia if in fact this work was done outside Nvidia.The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors' own implementation of A3C as well as to published reference scores.  The paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see.   I appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: ",0,437
"This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations.  The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.  Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed. Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.   While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence.  One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.  Another downside is that the authors dismiss navigation literature as ""not RL"". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations. I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training.   I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task! This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.  The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.",0,438
"This paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex task.  Faster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found.  It is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on. Is there still an advantage?  The authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input. However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution. Could the authors show experiments in this setting? would still be useful to have the proposed approach? what would the challenges be in this more realistic scenario?  In the second experiment the authors show results where the length of the program at training time is different than the length at test time. However, the results are shown when only 20% of the programs are finished. Could you show results for finding all programs?   The paper is missing an analysis of the results. What type of programs are difficult? how often is the NNet wrong? how does this affect speed? what are the failure modes of the proposed method?  The authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation. However, why would average pooling make sense here? would it make more sense to combine the predictions at the decoder, not the encoder?  Learning from only 5 executions seems very difficult to me. For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable.   In summary an interesting paper. This paper tackles a problem that is outside my area of expertise so I might have miss something important.  The paper presents a technique to combine deep learning style input-output training with search techniques to match the input of a program to the provided output. Orders of magnitude speedup over non-augmented baselines are presented.  Summary: ——— The proposed search for source code implementations based on a rather small domain specific language (DSL) is compelling but also expected to some degree  Quality: The paper is well written. Clarity: Some of the derivations and intuitions could be explained in more detail but the main story is well described. Originality: The suggested idea to speed up search based techniques using neural nets is perfectly plausible. Significance: The experimental setup is restricted to smaller scales but the illustrated improvements are clearly apparent.  Details: ———— 1. The employed test set of 100 programs seems rather small. in addition the authors ensure that the test set programs are semantically disjoint from the training set programs. Could the authors provide additional details about the small size of the test set and how to the disjoint property is enforced?  2. The length of the programs is rather small at this point in time. A more detailed ablation regarding the runtime seems useful. The search based procedure is probably still the computationally most expensive part. Hence the neural net provides some additional prior information rather than tackling the real task.This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs.   The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain!  The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code.  I see this paper as being above the threshold for acceptance.  ",0,439
"This paper considers the problem of model-based policy search. The authors  consider the use of Bayesian Neural Networks to learn a model of the environment and advocate for the $\alpha$-divergence minimization rather than the more usual  variational Bayes.   The ability of alpha-divergence to capture bi-modality however  comes at a price and most of the paper is devoted to finding tractable approximations.  The authors therefore use the approach of Hernandez-Lobato et al. (2016) as proxy to the alpha-divergence .   The environment/system dynamics is clearly defined as a well as the policy parametrization  (section 3) and would constitute a useful reference point for other researchers.  Simulated roll-outs, using the learned model, then provide samples of the expected  return. Since a model of the environment is available, stochastic gradient descent  can be performed in the usual way, without policy gradient estimators, via automatic  differentiation tools.   The experiments demonstrate that alpha-divergence is capable of capturing multi-model  structure which competing methods (variational Bayes and GP) would otherwise struggle with. The proposed approach also compares favorably in a real-world batch setting.  The paper is well-written, technically rich and combines many recent tools  into a coherent algorithm. However, the repeated use of approximations to original  quantities seems to somehow defeat the benefits of the original problem formulation.  The scalability and computational effectiveness of this approach is also questionable  and I am uncertain if many problem would warrant such complexity in their solution.  As with other Bayesian methods, the proposed approach would probably shine in low-samples  regime and in this case might be preferable to other methods in the same class (VB, GP).  The authors propose a novel way of using Bayesian NNs for policy search in stochastic dynamical systems. Specifically, the authors minimize alpha-divergence with alpha=0.5 as opposed to standard VB. The authors claim that their method is the first model-based system to solve a 20 year old benchmark problem; I'm not very familiar with this literature, so it's difficult for me to assess this claim.  The paper seems technically sound. I feel the writing could be improved. The notation in sections 2-3 feels a bit dense and there are a lot of terminology / approximations introduced, which makes it hard to follow. The writing could be better structured to distinguish between novel contributions vs review of prior work. If I understand section 2.3 correctly, it's mostly a review of black box alpha divergence minimization. If so, it would probably make sense to move this to the appendix.   There was a paper at NIPS 2016 showing promising results using SGHMC for Bayesian optimization: ""Bayesian optimization with robust Bayesian neural networks"" by Springenberg et al. Could you comment on applicability of stochastic gradient MCMC (SGLD / SGHMC) for your setup?  Can you comment on the computational complexity of the different approaches?  Section 4.2.1: why can't you use the original data? in what sense is it fair to simulate data using another neural network? can you evaluate PSO-P on this problem?This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using \alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios.   The paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with \alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature.  That said, I have a few questions and suggestions:  1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment?  2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be?  3) How important is the normality assumption in z_n? How is the variance \gamma established?  4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ?  5) Equation (3), denominator \mathbf{y} should be \mathbf{Y} ?  6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used.  7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen.  8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7.  9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t.  10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what’s made use of here. At least, this strand of work should be mentioned in Section 5.   References:  Girard, A., Rasmussen, C. E., Quiñonero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552. ",0,440
"This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.  The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It’s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I’m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I’ll come back to this point in the next paragraph because it’s mainly a clarity issue.  The abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it’s not clear from the text how the hyperparameter \bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don’t consider this a very serious flaw because I’m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.  To my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper’s strongest points.   It’s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it’s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs.   Pros: Original clever idea. Nice interesting visualizations. Interesting experiments.  Cons: Some experimental details are not clear. I’m not convinced of the strength of the baseline. The paper shouldn’t claim actual computational savings without reporting wall-clock times.  Edit: I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed.   Edit:  Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out. This is high novelty work, and an enjoyable read.  My concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.)  In pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation.  --- 2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment.TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., ""variable computation"") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.  === Gating Mechanism === At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.  Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.  This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.  A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.  === Variable Computation === One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell.  === Evaluation === This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.  The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.  One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar.  === Minor === * Please add Equations numbers to the paper, hard to refer to in a review and discussion!  References Chung et al., ""Hierarchical Multiscale Recurrent Neural Networks,"" in 2016. Graves et al., ""Adaptive Computation Time for Recurrent Neural Networks,"" in 2016. Wu et al., ""On Multiplicative Integration with Recurrent Neural Networks,"" in 2016.",0,441
"Thank you for an interesting read. I personally like the information bottleneck principle and am very happy to see its application to deep neural networks. To my knowledge, this is the first paper that applies IB to train deep networks (the original papers only presented the concept), but see below for the note of independent work claim.   The derivation of the variational lowerbound is very clear, even for those who are not very familiar with variational inference. Also the explanation of the IB principle is clear. Experimental results seem to be very promising.  I found the presentation for the model a bit confusing. In variational inference/information maximisation, p usually denotes the model and q represents the ""inference engine"". This means the choice of inference method is independent to the modelling procedure. However the presented VIB assumed p(x, y) as the **underlying data distribution** (and approximated by the empirical distribution), thus here the model is actually q(y|z)p(z|x). Then the authors presented p(y|x) as the **predictive distribution** in page 8, paragraph 2 of section 4.2.3. Predictive in what sense? I guess you meant p(y|x) = \int q(y|z) p(z|x) dz in this case, but this makes the two definitions contradict to each other!  The authors have made an interesting connection to variational auto-encoder and the warm-up training (by tuning beta). However, even when the loss function formula is the same to the variational lowerbound used in VAE (in this case beta = 1), the underlying model is different! For example, r(z) in VIB is the variational approximation to p(z) (which means r(z) is not a component in the model), while in VAE it is the prior distribution which is actually defined in the modelling procedure. Similaly p(z|x) in VIB is included in the model, while in VAE that is the approximate posterior and can be independently chosen (e.g. you can use p(x|z) as a deep NN but p(z|x) as a deep NN or a Gaussian process).  In summary, I think the presentation for the modelling procedure is unclear. I hope these point would be made clearer in revision since the current presentation makes me uncomfortable as a Bayesian person. In the VAE part, it's better to clearly mention the difference between VIB and VAE, and provide some intuitions if the VIB interpretation is preferred.   Typos: Eq. 9-11: did you mean q(y|z) instead of q(z|y)? Fig 2 ""as beta becomes smaller"": did you mean ""larger""?  **claim for independent work** The authors claimed that the manuscript presented an independent work to Chalk et al. 2016 which is online since May 2016. It seems to me that nowadays deep learning research is very competitve that many people publish the same idea at the same time. So I would trust this claim and commend the authors' honesty, but in case this is not true, I would not recommend the manuscript for acceptance.Summary: The paper “Deep Variational Information Bottleneck” explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). On the example of MNIST, the authors show that this may be used for regularization or to improve robustness against adversarial attacks.  Review: The IB is potentially very useful for important applications (regularization, adversarial robustness, and privacy are mentioned in the paper). Combining the IB with recent advances in deep learning to make it more widely applicable is an excellent idea. But given that the theoretical contribution is a fairly straight-forward application of well-known ideas, I would have liked to see a stronger experimental section.  Since the proposed approach allows us to scale IB, a better demonstration of this would have been on a larger problem than MNIST. It is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers.  Why is dropout not included in the quantitative comparison of robustness to adversarial examples (Figure 4)?  How was the number of samples (12) chosen?  What are the error bars in Figure 1 (a)?  On page 7 the authors claim “the posterior covariance becomes larger” as beta “decreases” (increases?). Is this really the case? It’s hard to judge based on Figure 1, since the figures are differently scaled.  It might be worth comparing to variational fair autoencoders (Louizos et al., 2016), which also try to learn representations minimizing the information shared with an aspect of the input.  The paper is well written and easy to follow.Update: raised the score, because I think the arguments about adversarial examples are compelling.  I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not convinced that it's a competitive regularizer.  For example, I don't believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc.  I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply).    ----  Summary: If I understand correctly, this paper proposes to take the ""bottleneck"" term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective.    The argument is that this is an effective regularizer and increases robustness to adversarial attacks.    Pros:   -The presentation is quite good and the paper is easy to follow.    -The idea is reasonable and the relationship to previous work is well described.    -The robustness to adversarial examples experiment seems convincing, though I'm not an expert in this area.  Is there any way to compare to an external quantitative baseline on robustness to adversarial examples?  This would help a lot, since I'm not sure how the method here compares with other regularizers in terms of combatting adversarial examples.  For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)?    Cons:   -MNIST accuracy results don't seem very strong, unless I'm missing something.  The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%.  So the 1.13% error rate listed here doesn't necessarily prove that the method is a competitive regularizer.  I also suspect that tuning this method to make it work well is harder than other regularizers like dropout.    -There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z.  For example, the output could directly follow z, or there could be several layers between z and the output.  As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it's not obvious why this choice was made.  Did it work best empirically?    Other:   -I wonder what would happen if you ""trained against"" the discovered adversarial examples while also using the method from this paper.  Would it learn to have a higher variance p(z | x) when presented with an adversarial example?  ",1,442
"This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new.   The experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see   Deep Speech 2: End-to-End Speech Recognition in English and Mandarin  from Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are  1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?  2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences.   The paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public. The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.  Strong points: - well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models Handling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community - strong experimental section, with some strong results (though not complete: see weak points)  Weak points: - Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables) - What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?  Some other (minor) comments:  - Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities).  - How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever? - Some more comments on details of the auxiliary direct model would be definitely of interest. - How crucial is the correct choice of the pruning variables (K1 and K2)?  - Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?  Typos: Table 1: chanel -> channel (one before last row)  Apologies for late review.This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.  PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results.  CONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It's worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.  Other Comments:   - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper.    - It initially seems strange to suggest a noisy-channel model as a way of addressing the ""explaining away"" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever. ",0,443
"This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.  Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.  It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.  Other comments: - Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$. - Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text. - In Eq. (13), define $c_0 = 0$. - Eq. (13) is exactly the same as Eq. (15). Is there a mistake? - In Table 1, third column should have word ""film"" highlighted. - ""are shown in 2"" -> ""are shown in Table 2"". - Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.  The questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?  The approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types.  It would also be good to see an attention model as a baseline in addition to the gradient-based baseline.  Minor comments: - P and Q seem to be undefined. - Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1. - In the paragraph above section 6.3: 'adam' -> 'Adam'.EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score.  This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach.   I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and  the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers.  Comments: - Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it's not clear at that point that the answer is an entity within the document. - 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?) - 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction? - how does performance of the pattern matching change with different cutoff constant values? - 5.2: are there questions whose answers are not entities?  - how could the proposed approach be used when predictions aren't made at every word? is there any extension for, say, sentence-level sentiment classification? ",0,444
" SUMMARY: This paper describes a set of experiments evaluating techniques for training a dialogue agent via reinforcement learning. A standard memory network architecture is trained on both bAbI and a version of the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous experiments are performed comparing the behavior of different training algorithms under various experimental conditions.  STRENGTHS: The experimentation is comprehensive. I agree with the authors that these results provide additional useful insight into the performance of the model in the 2016 paper (henceforth W16).  WEAKNESSES: This is essentially an appendix to the earlier paper. There is no new machine learning content. Secondarily, the paper seems to confuse the distinction between ""training with an adaptive sampling procedure"" and ""training in interactive environments"" more generally. In particular, no comparisons are presented to the to the experiments with a static exploration policy presented in W16, when the two training can & should be evaluated side-by-side. The only meaningful changes between this work and W16 involve simple (and already well-studied) changes to the form of this exploration policy.  My primary concern remains about novelty: the extra data introduced here is welcome enough, but probably belongs in a *ACL short paper or a technical report. This work does not stand on its own, and an ICLR submission is not an appropriate vehicle for presenting it.  ""REINFORCEMENT LEARNING""  [Update: concerns in this section have been addressed by the authors.]  This paper attempts to make a hard distinction between the reinforcement learning condition considered here and the (""non-RL"") condition considered in W16. I don't think this distinction is nearly as sharp as it's made out to be.   As already noted in Weston 2016, the RBI objective is a special case of vanilla policy gradient with a zero baseline and off-policy samples. In this sense the version of RBI considered in this paper is the same as in W16, but with a different exploration policy; REINFORCE is the same objective with a nontrivial baseline. Similarly, the change in FP is only a change to the sampling policy. The fixed dataset / online learning distinction is not especially meaningful when the fixed dataset consists of endless synthetic data.  It should be noted that some variants of the exploration policy in W16 provide a stronger training signal than is available in the RL ""from scratch"" setting here: in particular, when $\pi_acc = 0.5$ the training samples will feature much denser reward. However, if I correctly understand Figures 3 and 4 in this paper, the completely random initial policy achieves an average reward of ~0.3 on bAbI and ~0.1 on movies---as good or better than the other exploration policies in W16!  I think this paper would be a lot clearer if the delta from W16 were expressed directly in terms of their different exploration policies, rather than trying to cast all of the previous work as ""not RL"" when it can be straightforwardly accommodated in the RL framework.  I was quite confused by the fact that no direct comparisons are made to the training conditions in the earlier work. I think this is a symptom of the problem discussed above: once this paper adopts the position that this work is about RL and the previous work is not, it becomes possible to declare that the two training scenarios are incomparable. I really think this is a mistake---to the extent that the off-policy sample generators used in the previous paper are worse than chance, it is always possible to compare to them fairly here. Evaluating everything in the ""online"" setting and presenting side-by-side experiments would provide a much more informative picture of the comparative behavior of the various training objectives.  ON-POLICY VS OFF-POLICY  Vanilla policy gradient methods like the ones here typically can't use off-policy samples without a little extra hand-holding (importance sampling, trust region methods, etc.). They seem to work out of the box for a few of the experiments in this paper, which is an interesting result on its own. It would be nice to have some discussion of why that might be the case.  OTHER NOTES  - The claim that ""batch size is related to off-policy learning"" is a little odd. There are lots of on-policy algorithms that require the agent to collect a large batch of transitions from the current policy before performing an  (on-policy) update.  - I think the experiments on fine-tuning to human workers are the most exciting part of this work, and I would have preferred to see these discussed (and explored with) in much more detail rather than being relegated to the penultimate paragraphs.This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model’s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher’s feedback to significantly improve performance.  Overall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.  My main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston:  “(i) That earlier work did not use the natural reinforcement learning/online setting, but “cheated” with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.”  Point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that “the model also works if we collect the data online (i.e. the agent’s policy is used to collect data rather than a fixed policy beforehand)”. While this is a step in the right direction, I’m not sure if it’s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject.   EDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution. As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting.  several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success?  The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision.  2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges.  3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal.  4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing.  There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston'16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.",0,445
"This paper extends the GAN framework to allow for latent variables. The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x). The joint distribution on x,z is then modeled using a joint generator model p(x,z)=p(z)p(x|z).  Both q and p are then trained by trying to fool a discriminator. This constitutes a worthwhile extension of GANs: giving GANs the ability to do inference opens up many applications that could previously only be addressed by e.g. VAEs.  The results are very promising. The CIFAR-10 samples are the best I've seen so far (not counting methods that use class labels). Matching the semi-supervised results from Salimans et al. without feature matching also indicates the proposed method may improve the stability of training GANs.After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.  --------------- Initial Review:  This paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.  There are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.  Since ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.  The quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.  There are two sets of semi-supervised results:  The first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.  The second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above.This is a parallel work with BiGAN.  The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result.",0,446
"This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.  The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.  Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.   Other comments: - The abstract uses the phrase ""interactive dialogue agents"". What is meant by ""interactive"" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive? - A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states ""For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)"". This means the agent learns *when* to ask questions but not *what* questions to ask. - Related to the previous comment, in the sub-section ""ONLINE REINFORCEMENT LEARNING (RL)"" the paper states ""We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask."". Please clarify this by removing the part ""what to ask"". - The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the ""TrainAQ(+FP)"" and ""TrainMix"" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix. - Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.  --- UPDATE ---  Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information. This can be interpreted as a form of interaction between the dialogue agent and a ‘teacher’.  The problem under investigation is indeed very important. The authors create a synthetic environment in which to test their agent. The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher’s question, and different ways the agent can ask for extra information.   I am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots. I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue. However, I think the ‘ground-up’ approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction. Of course, the paper would be much more convincing with human experiments.   Additional notes: I think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: “the learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question”, is particularly limited since only word misspellings are considered (and the models used don’t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context. I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings.  EDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the ""teacher"" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions.    -- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction. -- The paper studies three different types of tasks where the agent can benefit from user feedback. -- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings.  Other comments/questions:  -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions? -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. -- The first point on Page 10 ""The performance of TestModelAQ is worse than TestAQ but better than TestQA."" is not true for Task 2 from the numbers in Tables 2 and 4. -- What happens if the conversational history is smaller or none?  -- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student.  -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? -- Figure 1, Task 2 AQ, last sentence should have a negative response ""(-)"" instead of positive as currently shown.   Preliminary Evaluation:  A good first step in the research direction of learning dialogue agents from unstructured user interaction. ",1,447
"This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice. The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.  Minor point on presentation: Speaking of the ""evolution"" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?  In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of “training algorithm”?  Comments on central claims: Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, ",1,448
"This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure.   This paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.  However, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered. The number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:  - In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.  - The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.  - A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].  - It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.  - The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.  Overall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture.   [1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. ""Inception-v4, inception-resnet and the impact of residual connections on learning."" arXiv preprint arXiv:1602.07261 (2016). This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep” networks while residuals are incidental.  The main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep” networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.  Authors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.  DenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation.   Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.  Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.   Although the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant.   Pros: Provides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis  cons:      -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters The claims are intuitive but not supported well with empirical evidence Path regularization does not yield improvement when the data augmentation is used      -     The empirical results do not show whether the method is promising for “ultra-deep” networks This paper presents a strategy for building deep neural networks via rules for expansion and merging of sub-networks. pros: - the idea is novel - the approach is described clearly cons: - the experimental evaluation is not convincing, e.g. no improvement on SVHN - number of parameters should be mentioned for all models for fair comparison - the effect of drop-path seems to vanish with data augmentation",0,449
"  ## Paper summary  The paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.  A theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.  ## Review summary   The paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.  ## Major comments / questions   My main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.  * Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: ""... the random variable n_te \hat{t} follows a Binomial(nte, p)..."" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.  * In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality.   Anyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.  For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.  As such, a straightforward threshold can be derived for any classifier.  Finding a more powerful test then amounts to training a better classifier.  One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.  + The approach is sound and very general + The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have  - The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.  For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.  Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well  * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)  * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.  Arthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful.  If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.  The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment.I would like first to apologize for the delay.  Summary: A framework for two-samples statistical test using binary classification is proposed. It allows multi-dimensional sample testing and an interpretability that other tests lack. A theoritical analysis is provided and various empirical tests reported.  A very interesting approach. I have however two main concerns.  The clarity of the presentation is obscured by too much content. It would be more interesting if the presentation could be somewhat self-contained. You could consider making 2 papers out of this paper.  Seriously, you cram a lot of experiments in this paper. But the setting of the experiments is not really explained. We are supposed to have read Jitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All  this is okay but reduces your public to a very few.  For example, if I am not mistaken, you never explained what SCF is, despite the fact that its performances are reported.   As a second point, given also that the number of submissions to this conference are exploding, I would like to challenge you with the following question:  Why is this work significant to the representation learning community? ",0,450
"This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.  Pros: 1. Providing new theory about existence of ""poor"" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer. 2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is.   Cons: The results are very specific in both topology and geometry analysis. 1. The analysis is performed only over a ""single"" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically.  2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.  With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.This paper studies the energy landscape of the loss function in neural networks.  It is generally clearly written and nicely provides intuitions for the results.  One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized.  It also quantifies, in a way, the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path.  It would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent.  The paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path.  Using this they show that the loss seems to become more nonconvex when the loss is smaller.  This is also quite interesting.  The work does have some significant limitations, which is not surprising given the difficulty of fully analyzing the network loss function.  However, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss.  I would have also appreciated a little more practical discussion of the bound in Theorem 2.4.  It is hard to tell whether this bound is tight enough to be practically relevant. This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.",0,451
"This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).  Here are some comments and questions, for improving the paper:  The introduction states that ""all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k"". This statement is too strong, and is actually disproved in the experiments — repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.  In the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)  Experiments: Can you provide error bars on the experimental results? (from running multiple random seeds)  It would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.  The TRPO evaluation is different from the results reported in Duan et al. ICML ’16. Why not use the same benchmark?  Videos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?  How many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).  Can the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.  Minor comments: -- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500. -- “idea of deciding when necessary” - seems like it would be better to say “idea of only deciding when necessary"" -- ""spaces.Durugkar et al.” — missing a space. -- “R={4}” — why 4? Could you use a letter to indicate a constant instead? (or a different notation) This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.  Comments:  - The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?  - It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.  - Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)  - In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?  - The section on DDPG is confusingly written. ""Concatenating"" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)?   - Is the 'name_this_game' name in the tables  intentional?  - A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.  A few comments/questions: - Table 1 could be easier to interpret as a figure of histograms. - Figure 3 could be easier to interpret as a table. - How was the subset of Atari games selected? - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores. - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR. - FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.  Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using.  --------------------  I'm increasing my score to 8 based on the rebuttal and the revised paper.",1,452
"Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix.The paper presents a second-order method for training a neural networks while ensuring at the same time that weights (and activations) are binary. Through binarization, the method aims to achieve model compression for subsequent deployment on low-memory systems. The method is abbreviated BPN for ""binarization using proximal Newton algorithm"".  The method incorporates the supervised loss function directly in the binarization procedure, which is an important and desirable property. (Authors mention that existing weight binarization methods ignore the effect of binarization to the loss.) The method is clearly described and related analytically to the previously proposed weight binarization methods.  The experiments are extensive with multiple datasets and architectures, and demonstrate the generally higher performance of the proposed approach.  A minor issue with the feed-forward network experiments is that only test errors are reported. Such information does not really give evidence for the higher optimization performance. (see also comment ""RE: AnonReviewer3's questions"" stating that all baselines achieve near perfect training accuracy.) Making the optimization problem harder (e.g. by including an explicit regularizer into the training objective, or by using a data extension scheme), and monitoring the training objective instead of the test error could be a more direct way of demonstrating superior optimization performance.  The superiority of BPN is however becoming more clearly apparent in the subsequent LSTM experiments.This paper proposed a proximal (quasi-) Newton’s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton’s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the theoretical analysis is not very convincing or useful. The formulated optimization problem (3)-(4) is essentially a mixed integer programming. Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^t]_k > \beta in Theorem 3.1. This relation could be hard to hold in DNN as the loss surface could be extremely complicated. ",1,453
"This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator's output domain. The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator. The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity). Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.  Pros: * The paper is well-written. * The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs. * The theorems regarding optimality of the Nash equilibrium appear to be correct. * Thorough exploration of hyperparameters in the MNIST experiments. * Semi-supervised results show that contrastive samples from the generator improve classification performance.  Cons: * The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper. * From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets. * It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.  Specific Comments * Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions. * Sec 2.4: It is confusing that ""pulling-away"" is abbreviated as ""PT"". * Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST. Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here. * Figure 3: There is little variation across the histograms, so this figure is not very enlightening. * Appendix A: In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.  Typos / Minor Comments * Abstract: ""probabilistic GANs"" should probably be ""traditional"" or ""classical"" GANs. * Theorem 2: ""A Nash equilibrium ... exists"" * Sec 3: Should be ""Several papers were presented""  Overall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.  [1] Springenberg, Jost Tobias. ""Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks."" arXiv preprint arXiv:1511.06390 (2015). [2] Kim, Taesup, and Yoshua Bengio. ""Deep Directed Generative Models with Energy-Based Probability Estimation."" arXiv preprint arXiv:1606.03439 (2016).This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet). As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices.   First, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum). This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise -- that one could recover a meaningful score function from the trained energy-function (discriminator). This should be clarified.   Second, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function. These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction.   The two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.  The theoretical results seem solid to me and make a nice contribution.  Regarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results. Though it is not mentioned here, the use of bold typically indicates the state-of-the-art.   I think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments. That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR.  This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.  The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.  I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.",1,455
"Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.  CMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it’s the lowest hanging fruit.  Below I give more detailed feedback.  One way to speed up MMD is to use a random Fourier basis as was done in “Fastmmd: Ensemble of circular discrepancy for efficient two-sample test” by Zhao and Meng, 2015. There are also linear time estimators, e.g., in “A Kernel Two-Sample Test“ by Gretton et al., 2012. I don’t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.  The paper “Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy” by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.  How limiting is the assumption that the distribution has independent marginals?  The sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.  I’m concerned that the central moments won’t be numerically stable at higher orders when backpropagating. This doesn’t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I’m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I’m referring specifically to the individual terms within.  Figure 3 is rather cluttered, and aside from the mouse class it’s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.  I would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?  Figure 4 should have a legendThis paper proposed a new metric central moment discrepancy (CMD) for matching two distributions, with applications to domain adaptation.  Compared to a more well-known variant, MMD, CMD has the benefit of not over penalizing the mean, and therefore can focus more on the shape of distribution around the center.  In terms of discriminative power (the ability to tell two distributions apart), MMD and CMD should be equivalent, but in practice I can understand that CMD may be better as MMD tries to match the raw moments which may over penalize data that are not zero centered.  In the paper CMD is used only up to Kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the CMD objective, I think this is mostly motivated for computation efficiency.  A natural comparison with MMD therefore can be made, by also explicitly include raw moments up to Kth order.  Another thing to compare against is to include all moments, not just the diagonal terms, in the objective.  This is computationally expensive, but can be done for e.g. 1st and 2nd orders.  Since the experiments only compare CMD in the above form with kernelized MMD, the claim that explicit moment matching is helpful is not very well supported.  To make this a solid claim CMD should be compared against MMD with explicit raw moments.  The claim that the kernel parameter in MMD is hard to tune and CMD does not have such parameters only applies to kernel MMD, not explicit MMD.  For kernel MMD, there are also studies on how to set these parameters, for example:  Sriperumbudur et al.  Kernel choice and classifiability for rkhs embeddings of probability distributions. Gretton et al.  A kernel two-sample test.  and also using multiple kernels (Li et al. 2015) which removes the need to tune them.  Tuning the beta directly like done in this paper is usually not the way MMD is tuned.  At least simple heuristics like dividing |x-y|^2 by dimensionality or mean pairwise distance first should be applied first before trying beta in the way done in this paper.  Overall I think CMD could be better than MMD, and could have applications in many domains.  But it also has the problem of not easily kernelizable (you can argue this both ways though).  The experiments demonstrating that CMD is better could be done more convincinly.  The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance.   The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption?   Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class? ",1,456
"Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compareThe idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.  To improve the paper:  1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.  2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The ""5 bits"" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2. There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.  Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.",0,457
"This paper presents a principled approach to finding flat minima. The motivation to seek such minima is due to their better generalization ability. The idea is to add to the original loss function a new term that exploits both width and depth of the objective function. In fact, the regularization term can be interpreted as Gaussian convolution of the exponentiated loss. Therefore, the introduced regularization term is essentially Gaussian smoothed version of the exponentiated loss. The smoothing obviously tends to suppress sharp minima.  Overall, developing such regularization term based on thermodynamics concepts is very interesting. I have a couple of concerns that the authors may want to clarify in the rebuttal.  1. When reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain SGD. Is this the number of epochs it takes by line 7 of your algorithm, or it is the total number of epochs (line 3 and 7 all combined)? If the former, it is not a fair comparison. If you multiply the number of epochs of SGD (line 7) by the number iterations it takes to approximate Langevin dynamics, it seems you obtain little gain against plain SGD.  2. The proposed algorithm approximates the smoothed ""exponentiated"" loss (by smoothing I refer to convolution with the Gaussian). I am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)? Is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows simpler approximation (Monte Carlo integration instead of MCMC), e.g. see section 5.3 of The paper introduces a new regularization term which encourages the optimizer  to search for a flat local minimum of reasonably low loss instead of seeking a  sharp region of a low loss. This is motivated by some empirical observations that local minima of good generalization performance tend to have flat shape.  To achieve this, a regularization term based on the free local energy is proposed and the gradient of this term, which do not have tractable closed-form solution,  is obtained by performing Monte Carlo estimation using SGLD sampler. In the  experiments, the authors show some evidence of the flatness of good local  minima, and also the performance of the proposed method in comparison to the Adam optimizer.   The paper is well and clearly written. I enjoyed reading the paper. The connection to the concept of free energy in optimization framework seems interesting. The  motivation of pursuing flatness is also well analyzed with a few experiments. I'm wondering if the first term in eqn. (8) is correct. I guess it should be f(x') not f(x)? Also, I'm wondering why the authors did not add the experiment results on RNN in the evaluation of the performance because char-lstm for text generation was  already used for the flatness experiments. I think adding more experiments on  various models and applications of deep architectures (e.g., RNN, seq2seq, etc.)  will make the author's claim more persuasive. I also found the mixed usage of the terminology, e.g., free energy and free entropy, a bit confusing.  Overview:   This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.  Pros: - Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks - Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima - Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area  Cons / points suggested for a rebuttal: (1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“ This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.” It's not clear to me how to reconcile those two claims.  (2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.” It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.  (3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“ According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used.  (4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.  (5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper. ",1,458
"This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-deﬁned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.  Although factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used.   BTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g.,   Huang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE.The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing.   As mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this.   One question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?    The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.  The framework is nice and appealing.   However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning"" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods. ",1,459
"The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher’s latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.  The claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.  The proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.  The paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don’t really tease apart the effect of each of the various innovations, so it’s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn’t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.  The paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7. This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.  As mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.  However, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.  Some technical aspects which need clarifications: - For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this. - It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing? - In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term? - The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper ""Prioritized Experience Replay"" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.   Other comments: - Please move Section 7 to the appendix. - ""Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability"": I think what is meant is using *large* values of lambda. - Above eq. (6) mention that the squared error is used. - Missing a ""t"" subscript at the beginning of eq. (9)? - It was hard to understand the stochastic duelling networks. Please rephrase this part. - Please clarify this sentence ""To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games."" - Figure 2 (Bottom): Please add label to vertical axes.This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful. ",1,460
"This paper presents a semi-supervised technique for “self-ensembling” where the model uses a consensus prediction (computed from previous epochs) as a target to regress to, in addition to the usual supervised learning loss. This has connections to the “dark knowledge” idea, ladder networks work is shown in this paper to be a promising technique for scenarios with few labeled examples (but not only). The paper presents two versions of the idea: one which is computationally expensive (and high variance) in that it needs two passes through the same example at a given step, and a temporal ensembling method that is stabler, cheaper computationally but more memory hungry and requires an extra hyper-parameter.    My thoughts on this work are mostly positive. The drawbacks that I see are that the temporal ensembling work requires potentially a lot of memory, and non-trivial infrastructure / book-keeping for imagenet-sized experiments. I am quite confused by the Figure 2 / Section 3.4 experiments about tolerance to noisy labels: it’s *very* incredible to me that by making 90% of the labels random one can still train a classifier that is either 30% accurate or ~78% accurate (depending on whether or not temporal ensembling was used). I don’t see how that can happen, basically.   Minor stuff: Please bold the best-in-category results in your tables.  I think it would be nice to talk about the ramp-up of w(t) in the main paper.  The authors should consider putting the state of the art results for the fully-supervised case in their tables, instead of just their own. I am confused as to why the authors chose not to use more SVHN examples. The stated reason that it’d be “too easy” seems a bit contrived: if they used all examples it would also make it easy to compare to previous work. This work explores taking advantage of the stochasticity of neural network outputs under randomized augmentation and regularization techniques to provide targets for unlabeled data in a semi-supervised setting. This is accomplished by either applying stochastic augmentation and regularization on a single image multiple times per epoch and encouraging the outputs to be similar (Π-model) or by keeping a weighted average of past epoch outputs and penalizing deviations of current network outputs from this running mean (temporal ensembling). The core argument is that these approaches produce ensemble predictions which are likely more accurate than the current network and are thus good targets for unlabeled data. Both approaches seem to work quite well on semi-supervised tasks and some results show that they are almost unbelievably robust to label noise.  The paper is clearly written and provides sufficient details to reproduce these results in addition to providing a public code base. The core idea of the paper is quite interesting and seems to result in higher semi-supervised accuracy than prior work. I also found the attention to and discussion of the effect of different choices of data augmentation to be useful.	  I am a little surprised that a standard supervised network can achieve 30% accuracy on SVHN given 90% random training labels. This would only give 19% correctly labeled data (9% by chance + 10% unaltered). I suppose the other 81% would not provide a consistent training signal such that it is possible, but it does seem quite unintuitive. I tried to look through the github for this experiment but it does not seem to be included.   As for the resistance of Π-model and temporal ensembling to this label noise, I find that somewhat more believable given the large weights placed on the consistency constraint for this task. The authors should really include discussion of w(t) in the main paper. Especially because the tremendous difference in w_max in the incorrect label tolerance experiment (10x for Π-model and 100x for temporal ensembling from the standard setting).  Could the authors comment towards the scalability for larger problems? For ImageNet, you would need to store around 4.8 gigs for the temporal ensembling method or spend 2x as long training with Π-model.  Can the authors discuss sensitivity of this approach to the amount and location of dropout layers in the architecture?   Preliminary rating: I think this is a very interesting paper with quality results and clear presentation.   Minor note: 2nd paragraph of page one 'without neither' -> 'without either' This paper presents a model for semi-supervised learning by encouraging feature invariance to stochastic perturbations of the network and/or inputs.  Two models are described:  One where an invariance term is applied between different instantiations of the model/input a single training step, and a second where invariance is applied to features for the same input point across training steps via a cumulative exponential averaging of the features.  These models evaluated using CIFAR-10 and SVHN, finding decent gains of similar amounts in each case.  An additional application is also explored at the end, showing some tolerance to corrupted labels as well.  The authors also discuss recent work by Sajjadi &al that is very similar in spirit, which I think helps corroborate the findings here.  My largest critique is it would have been nice to see applications on larger datasets as well.  CIFAR and SVHN are fairly small test cases, though adequate for demonstration of the idea.  For cases of unlabelled data especially, it would be good to see tests with on the order of 1M+ data samples, with 1K-10K labeled, as this is a common case when labels are missing.  On a similar note, data augmentations are restricted to only translations and (for CIFAR) horizontal flips.  While ""standard,"" as the paper notes, more augmentations would have been interesting to see --- particularly since the model is designed explicitly to take advantage of random sampling.  Some more details might also pop up, such as the one the paper mentions about handling horizontal flips in different ways between the two model variants.  Rather than restrict the system to a particular set of augmentations, I think it would be interesting to push it further, and see how its performance behaves over a larger array of augmentations and (even fewer) numbers of labels.  Overall, this seems like a simple approach that is getting decent results, though I would have liked to see more and larger experiments to get a better sense for its performance characteristics.    Smaller comment: the paper mentions ""dark knowledge"" a couple times in explaining results, e.g. bottom of p.6.  This is OK for a motivation, but in analyzing the results I think it may be possible to have something more concrete.  For instance, the consistency term encourages feature invariance to the stochastic sampling more strongly than would a classification loss alone. ",0,461
"I reviewed the manuscript on December 5th.  Summary: The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.  Major comments:  The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases.   A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.  My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.  Minor comments:  If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.  - X-axis label is wrong in Figure 2 right.  Measure the transferability of the detector?  - How is \sigma labeled on Figure 5?  - Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.  This takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.  The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.  The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise. This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.  My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples. That being said, the novelty of this paper is still significant.  Minor comment: The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.   ",1,462
"This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning.  The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.  A few comments. 1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes.   2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?  3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks.   4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel.   The experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.  For example, it would be interesting to see whether the proposed method is better than training with only less but high quality data.   From Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful.    This paper looks at how to train if there are significant label noise present. This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.  The second directly integrates out the true label and simply optimizes the p(z|x).  Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated. Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.  - comments: Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes? It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method.   Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities. ",1,463
"In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.  I like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:  - sentiment analysis, 86.5 v.s. 89.7 (accuracy);  - semantic relatedness, 0.32 v.s. 0.25 (MSE);  - textual entailment, 80.5 v.s. 84.6 (accuracy). From the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.  PROS:  - interesting idea: learning structures of sentences adapted for a downstream task.  - well written paper. CONS:  - weak experimental results (do not really support the claim of the authors).  Minor comments: In the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences. Paragraph titles (e.g. in section 3.2) should have a period at the end.  ---------------------------------------------------------------------------------------------------------------------- UPDATE  I am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results. However, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left).   The paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.  I have the following comments: - it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016). - because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice. - because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.  Ref: Andreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016I have not much to add to my pre-review comments. It's a very well written paper with an interesting idea. Lots of people currently want to combine RL with NLP. It is very en vogue. Nobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task. Most people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow. Hence, I believe this direction hasn't shown much promise yet and it's not yet clear it ever will due to the slowness of RL. But many directions need to be explored and maybe eventually they will reach a point where they become relevant.  It is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.  Regardless, it's an interesting exploration, worthy of being discussed at the conference. ",1,464
"I reviewed the manuscript as of December 7th.  Summary: The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.  Major Comments: 1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.  2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.  3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.  4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016).   As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.  Areas to Trim the Paper: - Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text. - Condense Section 2.2.1 and cite heavily. - Figure 2 panels may be overlaid to highlight a comparison. This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of ""attacks"" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles.  The paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on ""clarifai.com"" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).  To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes).  Arguably, The paper still has some weaknesses:   - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that ""One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet."", i.e., the three ResNet-based networks.   - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).   - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.   - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).   - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6).   To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs.  I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples.    There are, however, some concerns:  1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story:   - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3)  - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai)  - Also, here are all the other details and explorations.   2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would've been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.",1,465
"This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper:  -Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks.  -Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this.  -Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer.  Minor comments:  1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea.  2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1   Paper Summary:  Authors investigate identity re-parametrization in the linear and the non linear case.   Detailed comments:  — Linear Residual Network:  The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result.    — Non linear Residual Network:  Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label.   1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify   2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point? In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.    3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ?  A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).  4- What does the construction tell us about the number of layers?   5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.  One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case.   Minors: one line before Eq. (3.1), U \in R ? \times k  ",0,466
"The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.  I see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.  Still, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.  Coming back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.  The paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.  Overall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).  Minor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.This is a parallel work with ALI.  The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result. For feature learning part of BiGAN, there still is a lot of space to improve, compare to standard supervised convnet. This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion. ",1,467
"The paper has two main contributions:  1) Shows that uniform quantization works well with variable length (Huffman) coding  2) Improves fixed-length quantization by proposing the Hessian-weighted k-means, as opposed to standardly used vanilla k-means. The Hessian weighting is well motivated, and it is also explained how to use an efficient approximation ""for free"" when using the Adam optimizer, which is quite neat. As opposed to vanilla k-means, one of the main benefits of this approach (apart from improved performance) is that no tuning on per-layer compression rates is required, as this is achieved for free.  To conclude, I like the paper: (1) is not really novel but it doesn't seem other papers have done this before so it's nice to know it works well, and (2) is quite neat and also works well. The paper is easy to follow, results are good. My only complaint is that it's a bit too long.  Minor note - I still don't understand the parts about storing ""additional bits for each binary codeword for layer indication"" when doing layer-by-layer quantization. What's the problem of just having an array of quantized weight values for each layer, i.e. q[0][:] would store all quantized weights for layer 0, q[1][:] for layer 1 etc, and for each layer you would have the codebook. So the only overhead over joint quantization is storing the codebook for each layer, which is insignificant. I don't understand the ""additional bit"" part. But anyway, this is really not a important as I don't think it affects the paper at all, just authors might want to additionally clarify this point (maybe I'm missing something obvious, but if I am then it's likely some other people will as well).  This paper proposes a novel neural network compression technique. The goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss. It assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network. In contrast to previous work (Han et al. 2015a, Gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation.   Unfortunately, the submitted paper is 20 pages, rather than the 8 recommended. The length of the paper seems unjustified to me, since the first three sections (first five pages) are very generic and redundant can be largely compressed or skipped (including figures 1 and 2). Although not a strict requirement by the submission guidelines, I would suggest the authors to compress their paper to 8 pages, this will improve the readability of the paper.  To take into account the impact on the network’s loss the authors propose to use a second order approximation of the cost function of the loss. In the case of weights that originally constitute a local minimum of the loss, this leads to a formulation of the impact of the weight quantization on the loss in terms of a weighted k-means clustering objective, where the weights are derived from the hessian of the loss function at the original weights. The hessian can be computed efficiently using a back-propagation algorithm similar to that used to compute the gradient, as shown in cited work from the literature.  The authors also propose to alternatively use a second-order moment term used by the Adam optimisation algorithm, since it can be loosely interpreted as an approximate Hessian.   In section 4.5 the authors argue that with their approach it is more natural to quantise weights across all layers together, due to the hessian weighting which takes into account the variable impact across layers of quantisation errors on the network performance.  The last statement in this section, however, was not clear to me:  “In such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.” Perhaps the authors could elaborate a bit more on this point?  In section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process.  The first method described by the authors (based on previous work), is uniform quantisation of the weight space, which is then further optimised by their hessian-weighted clustering procedure from section 4.  For the case of nonuniform codeword lengths to encode the cluster indices, the authors develop a modification of the Hessian weighted k-means algorithm in which the code length of each cluster is also taken into account, weighted by a factor lambda. Different values of lambda give rise to different compression-accuracy trade-offs, and the authors propose to cluster weights for a variety of lambda values and then pick the most accurate solution obtained, given a certain compression budget.    In section 6 the authors report a number of experimental results that were obtained with the proposed methods, and compare these results to those obtained by the layer-wise compression technique of Han et al 2015, and to the uncompressed models.  For these experiments the authors used three datasets, MNIST, CIFAR10 and ImageNet, with data-set specific architectures taken from the literature.  These results suggest a consistent and significant advantage of the proposed method over the work of Han et al. Comparison to the work of Gong et al 2014 is not made. The results illustrate the advantage of the hessian weighted k-means clustering criterion, and the advantages of the variable bitrate cluster encoding.    In conclusion I would say that this is quite interesting work, although the technical novelty seems limited (but I’m not a quantisation expert). Interestingly, the proposed techniques do not seem specific to deep conv nets, but rather generically applicable to quantisation of parameters of any model with an associated cost function for which a locally quadratic approximation can be formulated. It would be useful if the authors would discuss this point in their paper. This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters.  The authors assume that the network is already pruned and aim for compressing the non-pruned parameters. The problem of network compression is a well-motivated problem and of interest to the ICLR community.   The main drawback of the paper is its novelty. The paper is heavily built on the results of Han 2015 and only marginally extends Han 2015 to overcome its drawbacks. It should be noted that the proposed method in this paper has not been proposed before.   The paper is well-structured and easy to follow. Although it heavily builds on Han 2015, it is still much longer than Han 2015. I believe that there is still some redundancy in the paper. The experiments section starts on Page 12 whereas for Han 2015 the experiments start on page 5. Therefore, I believe much of the introductory text is redundant and can be efficiently cut.   Experimental results in the paper show good compression performance compared to Han 2015 while losing very little accuracy. Can the authors mention why there is no comparison with Hang 2015 on ResNet in Table 1?  Some comments: 1) It is not clear whether the procedure depicted in figure 1 is the authors’ contribution or has been in the literature. 2) In section 4.1 the authors approximate the hessian matrix with a diagonal matrix. Can the authors please explain how this approximation affects the final compression? Also how much does one lose by making such an approximation?  minor typos (These are for the revised version of the paper): 1) Page 2, Parag 3, 3rd line from the end: fined-tuned -> fine-tuned 2) Page 2, one para to the end, last line: assigned for -> assigned to 3) Page 5, line 2, same as above 4) Page 8, Section 5, Line 3: explore -> explored",0,468
"This paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass. As I understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity). They evaluate their method on both AlexNet and GoogLeNet as well as on various platforms. The authors make code available online. The paper is well written and does a good job of putting this work in the context of past model reduction techniques.  My main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work. The authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.The authors provide a well engineered solution to exploiting sparsity in convolutional layers of a deep network by recasting it as sparse matrix-vector multiplication. This leads to very nice speedups and the analysis of when this is possible is also useful for practitioners. My main concern with this paper is that the ""research"" aspect of it seems rather minimal, and it's mostly about performance engineering and comparisons. It is upto the area chairs to decide how well such a paper fits in at ICLR.The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs.  The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.  The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly.  It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work.  One point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly.  Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in Section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5). ",0,469
"Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.  In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.  Comments: - sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc. - sec 2.2: two comma's - text flow eq 6: please refer to appendix with the closed-form KL divergence - ""The v's are sampled via"" => ""In the posterior, the v's are sampled via"". It's not clear you're talking about the posterior here, instead of the prior. - The last paragraph of section 4 is great. - Sec 7.1: ""Density estimation"" => Technically you're also doing mass estimation. - Sec 7.1: 100 IS samples is a bit on the low side.  - Figure 3(f). Interesting that k-NN works so well on raw pixels.  The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion. After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are ""used"" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model. With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable. Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway. The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.  There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic ""black-box"" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML.  Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.  I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention.  The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.  Overall, I found this to be an interesting paper, it would be a good fit for ICLR. ",1,470
"The paper discuss a ""batch"" method for RL setup to improve chat-bots. The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices.   I find the writing clear, and the algorithm a natural extension of the online version.  Below are some constructive remarks: - Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option: - For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function. - section 2.2:    sentence before last: s' is not defined.     last sentence: missing ""... in the stochastic case."" at the end. - Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""    The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots. The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on ""clarification regarding batch vs. online setting""). The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning – training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.  While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand.   My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.  References:  Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. ""A Network-based End-to-End Trainable Task-oriented Dialogue System."" arXiv preprint arXiv:1604.04562 (2016). ",0,471
" I'd like to thank the authors for their detailed response to my questions.  The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.  The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it.   Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.  The authors also propose a fast encoding scheme for their proposed method.  The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. ""Converting"" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).    In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail.   In the abstract the authors mention “extensive experimental results …”, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in “CUDA C++ with extreme efficiency” (page 10). But more importantly, I find it hard to interpret and compare the results: The paper reports accuracy and and normalized mutual information for a image retrieval / clustering task where the proposed SRSC is used as a feature extractor. The improvements relative to standard Sparse Coding seem very small  (often < 1% in terms of NMI; it looks more promising in terms of accuracy) and if I understand the description on page 11 correctly, than the test set was used to select some hyperparameters (the best similarity measure for clustering step)? There is no comparisons to other baselines / state of the art image clustering methods. Besides of providing features for a small scale image clustering system, are there maybe ways to more directly evaluate the properties and qualities of  a sparse coding approach? E.g. reconstruction error / sparsity; maybe even denoising performance?   In summary, I think in it current form the paper lacks the evaluation and experimental results for an ICLR publication. Intuitively, I agree with the authors that the proposed regularization is an interesting direction, but I don’t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where SRSC is used as a feature extractor are very modest.The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation).  Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance.  Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly.   Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective. ",1,472
"This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding.   I see two main drawbacks from this framework: The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets.  The augmented loss is heavily “engineered” to produce the desired result of parameter tying. It’s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way.   Nevertheless the argument is very interesting, and clearly written. The simulated results indeed validate the argument, and the PTB results seem promising.  Minor comments: Section 3: Can you clarify if y~ is conditioned on the t example or on the entire history. Eq. 3.5: i is enumerated over V (not |V|) This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. Experiments on PTB shows significant improvement. The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting. I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted. Have you considered using character or sub-word units in that context?  This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.  It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done.  The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1.  Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn't the RHS in Eq 3.5 be \sum \tilde{y_{t,i}}(\frac{\hat{y}_t}{\tilde{y_{t,i}}} - e_i) ?",0,473
"This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach’s chorales) and propose new rules for the student to learn so that it could improve.  The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here.   I found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way.   Overall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions.Summary:  The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and ""satisfactory gaps"" have been generated in terms of sets of learned rules.   1. Quality:  a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable. The authors also demonstrate an effective memory selection to speed up the learning.   b) Flaws: The paper only discussed N<=5, which might limit the learning and interpretation capacities of the proposed model, failing to capture long-distance dependence of music. (In the replies to questions, the authors mentioned they had experimented with max N=10, but I'm not sure why related results were not included in the paper). Besides the elaborated interpretation of results, a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive.  2. Clarity:  a) Pros: The paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory. Proper analogies and examples help the reader perceive the ideas more easily.   b) Cons: Although detailed definitions can be found in the authors' previous MUS-ROVER I papers, it would be great if they had described the optimization more clearly (in Figure 1. and related parts).  The ""(Conceptual-Hierarchy Filter)"" row in equations (3): the prime symbol should appear in the subscript.  3. Originality: The representation of music concepts and rules is still an open area, the paper investigate the topic in a novel way. It illustrates an alternative besides other interpretable feature learning methods such as autoencoders, GAN, etc.   4. Significance: It is good to see some corresponding interpretations for the learned rules from music theory. The authors mentioned students in music could and should be involved in the self-learning loop to interact, which is very interesting. I hope their advantages can be combined in the practice of music theory teaching and learning. After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper.  Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity.  I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..). I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc. A short conference paper just seems to me to be too dense a format for giving this project the description it merits. If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that.   If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score. I do think that the effort would be better invested in turning this into a long (and clearer) journal submission.  [Addendum: based on discussions here & revisions, I have revised my score] ",0,474
"This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations).  A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice.  The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode. Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference.  This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets.   The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!  The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)  I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment “variance of the derivatives of F^{-1}” below. I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.)  Another concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive.   Minor: the second paragraph of Section 3 needs a reference to Appendix A. Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101).   Overall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:  Graham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the 26th International Conference on Machine Learning (ICML), 1025–1032, 2009.  Discussion of this should certainly be added.  ",0,475
"This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.  Pros + This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution. + It builds nicely on the preliminary results in Ba & Caruana, 2014.  Cons - It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.  Section 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).  The paper does not follow the ICLR citation style.  Quoting from the template:  ""When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”).""  There are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.  necessary when training student models with more than 1 convolutional layers → necessary when training student models with more than 1 convolutional layer  remaining 10,000 images as validation set → remaining 10,000 images as the validation set  evaluate the ensemble’s predictions (logits) on these samples, and save all data → evaluated the ensemble’s predictions (logits) on these samples, and saved all data  more detail about hyperparamter optimization → more detail about hyperparameter optimization  We trained 129 deep CNN models with spearmint → We trained 129 deep CNN models with Spearmint  The best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. → The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.  the sizes and architectures of three best models → the sizes and architectures of the three best models  clearly suggests that convolutional is critical →  clearly suggests that convolution is critical  similarly from the hyperparameter-opimizer’s point of view → similarly from the hyperparameter-optimizer’s point of view  This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters.  To this end the authors conducted a series of experiments on the CIFAR10 dataset. They find that there is a significant performance gap between the two approaches, in favour of deep CNNs.  The experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner. They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.  My take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.  An interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper.  It was not quite clear to me why were the experiments were limited to use  30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.  The authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment.  Could the authors argue why they think this to be the case?  One could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture.  Since MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance. This issue could for example be examined on ImageNet when varying the amount of training data. Also, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.  Experiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.  Description. This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used.   Strong  points. - The experiments are carefully done with thorough selection of hyperparameters.  - The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014). - The paper is well and clearly written.  Weak points: - CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?  Originality: - This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.  Quality: - The experiments are well done.  Clarity: - The paper is well written and clear.  Significance: - The results go against some of the conclusions from previous work, so should be published and discussed.  Overall: Experimental paper with interesting results. Well written. Solid experiments.  ",0,476
"Pros :  - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation  Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first. Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.  The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.  Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).  In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well."" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.",1,478
"This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers.  There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments:  1. Actually the model in the paper is not single model, it proposed two models. One consists of ""reading"", ""writing"", ""adaptive computation"" and "" Answer module 2"", the other one is ""reading"", ""composing"", ""writing"", ""gate querying"" and ""Answer module 1"". Based on the method section and the experiment, it seems the ""adaptive computation"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.  2. What is the MLP setting in the composing module?   3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers?  4. It needs more ablation study about using different T such as T=1,2..  5. According to my understanding, for the adaptive computation,  it would stop when the P_T <0. So what is the distribution of T in the testing data?First I would like to apologize for the delay in reviewing.  Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.  In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.  The dataset description in section 2.2, should be moved to section 4 where the other datasets are described.",1,479
"This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents. The paper is well written, clear in its presentation and backed up by good experiments. They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states, allowing more accurate classification with less training data. They also show how the information learned by the network is interpretable and organised in a hierarchy.  Weaknesses: - a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper. - moreover, a discussion on how this approach could scale to more challenging scenarios ""involving animals"" and visual input for instance and more general ""behaviours"" is also missing; The criticism here is pointed at the fact that the title/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios, making the original claim a little bit far fetched unless its backed up by additional evidence. Using ""Insects"", or ""fruit flies"" would be more appropriate than ""animals"".The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.  The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction. Motion is discretized and predicted using classification. The model is trained on classification loss combined with a loss on motion prediction. The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.  The idea of leveraging predictions to train feature representations for discrimination is not new. However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.  My biggest concern is with the experimental evaluation. The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way. However, quantitative evaluation is rarer.  - On the fly application, the authors compare the classification performance with another method previously published by the first author. - Again on the fly application, the performance gain on motion prediction in figure 5c looks small compared to the baseline. I am not sure it is significant. - I did not see any recognition results on the handwriting application. Has this part not been evaluated?  Figure 5a is difficult to understand and to interpret. The term ""BesNet"" is used here without any introduction.  Figure 4 seems to tell multiple and different stories. I'd suggest splitting it into at least two different figures. While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between).   The overall approach is interesting: all three of the key techniques (aux. tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.  I found some of the results hard to understand/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations.   It may be worthwhile very briefly mentioning the relationship of ""diagonal"" connections to other emerging terms for similar ideas (e.g. skip connections, etc). ""Skip"" seems to me to be accurate regardless of how you draw the network, whereas ""diagonal"" only makes sense for certain visual layouts.  In response to comment in the discussion below: ""leading to less over-segmentation of action bouts"" (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper. I have assumed that ""per-bout"" refers to ""per-action event"", but now I am not certain that I have understood this correctly (i.e. can a ""bout"" last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly.  In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours? i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being ""long-term""). If easy to investigate this, I would add a comment about it; if this is hard to investigate, it's probably not worth it at this point, although it's something you might want to look at in future.  In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what ""should"" work and what actually works, as I am sure the authors are aware. (Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do). What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories. The claims made should correspond to this.   I would consider adjusting my rating to a 7 depending on future revisions. ",0,480
"This paper is a well written paper. This paper can be divided into 2 parts: 1.Adversary training on ImageNet  2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity  For part [1], I don’t think training without clean example will not make reasonable ImageNet level model. Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian’s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.  For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation ""FGSM examples are most transferable"".    In this part the authors raise many interesting problems or guess, but lack theoretical explanations.   Overall I think these empirical observations are useful for future work.   This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments. The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.  Pros: For the first time in the literature, this paper proposed the concept of ‘label leaking’. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line. Using the ratio of 'clean accuracy' over ‘adversarial accuracy’ as the measure of robust is more reasonable compared to the existing works in the literature.   Cons: Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the ‘large-scale’ in the title and in the introduction seems improper.  Basically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets. This paper has two main contributions:  (1) Applying adversarial training to imagenet, a larger dataset than previously considered  (2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.  This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.",0,481
"This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.  The authors also did address the questions of the reviewers.  My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.  -----  This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.  Strengths: - Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning. - Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results. - Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines. - Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.  Weaknesses: - The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage. - The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are ""significant"" (even in an informal sense). - The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?  I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.  For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:  -The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.  - To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.  - There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye).   Overall, a nice paper.",0,482
"This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.  Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.  The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.  My only ""gripe"" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the ""recurrent"" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the ""recurrent"" part really helps.   This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns, thus helping to prune irrelevant information from the video and improve action recognition.  The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. It has also shown promise for helping action clip classification.  The paper would benefit from a discussion on the role of context in attention. For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?  One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair. The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.  Is there a way to combine the context and attention without concatenation? The rational for concatenating the features extracted from the original clip, and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance.  The authors should also mention the current state-of-the-art results in Table 4, for comparison.  # Other comments: # Abstract - Typo: `mixed with irrelevant ...' ``Time consistency in videos ... expands the temporal domain from few frames to seconds'' - These two points are not clear, probably need a re-write.  # Contributions - 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though..   # Section 3.1 The number of fixation points is controlled to be fixed for each frame - how is this done?  In practice we freeze the layers of the C3D network to values pretrained by Tran etal. What happens when you allow gradients to flow back to the C3D layers? Is it not better to allow the features to be best tuned for the final task?  The precise way in which the features are concatenated needs to be clarified in section 3.4.  Minor typo: `we added them trained central bias'The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.  quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?  You claim that your ""results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.  clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.  Other issues:  You cite Kümmerer et. al 2015 as a model which ""learns ... indirectly rather than from explicit information of where humans look"", however the their model has been trained on fixation data using maximum-likelihood.  Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.",1,483
"This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.  The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work.  In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.  For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.  So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.  For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.  It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.  Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.  In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.  I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.  A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.  However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling.  The paper does not explicitly discuss whether their results provide insight into this behavior.  Overall, I think that the paper attacks an important problem in an interesting way.  It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling.  The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).  While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation. My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.  To summarize my understanding of the key theorem 1 result: - The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition. - In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.  If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.    While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:  On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk.   On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)  In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios.   Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature.   This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases.   This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.  It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.         ",0,484
"SUMMARY  This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers.   PROS  Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation.   CONS  The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less).   COMMENTS  It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks.  Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks.   MINOR COMMENTS  - Figure 1 could be referenced first in the text.   - ``Color coded'' where the color codes what?  - Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold.  - On page 5, mention how the orthogonal projection on S_k is realized in the network.  - On page 6 ``divided into segments'' here `segments' is maybe not the best word.  - On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean?  Summary: In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space.  They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.  They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy.   Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.  Comments:  The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction.  However, the current version of the paper could use some more work:  The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.  It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression  The theory sections could do with being more clearly written -- I’m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that “accurately and efficiently” preserves a monotonic chain, etc. The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call ""monotonic chains of linear segments"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.  While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very ""compatible"". In particular, I have three main concerns with respect to the results presented in this paper:  (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?  (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.  (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the ""bound for this case is very loose"". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.  I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.   Minor comments:   - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using ""the difference between the ground truth distance ... and the distance computed by the network"" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?",1,485
"This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.  This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.  It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.  Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?  Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.  The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.  The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.   Some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503.  Gideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955–984. David Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598. Joseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases to Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853– 863. Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96–103. The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups.   Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ",1,486
"From my original comments:  The results looks good but the baselines proposed are quite bad.  For instance in the table 2 ""Misclassification rate for a 784-1024-1024-1024-10 "" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see ""significant"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...  In CIFAR-10 experiments, i do not understand  why ""Sparsely-Connected 90% + Single-Precision Floating-Point"" is worse than ""Sparsely-Connected 90% + BinaryConnect"". So it is better to use binary than float.   Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision.   In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.  ----  The authors reply still does not convince me.  I still think that the same technique should be applied on more challenging scenarios.  The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks.  The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements. The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation.  This explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections. Algorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text. Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: Experimental results look reasonable, validated on 3 tasks.  References could be improved, for example I would rather see Rumelhart's paper cited for back-propagation than the Deep Learning book. ",0,487
"*** Paper Summary ***  This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.  *** Review Summary ***  The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.  *** Detailed Review ***  The paper reads well. I have only a few comments regarding experiments and link to prior resarch:  Experiments:  - In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012? - As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)? - I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?  Related Work:  I think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.  Also it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.  *** References ***  Marginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger. Stacked Denoising Autoencoders. Pascal Vincent. JMLR 2011. Learning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011. Learning Deep Architectures for AI, Yoshua Bengio 2009 Large Scale Transductive SVMs. Ronan Collobert et al 2006 Optimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish.   I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. The authors propose to apply virtual adversarial training to semi-supervised classification.  It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.  In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).  Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).  Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.",1,488
"This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.  Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.  One minor red flag:  - The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.  Two writing comments: - I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode. - Saying that ""LSTM auto-encoders are more effective at encoding word order than word content"" doesn't really make sense. These two quantities aren't comparable. This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).  There are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper bound which can be deduced from natural language statistics?' This is investigated from one angle in Section 7, but I would have preferred to the effect of natural language statistics discussed up front rather than presented as the explanation to a 'surprising' observation. I had a similar reaction to the word-order experiments.  Most of the interesting results, in my opinion, are about the ED model. It is fascinating that the LSTM encoder does not seem to rely on natural-language ordering statistics -- it seems like doing so should be a big win in terms of per-parameter expressivity. I also think that it's strange that word content accuracy begins to drop for high-dimensional embeddings. I suppose this could be investigated by handicapping the decoder.  Overall, this is a very nice paper investigating some aspects of the information content stored in various types of sentence embeddings. I recommend acceptance. The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.  Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.  While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).  The word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is ""Generating Text with Recurrent Neural Networks"" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as ""debagging.""  Although this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.",1,489
"This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs.  The idea is appealing in general for context biasing and the specific approach appears quite simple.  The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models, but not as a mixture model, as in this paper.  The paper is clearly written and the results seem promising. The new dataset the authors created (WikiText) also seems of high interest.   A comment regarding notation: The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i)  This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words. It would be helpful to use different symbol for the two objects.  This work is basically a combined pointer network applied on language modelling.  The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences.  Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words.   Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate.  This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling.  I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results.   In addition, the new WikiText language modelling dataset is very interesting.  It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset.   Overall, this is a well-written paper. I recommend it to be accepted.This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.  The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.  While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1: “Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.” As far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?  In addition, quoting from section 3 which describes the model of Gulcehre et al.: “Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use” This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.  Finally, in the following quote, also from section 3:  “The pointer network is not used as a source of information for the switching network as in our model.”  It is not clear what the authors mean by “source of information” here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.  With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?  I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part. ",0,490
"The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.  An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results. The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.  The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an ""uncertain"" label.   While the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity):  (a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and  (b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting.   The reason why I am confused is that ""The standard approach to supervised classification"", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.  === question: In the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest ""Assuming the class is chosen according to p(y|X, θ)""), or the more standard deterministic classifier argmax_y P(y|x, theta) ?  As far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification).   === comments: - The section ""allowing uncertainty in the decision"" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) ""Classification with a Reject Option using a Hinge Loss"" or Sayedi et al. (2010) ""Trading off Mistakes and Don’t Know Predictions"".  - there seems to be a ""-"" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.  - The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.  Final comments: I think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the ""smoothed"" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional ""uncertain"" label. I increase my score from 5 to 6.The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values.  Consequently, the optimized upper bound (log-loss) gets looser.   As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue.   I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper.   There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve).  [1] Training Highly Multiclass Classifiers, Gupta et al. 2014. [2] Direct Loss Minimization for Structured Prediction, McAllester et al.  [3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.  Final comment: I believe the material presented in this paper is of interest to a wide audience at ICLR. The problem studied is interesting and the proposed approach is sound.  I recommend to accept the paper and increase my score (from 7 to 8).  ",0,493
"The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.  It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines.   I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2): -	Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.   -	Section 2 describes how the data was splits into train and test set. One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures?    It also unclear how the deep learning models are applied. Let’s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector – and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input?   My another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is.   Did the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models?   Overall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer – it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset. From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset.   My current rating is a weak reject, but if the authors address my concerns I would change to an accept.   ",1,494
"This paper shows:    1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.   2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.   3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.  The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:  1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?  2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?SUMMARY  This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer.   PROS  The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth.   CONS The main result appears to address only strongly convex univariate functions.   SPECIFIC COMMENTS   - Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently.  I still have not read the revision and maybe you have already addressed some of these points there.   - The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview.   - In Lemma 3, there is an i that should be x  - In Theorem 4, ``\tilde f'' is missing the (x).   - Theorem 11, the lower bound always increases with L ?   - In Theorem 11, \bf x\in [0,1]^d?    ",0,495
"This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).  Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012.   The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.  Overall this paper presents a strong and novel model with promising experimental results.    On a minor note, I have few remarks/complaints about the writing and the related work:  - In the introduction: “One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim. “For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references. “in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996  - in the related work: “A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks. “The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.  Missing references: “Recurrent neural network based language model.”, Mikolov et al. 2010 “Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996 “Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007 “Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993 The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.  Pros: - Paper is well-motivated, exceptionally well-composed - Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation - The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative. Cons: - In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated. - It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.   This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.  Question) Can you extend it to bidirectional RNN?  ",0,496
"This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.  Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.  This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout “inference gap” which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.  One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I’d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.  MC dropout on page 8 is not defined, please define.  On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?  From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.  Couple of typos: - Pg. 2 “ … x is he input …” -> “ … x is the input …” - Pg. 5 “ … as defined in (1), is …” -> ref. to (1) is not right at two places in this paragraph  Overall it is a good paper, I think should be accepted and discussed at the conference.  summary  The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.  The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights). This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.  Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.  Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)   The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.  The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.  The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.  p6 line 8 typo: expecatation",1,498
"*** Paper Summary ***  The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.  *** Review Summary ***  Pros:  - I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight.  - LM and MT results are excellent.  Cons:   - The paper could be better written. It is too long for the conference format and need refocussing.  - On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it). - on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.  I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.  *** Detailed Review ***  Multiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.  Spending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that ""hypernetwork"" help the reader understand better what the proposed architecture compared to multiplicative interactions.  In section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations.   The work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015).   For RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.  Some of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.  The results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).  The MT experiments are insufficiently discussed in the main text.  Overall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.  *** References ***  M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, ""First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,""IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.  Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, ""Model Compression,"" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.  Dark knowledge, G Hinton, O Vinyals, J Dean 2014  W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.      --pros  This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.  --cons  The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.  --minor question,   The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly? Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way. Unfortunately, as the results show, the authors could not get better results with less parameters. However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.  The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent. E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.  Could the authors provide the num. of trainable parameters for Table 6?  Probably presenting less results could also improve the readability. Only marginal accept due to the writing style. ",1,499
"This paper proposes a multilayer architecture based upon stacking non-negative matrix factorization modules and fine-tuning the entire architecture with reconstruction error. Experiments on text classification and MNIST reconstruction demonstrate the approach.  During layer-wise initialization of the multilayer architecture NMF is performed to obtain a low-rank approximation to the input. The output of an NMF linear transform passes through a nonlinearity to form the input to the subsequent layer. These nonlinear outputs of a layer are K = f(H) where f(.) is a nonlinear function and H are linear responses of the input. During joint network training, a squared reconstruction error objective is used. Decoding the final hidden layer representation back into the input space is performed with explicit inversions of the nonlinear function f(.). Overall, the notation and description of the multi-layer architecture (section 3) is quite unclear. It would be difficult to implement the proposed architecture based only upon this description  Experiments on Reuters text classification and MNIST primarily focus on reconstruction error and visualizing similarities discovered by the model. The text similarities are interesting, but showing a single learned concept does not sufficiently demonstrate the model's ability to learn interesting structure. MNIST visualizations are again interesting, but the lack of MNIST classification results is strange given the popularity of the dataset. Finally, no experiments compare to other models e.g. simple sparse auto-encoders to serve as a baseline for the proposed algorithm.    Notes: -The abstract should be included as part of the paper - Matlab notation is paragraph 2 of section 2 is a bit strange. Standard linear algebra notation (e.g. I instead of eye) is more clear in this case - 'smoothen' -> smooth or apply smoothing to  Summary: - A stacking architecture based upon NMF is interesting - The proposed architecture is not described well. Others would have difficulty replicating the model. - Experiments do not compare to sufficient baselines or other layer-wise feature learners. - Experiments and visualizations do not sufficiently demonstrate the claim that NMF-based feature hierarchies are easier to interpretThe paper proposes to stack NMF models on top of each other. At each level, a non-linear function of normalized decomposition coefficients is used and decomposed using another NMF.  This is essentially an instance of a deep belief network, where the unsupervised learning part is done using NMF, which, to the best of my knowledge had not been done before.  The new method is then applied to document data where a hierarchy of topics seems to be discovered. Applications are also shown on reconstructing digits.  The extended abstract however does not give many details on all the specifics of the method.  Comments: -It would have been nice (a) to relate the hierachy to existing topic models [A,B], and (b) to see more topics. -On Figure 2, why are reconstruction errors decreasing with the number of features?  -On the digits, the differences between shallow and deep networks are not clear.  [A] D. Blei, T. Griffiths, and M. Jordan.   The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies.   Journal of the ACM, 57:2 1–30, 2010.    [B] R. Jenatton, J. Mairal, G. Obozinski, F. Bach. Proximal Methods for Hierarchical Sparse Coding. Journal of Machine Learning Research, 12, 2297-2334, 2011.  Pros: -Interesting idea of stacking NMFs.  Cons: -Experimental results are interesting but not great. What is exactly achieved is not clear.",1,5000
"This paper presents a theoretical analysis and empirical validation of a novel view of feature extraction systems based on the idea of Nystrom sampling for kernel methods.  The main idea is to analyze the kernel matrix for a feature space defined by an off-the-shelf feature extraction system.  In such a system, a bound is identified for the error in representing the 'full' dictionary composed of all data points by a Nystrom approximated version (i.e., represented by subsampling the data points randomly).  The bound is then extended to show that the approximate kernel matrix obtained using the Nystrom-sampled dictionary is close to the true kernel matrix, and it is argued that the quality of the approximation is a reasonable proxy for the classification error we can expect after training.  It is shown that this approximation model qualitatively predicts the monotonic rise in accuracy of feature extraction with larger dictionaries and saturation of performance in experiments.  This is a short paper, but the main idea and analysis are interesting.  It is nice to have some theoretical machinery to talk about the empirical finding of rising, saturating performance.  In some places I think more detail could have been useful.  One undiscussed point is the fact that many dictionary-learning methods do more than populate the dictionary with exemplars so it's possible that a 'learning' method might do substantially better (perhaps reaching top performance much sooner).  This doesn't appear to be terribly important in low-dimensional spaces where sampling strategies work about as well as learning, but could be critical for high-dimensional spaces (where sampling might asymptote much more slowly than learning).  It seems worth explaining the limitations of this analysis and how it relates to learning.    A few other questions / comments:  The calibration of constants for the bound in the experiments was not clear to me.  How is the mapping from the bound (Eq. 2) to classification accuracy actually done?  The empirical validation of the lower bound relies on a calibration procedure that, as I understand it, effectively ends up rescaling a fixed-shape curve to fit observed trend in accuracy on the real problem.  As a result, it seems like we could come up with a 'nonsense' bound that happened to have such a shape and then make a similar empirical claim.  Is there a way to extend the analysis to rule this out?  Or perhaps I misunderstand the origin of the shape of this curve.  Pros:   (1)  A novel view of feature extraction that appears to yield a reasonable explanation for the widely observed performance curves of these methods is presented.  I don't know how much profit this view might yield, but perhaps that will be made clear by the 'overshooting' method foreshadowed in the conclusion. (2) A pleasingly short read adequate to cover the main idea.  (Though a few more details might be nice.)  Cons: (1)  How this bound relates to the more common case of 'trained' dictionaries is unclear. (2)  The empirical validation shows the basic relationship qualitatively, but it is possible that this does not adequately validate the theoretical ideas and their connection to the observed phenomenon.The authors provide an analysis of the accuracy bounds of feature coding + linear classifier pipelines. They predict an approximate accuracy bound given the dictionary size and correctly estimate the phenomenon observed in the literature where accuracy increases with dictionary size but also saturates.  Pros: - Demonstrates limitations of shallow models and analytically justifies the use of deeper models.",0,5001
"The paper studies techniques for inferring a model of entities and relations capable of performing basic types of semantic inference (e.g., predicting if a specific relation holds for a given pair of entities).  The models exploit different types of embeddings  of entities and relations.  The topic of the paper is interesting and the contribution seems quite sufficient for a workshop paper. It should motivate an interesting discussion on how these models can be generalized to be applied to more complex datasets and semantic tasks (e.g.,  inferring these representation from natural language texts), and, in general,  on representation induction methods for semantic tasks.   The only concern I have about this paper is that it does not seem to properly cite much of the previous work on related subjects.  Though it mentions techniques for clustering semantically similar expressions, it seems to suggest that there has not been much work on inducing, e.g., subsumptions. However,  there has been a lot of  previous research on learning entailment (aka inference) rules (e.g.,  Chkolvsky and Pantel 2004; Berant et al, ACL 2011;  Nakashole et al, ACL 2012). Even more importantly, some of the very related work on embedding relations is not mentioned, e.g.,  Bordes et al (AAAI 2011), or, very closely related,  Jenatton et al (NIPS 2012).  However, these omissions may be understandable given the short format of the paper.   Pros: --  Interesting topics --  Fairly convincing experimental results Cons: --  Previous work on embedding relations is not discussed.This paper presents a framework for open information extraction.  This problem is usually tackled either via distant weak supervision from a knowledge base (providing structure and relational schemas) or in a totally unsupervised fashion (without any pre-defined schemas). The present approach aims at combining both trends with the introduction of universal schemas that can blend pre-defined ones from knowledge bases and uncertain ones extracted from free text.  This paper is very ambitious and interesting. The goal of bridging knowledges bases and text for information extraction is great, and this paper seems to go in the right direction. The experiments seem to show that mixing data sources is beneficial.  The idea of asymmetric implicature among relation is appealing but its implementation in the model remains unclear. How common is it that a tuple shares many relations? One can not tell anything for relations for which corresponding tuples are disjoint from the rest.   The main weakness of the system as it is presented here is that it relies on the fact that entities constituting tuples from the knowledge base (Freebase here) and tuples extracted from the text have been exactly matched beforehand. This is a huge limitation before any real application, because this involves solving a complex named entity recognition - word sense disambiguation - coreference resolution problem.  Is there any parameter sharing between latent feature vectors of entities and tuples (=pairs of entities)? And between relation vectors and neighbors weights?  Minor: the notation for the set of observed fact disappeared.  Pros: - great motivation and research direction - model and experiments are sound  Cons: - lack of details - many unanswered questions remain to apply it on real-world data.",0,5002
"his paper proposes a dataset to benchmark the correspodence problem in computer vision. The dataset consists of image patches that have groundtruth matching pairs (using separate algorithms). Extensive experiments show that RBMs perform well compared to hand-crafted features.  I like the idea of using itermediate evaluation metrics to measure the progress of unsupervised feature learning and deep learning. That said, comparing the methods on noisy groundtruth (results of other algorithms) may have some bias.  The experiments could be made stronger if algorithms such as Autoencoders or Kmeans (Coates et al, 2011, An Analysis of Single-Layer Networks in Unsupervised Feature Learning) are considered.  If we can consider the groundtruth as clean, will supervised learning a deep (convolutional) network using the groundtruth produce better results?This paper is a survey of unsupervised learning techniques applied to the unsupervised task of descriptor matching. Various methods such as Gaussian RBMs, sparse RBMs, and mcRBMs were applied to image patches and the resulting feature vectors were used in a matching task. These methods were compared to standard hand-crafted descriptors such as SIFT, SURF, etc.  Pros Provides a survey of descriptors for matching pairs of image patches.  Cons It is not clear what the purpose of the paper is. The paper compares several learning algorithms on the task of what essentially seems like clustering image patches to find their correspondences. The ground truth correspondences of the dataset were found by clustering the image patches to find correspondences... In this paper, simple clustering methods were not compared to such as kmeans or sparse coding which are less complicated models than RBMs and are meant for finding correspondences. Additionally, training in a supervised way makes much more sense for finding correspondences.  It is not clear from the paper alone what is considered at match between descriptors? Is it the distance being below a threshold, the pair of descriptors being closer than any other pair of descriptors, etc.?  The preprocessing of the image patches seems different for each method. This could lead to wildly different scales of the input pixels and thus the corresponding representations of the various methods.  In section 3.3 it is mentioned that it is surprising that L1 normalization works better because sparsity hurts classification typically. However, the sparsity in the paper is directly before the distance calculation, and not before being fed as input to a classifier which is a different setup and would thus be expected to behave differently with sparsity. This is the typical setup in which sparsity is found to hurt classification performance because information is being thrown away before the classifier is used.  Novelty and Quality: This paper is not novel in that it is survey of prior work applied to matching descriptors. It is well written but does not appear to apply to a wide audience as other papers have done a comparison of unsupervised methods in the past, for example: - A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning. In Proc. AISTATS, 2011. - A. Coates and A. Ng. The importance of encoding versus training with sparse coding and vector quanti- zation. In Proc. ICML, 2011.This paper proposes to evaluate feature learning algorithms by using a low-level vision task, namely  image patch matching. The authors compare three feature learning algorithms, GRBM. spGRBM and mcRBM against engineered features like SIFT and others. The empirical results unfortunately show that the learned features are not very competitive for this task.   Overall, the paper does not propose any new algorithm and does not improve performance on any task. It does raise an interesting question though which is how to assess feature learning algorithms. This is a core problem in the field and its solution could help a) assessing which feature learning methods are better and b) designing algorithms that produce better features (because we would have better loss functions to train them). Unfortunately, this work is too preliminary to advance our understanding towards the solution of this problem (see below for more detailed comments).  Overall quality is fairly poor: there are missing references, there are incorrect claims, the empirical validation is insufficient.   Pros -- The motivation is very good. We need to improve the way we compare feature learning methods. -- The filters visualization is nice.  Cons -- It is debatable whether the chosen task is any better for assessing the quality of feature learning methods. The paper almost suggested a better solution in the introduction: we should compare across several tasks (from low level vision like matching to high level vision like object classification). If a representation is better across several tasks, then it must capture many relevant properties of the input. In other words, it is always possible to tweak a learning algorithm to give good results on one dataset, but it is much more interesting to see it working well across several different tasks after training on generic natural images, for instance.  -- The choice of the feature learning methods is questionable, why are only generative models considered here? The authors do mention that other methods were tried and worked worse, however it is hard to believe that more discriminative approaches work worse on the chosen task. In particular, knowing the matching task it seems that a method that trains using a ranking loss (learning nearby features for similar patches and far away features for distant inputs) should work better. See: H. Mobahi, R. Collobert, J. Weston. Deep Learning from Temporal Coherence in Video. ICML 2009. -- The overall results are pretty disappointing. Feature learning methods do not outperform the best engineered features. They do not outperform even if the comparison is unfair: for instance the authors use 128 dimensional SIFT but much larger dimensionality for the learned features. Besides, the authors do not take into account time, neither the training time nor the time to extract these features. This would also be considered in the evaluation.  More detailed comments: -- Missing references. It is not true that feature learning methods have never been assessed quantitatively without supervised fine tuning. On a low level vision task, I would refer to: Learning to Align from Scratch Gary Huang, Marwan Mattar, Honglak Lee, Erik Learned-Miller. In Advances in Neural Information Processing Systems (NIPS) 25, 2012. Another missing reference is  2011 Memisevic, R. Gradient-based learning of higher-order image features.  International Conference on Computer Vision (ICCV 2011). and other similar papers where Memisevic trains features that relate pairs of image patches. --ROC curves should be reported at least in appendix, if not in the main text. -- I do not understand why SIFT results on tab 1 a) differs from those in tab. 1 b).",1,5003
"This paper analyzes recursive autoencoders for a binary sentiment analysis task.  The authors include two types of analyses: looking at example trees for syntactic and semantic structure and analyzing performance when the induced tree structures are cut at various levels. More in depth analysis of these new models is definitely an interesting task. Unfortunately, the presented analyses and conclusions are incomplete or flawed.  Tree Cutting Analysis: This experiment explores the interesting question of how important the word vectors and tree structures are for the RAE. The authors incorrectly conclude that 'the strength of the RAE lies in the embeddings, not in the induced tree structure'. This conclusion is reached by comparing the following two models (among others): 1) A word vector average model with no tree structures that uses about 50x10,000 parameters (50 dimensional word vectors and a vocabulary of around 10,000 words) and reaches 77.67% accuracy. 2) A RAE model with random word vectors that uses 50 x 100 parameters and gets 77.49% accuracy.  An accuracy difference of 0.18% on a test set of ~1000 trees means that ~2 sentences are classified differently and is not statistically significant. So the results of both models are the same. That means that the RAE trees achieved the same performance with 1/100 of the parameters of the word vectors. So, the tree structures seem to work pretty well, even on top of random word vectors.  A good comparison would be between models with the same number of parameters. Both models could easily be increased or decreased in size.  One possible take away message could have been that the benefits of RAE tree structures and word embeddings are equal but performance does not increase when both are combined in a task that only has a single label for a full sentence. But even that one is difficult: All columns of the main results table (cutting trees) have the same top performance when it comes to statistical significance, so it would have also been good to look at another dataset. Another problem is that the RAE induced vectors are only used by averaging all vectors in the tree.    More important analyses into the model could explore what the higher node vectors are capturing by themselves instead of only in an average with all lower vectors.   Tree Structure Analysis: The first analysis is about the induced tree structures and finds that that they do not follow traditional parsing trees.  This was already pointed out in the original RAE paper and they show several examples of phrases that are cut off. An interesting comparison here would have been to apply the algorithm on correct parse trees and analyze if it makes a difference in performance.   The second analysis is about sentiment reversal, such as 'not bad'.  Unfortunately, the given binary examples are hard to interpret.  Are phrases like 'not bad' positive in the original training data? It's not clear to me that 'not bad' is a very positive phrase. Do the probabilities change in the right direction? When does it work and when does it not work? Is the sentiment of the negated phrase wrong or is the negation pushing in the wrong direction? In order to understand what the model should learn, it would have been interesting to see if the effects are even in the training dataset. Another interesting analysis would be to construct some simple examples where the reversal is much clearer like 'really not good'.   The paper is well written. Only one typo: E_cE and E_eC both used.The paper considers the compositional model of Socher et al. (EMNLP 2011) for predicting sentence opinion polarity. The authors define several model simplification types (e.g., reducing the maximal number of levels) and study how these changes affect sentiment prediction performance. They also study how well the induced structures agree with human judgement, i.e. how linguistically plausible (both from syntactic and semantic point of view) the structures are.   I find this work quite interesting and the (main?) result somewhat surprising. What it basically shows is that the compositional part does not seem to benefit  the sentence-level sentiment performance.  In other words, a bag-of-words model with distributed word representations performs as well (or better).  An additional, though somewhat small-scale, annotation study show that the model is not particularly accurate in representing opinion shifters (e.g., 'not' does not seem to reverse polarity reliably).   Though some of the choices of model simplification seem relatively arbitrary (e.g., why choosing a single subtree, rather than, e.g., dropping several subtrees within some budget?) and the human evaluation is somewhat small scale (and, consequently, not entirely convincing), I found the above observation interesting and important.  It would also be interesting to see if the compositional model appears to be more important when an actual syntactic tree (as in Socher et al (NIPS 2011) for paraphrase detection) is used instead of automatically inducing the structure.     One point which might be a little worrying is that the same parameters are used across different learning architectures, though one may expect that different regularizations and training regimes might be needed. However, the full model is estimated with the parameters chosen by the model designers on the same datasets, so it should not affect the above conclusion.  Pros: -- It provides interesting analysis of the influential model of Socher et al (2011) -- Both analysis of linguistic plausibility are provided and analysis of the effect of model components on sentiment prediction performance.  Though, the original publication (Socher et al, EMNLP 2011) contained the BOW baseline it was not exactly comparable, the flat model studied here seems a more natural baseline.  Cons: -- Semantic and syntactic coherence analysis may be too small scale to be seriously considered (2 human experts on a couple dozens of examples).This research analyses the Semi Supervised Recurive Autoencoder (RAE) of Socher et al., obtained with the NLP task of sentiment classification from sentences of movie reviews.   A first qualitative analysis conducted wth th help of human annotators, reveals that the syntactic and semantic role of reversers ('not') is not modeled well in many cases.  Then a systematic quantitative analysis is conducted, using the representation of a sentence as the average of the representation output at each node of the tree to train a classifier of sentiment, and analysing what is lost or gained by using only specific subsets of the tree nodes. These results clearly indicate that intermediate nodes bring no additional value for classfication performance compared to using only the word embeddings learned at the leaf nodes.  The full depth of the tree appears to extract no more useful information than the leaf nodes only.  Pros: I believe that this paper's analysis is a significant contribution with an important message. It is well conducted and properly questions and sheds light on the meaning and usefulness of the tree *structure* learned by RAEs, showing that drastic structure simplifications yield the same state-of-the-art performance on the considered classification task. It has the potential to start a healthy controversy, that will surely seed interest into further investigation of this important point.  Cons:  The message would carry much more weight if a similar analysis of RAEs could be conducted also on several other (possibly more challenging) NLP tasks than movie sentiment classification and pointed towards similar conclusions.",0,5004
"This paper replaces the spatial pyramidal pooling in a spatial pyramid pooling by a sliding-window style pooling. By using this method and color SIFT descriptors, state-of-the-art results are obtained on the Caltech-101 dataset (83.5% accuracy).  The contribution in this paper would be rather slight as is, but this is all the more true since it seems the idea of using sliding window pooling has already appeared in an older paper, with good results (they call the sliding windows 'components'):  'A Boosting Sparsity Constrained Bi-Linear Model for Object  Recognition' IEEE Multimedia 2012 Chunjie Zhang  Jing Liu ;  Qi Tian ;  Yanjun Han ;  Hanqing Lu ;  Songde Ma   Simply using it with color SIFT descriptors does not constitute enough novelty for accepting this paper.Summary of contributions:  The paper presented a method to achieve a state-of-the-art accuracy on the object recognition benchmark Caltech101. The method used two major ingredients: 1. a sliding window of histograms (called sliding spatial pyramid matching) , 2. randomized vocabularies to generate different models and combine them. The authors claimed that, using only one image feature (transformed color SIFT), the method achieved really good results on Caltech101.    Assessment of novelty and quality:   Though the accuracy looks impressive, the paper offers limited research value to the machine learning community. The success is largely engineering, lacking insights that are informative to readers.   The sliding window representation does not explore multiple scales. Therefore  I don't understand why it is still called 'pyramid'.   I hope the authors would try the methods on large-scale datasets like ImageNet. If good result obtained, then the work will be of great value to application.",0,5005
"Affinity Weighted Embedding  Paper summary  This paper extends supervised embedding models by combining them multiplicatively, i.e. f'(x,y) = G(x,y) f(x,y). It considers two types of model, dot product in the *embedding* space and kernel density in the *embedding* space, where the kernel in the embedding space is restricted to k((x,y),(x','y)) = k(x-x')k(y-y'). It proposes an iterative algorithm which alternates f and G parameter updates.  Review Summary  The paper is clear and reads well. The proposed solution is novel. Combining local kernels and linear kernel in different embedding space could leverage the best characteristic for each of them (locality for non-linear, easier training for linear). The experiments are convincing. I would suggest adding the results for G alone.  Review Details  Step (2), i.e. local kernel, is interesting on its own. Could you report its result? The optimization problem seems harder than step (1), could you quantify how much the pretraining with step (1) helps step (2)?  A last related question, how do you initialize the parameters for step (3)?This work proposes a new nonlinear embedding model and applies it to a music annotation and image annotation task. Motivated by the fact that linear embedding models typically underfit on large datasets, the authors propose a nonlinear embedding model with greater capacity. This model weights examples in the embedding by their affinity in an initial linear embedding. The model achieves modest performance improvements on a music annotation task, and large performance improvements on ImageNet annotation. The ImageNet result achieves comparable performance to a very large convolutional net.  The model presented in the paper is novel, addresses an apparent need for a higher capacity model class, and achieves good performance on a very challenging problem.   The paper is clear but has a rushed feel, with some explanations being extremely terse. Although the details of the algorithms and experiments are specified, the intuition behind particular algorithmic design choices is not spelled out and the paper would be stronger if these were.  The experimental results are labeled 'preliminary,' and although they demonstrate good performance on ImageNet, they do not carefully investigate how different design choices impact performance. The ImageNet performance comparisons to related algorithms are hard to interpret because of a different train/testing split, and because a recent highly performing convolutional net was not considered (though the authors discuss its likely superior performance).   Finally, the presented experiments focus on performance on tasks of interest, but do not address the running time and storage cost of the algorithm. The authors mention the fact that their algorithm is more computationally and space-intensive than linear embedding; it would be useful to see running times (particularly in comparison to Dean et al. and Krizhevsky et al.) to give a more complete picture of the advantages of the algorithm.",1,5006
"Herding is a relatively recent idea [23]: create a dynamical system that evolves a vector, which when time-averaged will match desired expectations. Originally it was designed as a novel means to generalize from observed data with measured moments. In this work, the conditional distributions of a Gibbs sampler are matched, with the hope of sampling from arbitrary target distributions.  As reviewed by the paper itself, this work joins only a small number of recent papers that try to simulate arbitrary target distributions using a deterministic dynamical system. Compared to [19] this work potentially works better in some situations: O(1/T) convergence can happen, whereas [19] seems to emulate a conventional Gibbs sampler with O(1/T^2) convergence. However, the current work seems to be more costly in memory and less-generally applicable than Gibbs sampling, because it needs to track weights for all possible conditional distributions (all possible neighbourhood settings for each variable) in some cases. The comparison to [7] is less clear, as that is motivated by O(1/T) QMC rates, but I don't know if/how it would compare to the current work. (No comparison is given.)  One of the features of Markov chain Monte Carlo methods, such as Gibbs sampling, is that represents _joint_ distributions, through examples. Unlike variational approximation methods, no simple form of the distribution is assumed, but Monte Carlo sampling may be a less efficient way to get marginal distributions. For example, Kuss and Rasmussen http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf demonstrated that EP gives exceedingly accurate posterior marginals with Gaussian process classifiers, even though its joint approximation, a Gaussian, is obviously wrong. The experiment in section 4.1 suggests that the herded Gibbs procedure is prepared to move through low probability joint settings more often than it 'should', but gets better marginals as a result. The experiment section 4.2 also depends only on low-dimensional marginals (as many applications do). The experiment in section 4.3 involves an optimization task, and I'm not sure how herded Gibbs was applied (also with annealing? The most probable sample chosen? ...).  This is an interesting, novel paper, that appears technically sound. The most time-consuming research contributions are the proofs in the appendices, which seem plausible, but I have not carefully checked them. As discussed in the conclusion, there is a gap between the applicability of this theory and the applicability of the methods. But there is plenty in this paper to suggest that herded sampling for generic target distributions is an interesting direction.  As requested, a list of pros and cons:  Pros: - a novel approach to sampling from high-dimensional distributions, an area of large interest. - Good combination of toy experiments, up to fairly realistic, but harder to understand, demonstration. - Raises many open questions: could have impact within community. - Has the potential to be both general and fast to converge: in long term could have impact outside community.  Cons: - Should possibly compare to Owen's work on QMC and MCMC. Although there may be no interesting comparison to be made. - The most interesting example (NER, section 4.3) is slightly hard to understand. An extra sentence or two could help greatly to state how the sampler's output is used. - Code could be provided.   Very minor: paragraph 3 of section 5 should be rewritten. It's wordy: 'We should mention...We have indeed studied this', and uses jargon that's explained parenthetically in the final sentence but not in the first two.Herding has an advantage over standard Monte Carlo method, in that it estimates some statistics quickly, while Monte Carlo methods estimate all statistics but more slowly.  The paper presents a very interesting but impractical attempt to generalize Herding to Gibbs sampling by having a 'herding chain' for each configuration of the Markov blanket of the variables.  In addition to the exponential memory complexity, it seems like the method should have an exponentially large constant hidden in the O(1/T) convergence rate: Given that there are many herding chains, each herding parameter would be updated extremely infrequently, which would result in an exponential slowdown of the Herding effect and thus increase the constant in O(1/T).  And indeed, lambda from theorem 2 has a 2^N factor.  The theorem is interesting in that it shows eventual O(1/T) convergence in full distribution: that is, the empirical joint distribution eventually converges to the full joint distribution. However, in practice we care about estimating marginals and not joints.  Is it possible to show fast convergence on every subset of the marginals, or even on the singleton variables?  Can it be done with a favourable constant?  Can such a result be derived from the theorems presented in the paper?  Results about marginals would  be of more practical interest.  The experiments show that the idea works in principle, which is good.  In its current form, the paper presents a reasonable idea but is incomplete, since the idea is too impractical. It would be great if the paper explored a practical implementation of Gibbs herding, even an is approximate one.  For example, would it be possible to represent w_{X_{Ni}} with a big linear function A X_{Ni} for all X and to herd A, instead of slowly herding the various W_{X_{Ni}}?  Would it work? Would it do something sensible on the experiments?  Can it be proved to work in a special case?  In conclusion, the paper is very interesting and should be accepted.  Its weakness is the general impracticality of the method.This paper shows how Herding, a deterministic moment-matching algorithm, can be used to sample from un-normalized probabilities, by applying Herding to the full-conditional distributions. The paper presents (1) theoretical proof of O(1/T) convergence in the case of empty and fully-connected graphical models, as well as (2) empirical evidence, showing that Herded Gibbs sampling outperforms both Gibbs and mean-field for 2D structured MRFs and chain structured CRFs. This improved performance however comes at the price of memory, which is exponential in the maximum in-degree of the graph, thus making the method best suited to sparsely connected graphical models.  While the application of Herding to sample from joint-distributions through its conditionals may not appear exciting at first glance, I believe this represents a novel research direction with potentially high impact. A 1/T convergence rate would be a boon in many domains of application, which tend to overly rely on Gibbs sampling, an old and often brittle sampling algorithm. The algorithm's exponential memory requirements are somewhat troubling. However, I believe this can be overlooked given the early state of research and the fact that sparse graphical models represent a realistic (and immediate) domain of application.  The paper is well written and clear. I unfortunately cannot comment on the correctness of the convergence proofs (which appear in the Appendix), as those proved to be too time-consuming for me to make a professional judgement on.  Hopefully the open review process of ICLR will help weed out any potential issues therein.  PROS: * A novel sampling algorithm with faster convergence rate than MCMC methods. * Another milestone for Herding: sampling for un-normalized probabilities  (with tractable conditionals). * Combination of theoretical proofs (when available) and empirical evidence. * Experiments are thorough and span common domains of application: image denoising through MRFs and Named Entity Recognition through chain-CRFs.  CONS: * Convergence proofs hold for less than practicle graph structures.  * Exponential memory requirements of the algorithm make Herded Gibbs sampling impractical for lage families of graphical models, including Boltzmann Machines.The paper presents a deterministic 'sampling' algorithm for unnormalized distributions on discrete variables, similar to Gibbs sampling, which operates by  matching the statistics of the conditional distribution of each node given its Markov blanket. Proofs are provided for the independent and fully-connected cases, with an impressive improvement in asymptotic convergence rate to O(1/T) over O(1/sqrt(T)) available from Monte Carlo methods in the fully-connected case. Experimental results demonstrate herded Gibbs outperforming traditional Gibbs sampling in the sparsely connected case, a regime unfortunately not addressed by the provided proofs. The algorithm's Achilles heel is its prohibitive worst-case memory complexity, scaling exponentially with the maximal node degree of the network.  The paper is compelling for its demonstration that a conceptually simple deterministic procedure can (in some cases at least) greatly outperform Gibbs sampling, one of the traditional workhorses of Monte Carlo inference, both asymptotically and empirically. Though the procedure in its current form is of little use in large networks of even moderate edge density, the ubiquity of application domains involving very sparse interaction graphs makes this already an important contribution. The proofs appear to be reasonable upon cursory examination, but I have not as yet verified them in detail.  PROS  * A lucidly explained idea that gives rise to somewhat surprising theoretical results. * Proofs of convergence as well as experimental interrogations. * A step towards practical herding algorithms for dense unnormalized models, and an important milestone for the literature on herding in general.  CONS  * An (acknowledged) disconnect between theory and practice -- available proofs apply only in cases that are uninteresting or impractical. * Experiments in 4.3 make mention of NER with skip-chain CRFs, where Viterbi is not tractable, but resorts to experiments with chain CRFs instead. An additional experiment utilizing skip-chain CRFs (a more challenging inference task, not amenable to Viterbi) would have been more compelling,  though I realize space is at a premium.  Minor concerns: - The precise dimensionality of the image denoising problem is, as far as I can tell, never specified. This would be nice to know.  - More details as to how the herded Gibbs procedure maps onto the point estimate provided as output on the NER task would be helpful -- presumably the single highest-probability sample is used?",1,5007
"The paper give an example of a task that neural net solves perfectly when intermediate labels are provided but that is not solved at all by several machine learning algorithms including neural net when the intermediate labels are not provided. I consider the result important.  Comments: It is surprising that structured MLP does chance even on training set. On the other hand with 11 output units per parch this is perhaps not so surprising as the network has to fit everything into minimal representation. However one would expect to get better training set resuts with larger sizes. You should put such results into Table 1 and go to even larger sizes, like 100.   To continue on this, if you trained sparse coding with high sparsity on each patch you should get 1 in N representation for each instance (with 11x4x3 or more units). It would be good to see what the P2NN would do with such representation. I think this is the primary missing piece of this work.   It is not quite fair to compare to humans as humans have prior knowledge, specifically of rotations, probably learned from seeing objects rotate.  I don't think 'Local descent hypothesis' is quite true. We don't just do local approximate descent. First we do one shot learning in hippocampus. Second, we do search for explanations and solutions and we do planning (both unconsciously and consciously). Sure having more agents helps - it's a little like running a genetic algorithm - an algorithm that overcomes local minima.  At the end of page 6 you say P1NN had 2048 units and P2NN 1024 but this is reversed in 3.2.2. Typo?In this paper, the authors provide an exposition of curriculum learning and cultural evolution as solutions to the effective local minimum problem.  The authors provide a detailed set of simulations that support a curriculum theory of learning, which rely on a supervisory training signal of intermediate task variables that are relevant for the task.  Pros: This work is important to probe the limitations of current algorithms, especially as the deep learning field continues to have success.  A great thing about this paper is that it got me thinking about new classes of algorithms that might effectively solve the mid-level optimization and more effective strategies for training deep networks for practical tasks.  The simulations are well described and compelling.  Cons: The exposition on curriculum learning could be condensed. (minor) The demonstrative problem (sprite counting) is a visual perception problem and therefore carries with it the biases of our own perception and inferred strategies.  Maybe the overall argument might be bolstered by the addition of a more abstract example?   Here are some questions: why so many image regions?  Why use an 8x8 grid? Won't 3 regions suffice to make the point? Or is this related to the complexity of the problem. A related question: how are the results effected by the number of these regions?  Maybe some reduced tests at the extremes would be interesting, i.e. with only 3 regions, and 32 (you have 64 already)?  In the networks that solve the task, are the weights that are learned symmetric over the image regions? i.e. are these weights identical (maybe up to some scaling and sign flip).  Is there anything you have determined about the structure of the learned second layer of the IKGNN?  Furthermore, what about including a 'weight sharing' constraint in the general MLP model (the one that does not solve the problem, but has the same structure as the one that does)?  Would including this constraint change the solution? (the constraint is already in the P1NN, but what about adding it into the P2NN?) Another way to ask this is: Is enforcing translation invariance in the network sufficient to achieve good performance, or do we need to specifically train for the sprite discrimination?  A technical point about the assumption of human performance on this task: Do we know if humans can solve this problem 'in a glance?': flashing the image for a small amount of time ~100-200msecs.  Either with or without a mask? It seems that the networks you have derived are solving such a problem 'in a glance.'  A more meta comment: Is there an argument to be made that the sequential nature of language allows humans to solve this task? Even the way you formulate the problem suggests this sequential process: 'are all of the sprites in the image the same?': in other words 'find the sprites, then decide if they are the same' When I imagine solving this problem myself, I imagine performing a more sequential process: look at one sprite, then the next, (is it the same?, if it is): look at the next sprite (is it the same?). I know that we can consider this problem to be a concrete example of a more abstract learning problem, but it's not clear if humans can solve such problems without sequential processing. Anyway, this is not a criticism, per se, just food for thought.The paper by Gulcehre & Bengio entitled 'Knowledge Matters: Importance of Prior Information for Optimization' presents an empirical study which compares a two-tiered MLP architecture against traditional algorithms including SVM, decision trees and boosting. Images used for this task are 64x64 pixel images containing tetris-like sprite shapes. The proposed task consists in trying to figure out whether all the sprites in the image are from the same category or not (invariant to 2D transformations).   The main result from this study is that intermediate guidance (aka building by hand an architecture which 'exploits intermediate level concepts' by dividing the problem in two stages (a classification stage followed by a XOR stage)  solves the problem for which a 'naive' neural net (as well as classical machine learning algorithms) fail.  Pros: The proposed task is relatively interesting as it offers an alternative to traditional pattern matching tasks used in computer vision.  The experiments seem well conducted. The fact that a neural network and other universal approximators do not seem to even get close to learning the task with ~80K training examples is relatively surprising.  Cons:  The work by Fleuret et al (Comparing machines and humans on a visual categorization test. PNAS 2011) needs to be discussed. This paper focuses on a single task which appears to be a special case from the longer list of 'reasoning' tasks proposed by Fleuret et al.   In addition, the proposed study reports a null result, which is of course always a little problematic (the fact that the authors did not manage to train a classical NN to solve the problem does not mean it is impossible). At the same time, the authors have explored reasonably well the space of hyper parameters and seem to have done their best in getting the NN to succeed.   Minor points: The structure of the paper is relatively confusing. Sections 1.1 and 2  provide a review of some published work by the authors and does not appear to be needed for understanding the paper. In my view the paper could be shortened or at least most of the opinions/speculations in the introduction should be moved to the discussion section.",1,5008
"I fully admit that I don't know enough about group theory to evaluate this submission. However, I do know about convolutional networks, so it is troubling that I can't understand it.  Since this is only a workshop paper, we're not going to look for a new reviewer.  When you do eventually pursue conference publication, I would suggest that you consider the audience and adapt the presentation somewhat, so that people who are familiar with convolutional networks but not with group theory will be able to get an idea of what the paper is about, and can read about the appropriate subjects to be able to understand it better.  I would also suggest providing a high level summary of the paper that makes it clear what you consider your original contributions to be. I had a hard time telling what was original content and what was just describing what convolutional networks are in group theory notation.This short paper presents a discussion on the nature and the type of invariances that are represented and learned by convolutional neural networks.  It claims that the invariance a layer in a convolutional neural network can be expressed with a Lie group, and that the invariance of a deep convolutional neural network can be expressed with a product of groups.  This is a discussion paper that is difficult to understand without being familiar with group theory.  It would be easier to read if there were even toy examples that illustrate the concepts presented in this work.  In its current form the paper is incomplete; to be useful, it needs to use these ideas to somehow improve the training or generalization of convolutional neural networks.  On a related note, it is hard to understand the significance of the results.  So the invariance of a deep convolutional neural network can be expressed with a semi-direct product of some groups; this is nice, but what does it lead to, how can it be used?    To summarize, paper has intriguing ideas, but they are not sufficiently developed, and their significance is not clearly explained.    Networks",0,5009
"This paper describes a method for learning visual feature descriptors that are invariant to changes in illumination, viewpoint, and image quality. The method can be used for multi-view matching and alignment, or for robust image retrieval. The method computes a regularized linear projection of SIFT feature descriptors to optimize a weighted similarity measure. The method is applied to matching and non-matching patches from Flickr images. The primary contribution of this workshop submission is to demonstrate that a coarse weighting of the data samples according to the disparity between their semantic distance and their Euclidean distance in SIFT descriptor space.  The novelty of the paper is minimal, and most details of the method and the validation are not given. The authors focus on the weighting of the sample pairs to emphasize both the furthest similar pairs and the closest dissimilar pairs, but it is not clear that this is provides a substantial gain.The paper aims to present a method for discriminant analysis for image descriptors. The formulation splits a given dataset of labeled images into 4 categories, Relevant/Irrelevant and Near/Far pairs (RN,RF,IN,IF). The final form of the objective aims to maximize the ratio of sum of distances of irrelevant pairs divided by relevant pairs. The distance metric is calculated at the lower dimensional projected space. The main contribution of this work as suggested in the paper is selecting the weighting of 4 splits differently from previous work.  The main intuition or reasoning behind this choice is not given, neither any conclusive emprical evidence. In the only experiment that contains real images in the paper, data is said to be taken from Flickr. However, it is not clear if this is a publicly available dataset or some random images that authors collected. Moreover, for this experiment, one of the only two relevant methods are not included for comparison. Neither, any details of the training procedure nor the actual hyper parameters (eta) are explained in the paper.",0,5010
"This paper proposes a regularizer for auto-encoders with nonlinearities that have a zegion with zero-gradient. The paper mentions three nonlinearities that fit into that category: shrinkage, saturated linear, rectified linear.  The regularizer basically penalizes how much the activation deviates from saturation. The insight is that at saturation, the unit conveys less information compared to when it is in a non-saturated region.  While I generally like the paper, I think it could be made a lot stronger by having more experimental results showing the practical benefits of the nonlinearities and their associated regularizers.  I am particularly interested in the case of saturated linear function. It will be interesting to compare the results of the proposed regularizer and the sparsity penalty. More concretely, f(x) = 1 would incur some loss under the conventional sparsity; whereas, the new regularizer does not. From the energy conservation point of view, it is not appealing to maintain the neuron at high activation, and the new regularizer does not capture that. But it may be the case that, for a network to generalize, we need to only restrict the neurons to be in the saturation regions. Any numerical comparisons on some classification benchmarks would be helpful.  It would also be interesting that the method is tested on a classification dataset to see if it makes a different to use the new regularizers.This paper proposes a novel kind of penalty for regularizing autoencoder training, that encourages activations to move towards flat (saturated) regions of the unit's activation function. It is related to sparse autoencoders and contractive autoencoders that also happen to encourage saturation. But the proposed approach does so more directly and explicitly, through a 'complementary nonlinerity' that depends on the specific activation function chosen.   Pros: + a novel and original regularization principle for autoencoders that relates to earlier approaches, but is, from a certain perspective, more general (at least for a specific subclass of activation functions).  + paper yields significant insight into the mechanism at work in such regularized autoencoders also clearly relating it to sparsity and contractive penalties.  + provides a credible path of explanation for the dramatic effect that the choice of different saturating activation functions has on the learned filters, and qualitatively shows it.  Cons: - Proposed regularization principle, as currently defined, only seems to make sense for activation functions that are piecewise linear and have some perfectly flat regions (e.g. a sigmoid activation would yield no penalty!) This should be discussed. - There is no quantitative measure of the usefulness of the representation learned with this principle. The usual comparison of classification or denoising performance based on the learned features, with those obtained with other autoencoder regularization principles would be a most welcome addition.Although this paper proposes an original (yet trivial) approach to regularize auto-encoders, it does not bring sufficient insights as to why saturating the hidden units should yield a better representation. The authors do not elaborate on whether the SATAE is a more general principle than previously proposed regularized auto-encoders(implying saturation as a collateral effect) or just another auto-encoder in an already well crowded space of models (ie:Auto-encoders and their variants). In the last years, many different types of auto-encoders have been proposed and most of them had no or little theory to justify the need for their existence, and despite all the efforts engaged by some to create a viable theoretical framework (geometric or probabilistic) it seems that the effectiveness of auto-encoders in building representations has more to do with a lucky parametrisation or yet another regularization trick.  I feel the authors should motivate their approach with some intuitions about why should I saturate my auto-encoders, when I can denoise my input, sparsify my latent variables or do space contraction? It's worrisome that most of the research done for auto-encoders has mostly focused in coming up with the right regularization/parametrisation that would yield the best 'filters'. Following this path will ultimately make the majority of people reluctant to use auto-encoders because of their wide variety and little knowledge about when to use what. The auto-encoder community should backtrack and clear the intuitive/theoretical noise left behind, rather than racing for the next new model.",1,5011
"The paper presents a benchmark for comparing representations of image data in brains and machines. The benchmark consists of looking at how the image categorization task is encoded in the leading kernel principal components of the representation, thus leading to an analysis of complexity and noise. The paper contains extensive experiments based on a representive set of state-of-the-art learning algorithms on the machine learning side, and real recordings of macaques brain activity on the neural side.  The research presented in this paper is well-conducted, timely and highly innovative. It is to my knowledge the first time, that representations obtained with state-of-the-art machine learning techniques for vision are systematically compared with real neural representations. The authors motivate the use of kernel analysis, by the inbuilt robustness to sample size being desirable in this heterogeneous setting.  The dataset used in the paper is composed of objects that are superposed to an independent background. While authors motivate their choice by controlling the factors of variations in the representation, it would be interesting to know whether machine learning or brain representations benefit most from this particular setting.  This paper also raises the important question of what is the best way of comparing representations. One can wonder, for example, whether the reduced set of kernels considered here (Gaussian kernels with multiple scales) introduces some bias in favor of 'Gaussian-friendly' representations. Also, as suggested by the authors, it could be that the way neural recordings are represented leads to underestimating their discriminative ability.This paper applies the methodology for 'kernel analysis of deep networks' (Montavon et al, 2011) to the neural code measured on two areas (V4 and IT) on the visual cortex of the macaque. It compares, on the same test set, the biological responses of V4 or IT (spike counts measured at about 100 electrode sites) to the hidden unit activations on the penultimate layer of several state-of-the-art deep learning architectures trained on large image datasets: the 10 million YouTube images and deep sparse auto-encoder paper by Le et al (2012), a convolutional network by Krizhevsky et al (2012), two papers by Pinto et al, one on the V1 model, another on the high throughput L3 model class and the unsupervised learning paper by Coates et al (2012).  The authors show that the IT area of the visual cortex seems to have a neural code that is more discriminative than the neural code of the V4 area for a 7-class image categorization task under variations of pose, position and scale. The authors also show that one supervised deep learning algorithm (Krizhevsky et al, 2012) even produces hidden layer representation that seems to outperform IT on that task.   Pros, novelty and quality:  This paper is the first to apply the same method for evaluating feature representations of both the biological neural code (measured on the visual cortex of a primate) and of hidden unit activations in state-of-the-art methods for image classification. It provides an extensive comparison of the penultimate hidden layer of several deep learning algorithms, vs. the V4 and IT areas of the visual cortex of two macaques. As such, it provides insight into which algorithms make a good hidden representation of images.  The method for evaluating the feature representations is essentially non-parametric and provides a robust way to assess the complexity of the decision boundary. The kernel analysis method measures what percentage of the information coming from the sample images is required to successfully train a nonlinear Gaussian SVM-like classifier on the features (neural code or hidden unit activations), or a linear classifier in the dual space, for a simple image categorization task. The kernel PCA approach of keeping the top d eigenvectors of the kernel matrix in the dual solution is more robust than the cross-validation performance or than the number of support vectors, when the number of samples is small.  The paper is well written, the claims are well supported by the experiments. The metric used in this study is robust and the main results (IT vs V4, Krizhevsky et al 2012 vs IT on high variations) are statistically significant.  Cons:  There are no cons per se in this paper, only limitations in the methodology (linked to the choice of the dataset) that could be improved upon by using a more extensive dataset. Most of these limitations have been preemptively mentioned and discussed by the authors in section 4.  * The two macaque subjects in the study by Majaj et al (2012) are unlikely to have been exposed to images of 3 object categories in the dataset: cars, planes or other animals such as cows and elephants. They may have been exposed to images from the 4 remaining object classes: faces, chairs, tables and fruits. By consequence, their V4 or IT cortical areas might not be trained to recognize, even after prolonged exposure, that the image of a car at an angle is still a car with a variation, and not another type of objects. The authors do raise the question whether the neural representation could be enhanced with increased exposure.  * The paper does mention that only about a hundred sites, on the cortex surface, are selected for the image categorization task, compared to all the tens of thousands of hidden units in the deep architecture. Some further discussion on the fairness of such a comparison would be welcome.  Other comments:  * The Gaussian kernel uses a single coefficient sigma for all the features (i.e., all the neurons / hidden units). On one hand, the neural data are taken on the visual cortex areas V4 and IT, where all the electrode sites are expected to measure information that is relevant for image recognition tasks in general, and the deep learning architectures were all trained on image classification tasks. On the other hand, not all the features (hidden units or electrode sites) are equally relevant, all the time, to all these tasks, but their values are all scaled nevertheless. Would it make sense to tune the individual per-feature sigma coefficients in the Gaussian kernel, as in Chapelle et al (2002) 'Choosing multiple parameters for support vector machines'?  * Are all the 5 references by Pinto et al. necessary for this paper?  Minor comments:  * The authors do not indicate how the images from the dataset were split among the two monkeys (were they shown the same images, or two, different, random sets of images?) and how the neural observations from the different electrode sites (58 IT and 70 V4 sites on one monkey, 110 IT and 58 V4 sites on the other monkey) were grouped. My guess is that the same sets of images were shown to the two monkeys and that their responses were concatenated into IT or V4 matrices of site vs image.  * The authors do not need to mention the low computational complexity of the LSE loss (section 2.2). It is not more complex than the logistic loss and the real point is what they say about intra-class variance and inter-class variance.  * I do not fully understand the protocol in section 2.3, namely: 'we evaluate 10 pre-defined subsets of images, each taking 80% of the data from each variation level'.  * Is total dimensionality D equal to the number of samples n?This paper assesses feature learning algorithms by comparing their performance on an object classification task to that of Macaque IT and V4 neurons. The work provides a new dataset of images, an analysis method for comparing feature representations based on kernel analysis, and neural feature vectors recorded from V4 and IT neurons in response to these images. The authors evaluate a number of recent representational learning algorithms, and identify a recent approach based on deep convolutional networks outperforms V4 and IT neurons.  The paper is the first of its kind in providing easy tools to evaluate new representations against high level neural visual representations. It's comparison method differs from prior work by investigating representational learning with respect to a task, and hence is less influenced by potentially task-irrelevant idiosyncrasies of the neural response. The final conclusion reached, that recent models are beginning to surpass V4 and IT models, is very interesting. The authors have clearly explained their rationale behind the many design choices required, and their choices seem very reasonable.  Because of the many design choices to be made in reducing neural data to a feature representation (the use of multi units rather than singular units, time averaging, short presentation times--many of which are discussed by the authors in the text), the resulting V4/IT performance is likely a lower bound on the true performance. To surpass a lower bound is good news, but to be a useful metric for future research efforts, this lower bound would should lie above current models' performance. The fact that the Krizhevsky model already outperforms V4/IT means there is less reason to compare future representation algorithms using the proposed metric in its current form.  The kernel analysis metric asks whether neural and artificial data can achieve similar classification performance for a given model complexity, but this is a separate question from asking whether the neural representation is similar to the artificial representation; e.g., for a classification task, one could imagine many different pairwise similarity structures that would remain linearly separable (or said with the standard metaphor, both a bird and a plane can fly, but rely on different mechanisms). While some aspects of the neural response may be task irrelevant, it may be complementary to augment the KA-AUC approach with a similarity-based approach. This could also be computed from the collected data and would help map levels within a computational model to visual brain areas. In general a more extensive discussion of and contrast with the Kriegeskorte approach would be helpful.    Machine",0,5012
"This paper proposes to use two consecutive SVDs to produce a continuous representation. This paper also introduces a property called focality. They claim that this property may be important for neural network: many classifiers cannot efficiently handle conjunctions of several features unless they are explicitly given as additional features; therefore a more focal representation of the inputs can be a promising way to tackle this issue. This paper  opens a very important discussion thread and provides some interesting starting points.   There are two contributions in this paper. First, the authors define and motivate the property of focality for the representation of the input. While the motivation is clear, its implementation is not obvious. For instance, the description provided in the subsection 'Discriminative task' is hard to understand: what is really measured how it is related to the focality property. This part of the paper could be rephrased to be more explicit. The second contribution is the representation derived by two consecutive SVDs. I would suggest to provide a bit more of discussion about the related work like LSA, PCA, or the denoising auto-encoder.  In the third paragraph of section 'Discussion', the authors may cite the work of (Collobert and Weston) and (Socher) for instance.This paper introduces a novel method to induce word vector representations from a corpus of unlabeled text. The method relies upon 'stacking' singular value decomposition with an intermediate normalization nonlinearity. The authors propose 'focality' as a metric for quantifying the quality of a learned representation. Finally, control experiments on a small collection of sentence text demonstrates stacked SVD as producing more focal representations than a single SVD.  The method of stacked SVD is novel as far as I know, but could perhaps be generalized to use other nonlinearities between the two SVD layers than length normalization alone. As the authors acknowledge, SVD is a linear transform so the intermediate nonlinearity is important as to have the entire method not reduce to a single linear transform. There are a huge number of ways to use matrix factorization to induce word vectors, Turney & Pantel (JAIR 2010) give a nice review. I would like to better understand the proposed method in the context of the many alternatives to SVD factorization (e.g. kernel PCA etc.).   The introduced notion of focality might serve as a good metric for analysis of learned representation quality. It seems however that measuring focality is only possible with brute force experiments which could make it an unwieldy tool. Expanding on focality as a tool for representation evaluation, both in theory and practice, could strengthen this paper significantly.   The experiments use a small text corpus to demonstrate two SVDs as producing better representations than one. There is much room for improvement in the experiment section. In particular, there are several word representation benchmarks the authors could use to assess the quality of the proposed method relative to previous work: - Turian et al (ACL 2010) compare several word representations and release benchmark code.  - Socher et al (EMNLP 2011) release a multi-dimensional sentiment analysis corpus and use neural nets to train word representations - Maas et al (ACL 2011) release a large semi-supervised sentiment analysis corpus and directly compare SVD-obtained word representations with other models  The experiments given are a reasonable sanity check for the model and demonstration of the introduced focality metric. However, the paper would be greatly improved by comparing to previous work on at least one of the tasks in papers listed above.   The 1LAYER vs 2LAYER experiment is not clearly explained. Please expand on the difference in 1 vs 2 layers and the experimental result.  To summarize: - Novel layer-wise SVD approach to inducing word vectors. Needs to be better explained in the context of matrix factorization alternatives - Novel 'focality' metric which could serve as a tool for measuring learned representation quality. Metric needs more explanation / analysis. - Experiments don't demonstrate the model relative to previous work. This is a major omission since many recent alternatives exist and comparison experiments should be straightforward with several public datasets exist - Overall paper is fairly clear but could use some work",1,5013
"GENERAL COMMENTS The paper promises to establish the relation between Amari's natural gradient and many methods that are called Natural Gradient or can be related to Natural Gradient because they use Gauss-Newton approximations of the Hessian.  The problem is that I find the paper misleading.  In particular the G of equation (1) is not the same as the G of equation (7).  The author certainly points out that the crux of the matter is to understand which distribution is used to approximate the Fisher information matrix, but the final argument is a mess.  This should be done a lot more rigorously (and a lot less informally.)  As the paper stands, it only increases the level of confusion.  SPECIFIC COMMENTS * (ichi Amari, 1997) - > (Amari, 1997) * differ -> defer * Due to this surjection:  A surjection is something else! * Equation (1):  please make clear that the expectation is an expectation on z distributed according p_theta  (not the ground truth nor the empirical distribution). Equation (7) then appears to be a mix of both. * becomes the conditional p_	heta(t|x) where q(x) represents:     where is q in p_	heta(t|x)Summary  The paper reviews the concept of natural gradient, re-derives it in the context of neural network training, compares a number of natural gradient-based algorithms and discusses their differences. The paper's aims are highly relevant to the state of the field, and it contains numerous valuable insights. Precisely because of its topic's importance, however, I deplore its lack of maturity, especially in terms of experimental results and literature overview.    Comments  -- The title raises the expectation of a review-style paper with a broad literature overview on the topic, but that aspect is underdeveloped. A paper such as this would be a perfect opportunity to relate natural gradient-related work in neural networks to closely related approaches in reinforcement learning [1,2] and stochastic search [3].  -- The discussion in section 2 is correct and useful, but would benefit enormously from an illustrative figure that clarifies the relation between parameter space and distribution manifold, and how gradient directions differ in both. The last sentence (Lagrange method) is also breezing over a number of details that would benefit from a more explicit treatment.  -- There is a recurring claim that gradient-covariances are 'usually confused' with Fisher matrices. While there are indeed a few authors who did fall victim to this, it is not a belief held by many researchers working on natural gradients, please reformulate.  -- The information-geometric manifold is generally highly curved, which means that results that hold for infinitesimal step-sizes do not generally apply to realistic gradient algorithms with large finite steps. Indeed, [4] introduces an information-geometric 'flow' and contrasts it with its finite-step approximations. It is important to distinguish the effect of the natural gradient itself from the artifacts of finite-step approximations, indeed the asymptotic behavior can differ, see [5]. A number of arguments in section 6 could be revised in this light.  -- The idea of using more data to estimate the Fisher information matrix (because if does not need to be labeled), compared to the data necessary for the steepest gradient itself, is promising for semi-supervised neural network training. It was previously was presented in [3], in a slightly different context with infinitely many unlabeled samples.  -- The new variants of natural gradient descent should be given in pseudocode in the appendix, and if possible even with a reference open-source implementation in the Theano framework.  -- The experiment presented in Figure 2 is very interesting, although I disagree with the conclusions that are derived from it: the variance is qualitatively the same for both algorithms, just rescaled by roughly a factor 4. So, relatively speaking, the influence of early samples is still equally strong, only the generic variability of the natural gradient is reduced: plausibly by the effect that the Fisher-preconditioning reduces step-sizes in directions of high variance.  -- The other experiments, which focus on test-set performance, have a major flaw: it appears each algorithm variant was run exactly once on each dataset, which makes it very difficult to judge whether the results are significant. Also, the effect of hyper-parameter-tuning on those results is left vague.    Minor points/typos -- Generally, structure the text such that equations are presented before they are referred to, this makes for a more linear reading flow. -- variable n is undefined -- clarify which spaces the variables x, z, t, theta live in. -- 'three most typical' -- 'different parametrizations of the model' -- 'similar derivations' -- 'plateaus'  -- axes of figures could be homogenized.  References [1] 'Natural policy gradient', Kakade, NIPS 2002. [2] 'Natural Actor-Critic', Peters and Schaal, Neurocomputing 2008. [3] 'Stochastic Search using the Natural Gradient', Sun et al, ICML 2009. [4] 'Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles', Arnold et al, Arxiv 2011. [5] 'Natural Evolution Strategies Converge on Sphere Functions', Schaul, GECCO 2012.This paper attempts to reconcile several definitions of the natural gradient, and to connect the Gauss-Newton approximation of the Hessian used in Hessian free optimization to the metric used in natural gradient descent.  Understanding the geometry of objective functions, and the geometry of the space they live in, is crucial for model training, and is arguably the greatest bottleneck in training deep or otherwise complex models.  However, this paper makes a confused presentation of the underlying ideas, and does not succeed in clearly tying them together.  More specific comments:  In the second (and third) paragraph of section 2, the natural gradient is discussed as if it stems from degeneracies in theta, where multiple theta values correspond to the same distribution p.  This is inaccurate.  Degeneracies in theta have nothing to do with the natural gradient.  This may stem from a misinterpretation of the role of symmetries in natural gradient derivations?  Symmetries are frequently used in the derivation of the natural gradient, in that the metric is frequently chosen such that it is invariant to symmetries in the parameter space.  However, the metric being invariant to symmetries does not mean that p is similarly invariant, and there are natural gradient applications where symmetries aren't used at all.  (You might find The Natural Gradient by Analogy to Signal Whitening, Sohl-Dickstein, http://arxiv.org/abs/1205.1828 a more straightforward introduction to the natural gradient.)  At the end of page 2, between equations 2 and 3, you introduce relations which certainly don't hold in general.  At the least you should give the assumptions you're using.  (also, notationally, it's not clear what you're taking the expectation over -- z? theta?)  Equation 15 doesn't make sense.  As written, the matrices are the wrong shape.  Should the inner second derivative be in terms of r instead of theta?  The text has minor English difficulties, and could benefit from a grammar and word choice editing pass.  I stopped marking these pretty early on, but here are some specific suggested edits: 'two-folded' -> 'two-fold' 'framework of natural gradient' -> 'framework of the natural gradient' 'gradient protects about' -> 'gradient protects against' 'worrysome' -> 'worrisome' 'even though is called the same' -> 'despite the shared name' 'differ' -> 'defer' 'get map' -> 'get mapped'",1,5014
"summary: the paper presents a framework to learn to classify images that can come either from known or unknown classes. This is done by first mapping both images and classes into a joint embedding space. Furthermore, the probability of an image being of an unknown class is estimated using a mixture of Gaussians. Experiments on CIFAR-10 show how performance vary depending on the threshold use to determine if an image is of a known class or not.  review: - The idea of learning a joint embedding of images and classes is not new, but is nicely explained in the paper. - the authors relate to other works on zero-shot learning. I have not seen references to similarity learning,   which can be used to say if two images are of the same class. These can obviously be used to determine   if an image is of a known class or not, without having seen any image of the class. - The proposed approach to estimate the probability that an image is of a known class or not is based   on a mixture of Gaussians, where one Gaussian is estimated for each known class where the mean is   the embedding vector of the class and the standard deviation is estimated on the training samples of   that class. I have a few concerns with this:   * I wonder if the standard deviation will not be biased (small) since it is estimated on the training     samples. How important is that?   * I wonder if the threshold does not depend on things like the complexity of the class and the number     of training examples of the class. In general, I am not convinced that a single threshold can be used     to estimate if a new image is of a new class. I agree it might work for a small number of well     separate classes (like CIFAR-10), but I doubt it would work for problems with thousands of classes     which obviously are more interconnected to each other. - I did not understand what to do when one decides that an image is of an unknown class. How should it   be labeled in that case? - I did not understand why one needs to learn a separate classifier for the known classes, instead of   just using the distance to the known classes in the embedding space.*A brief summary of the paper's contributions, in the context of prior work* This paper introduces a zero-shot learning approach to image classification. The model first tries to detect whether an image contains an object from a so-far unseen category.  If not, the model relies on a regular, state-of-the art supervised classifier to assign the image to known classes. Otherwise, it attempts to identify what this object is, based on a comparison between the image and each unseen class, in a learned joint image/class representation space. The method relies on pre-trained word representations, extracted from unlabelled text, to represent the classes. Experiments evaluate the compromise between classification accuracy on the seen classes and the unseen classes, as a threshold for identifying an unseen class is varied.   *An assessment of novelty and quality* This paper goes beyond the current work on zero-shot learning in 2 ways. First, it shows that very good classification of certain pairs of unseen classes can be achieved based on learned (as opposed to hand designed) representations for these classes. I find this pretty impressive.  The second contribution is in a method for dealing with seen and unseen classes, based on the idea that unseen classes are outliers. I've seen little work attacking directly this issue. Unfortunately, I'm not super impressed with the results: having to drop from 80% to 70% to obtain between 15% and 30% accuracy on unseen classes (and only for certain pairs) is a bit disappointing. But it's a decent first step. Plus, the proposed model is overall fairly simple, and zero-shot learning is quite challenging, so in fact it's perhaps surprising that a simple approach doesn't do worse.  Finally, I find the paper reads well and is quite clear in its methodology.  I do wonder why the authors claim that they 'further extend [the] theoretical analysis [of Palatucci et a.] ... and weaken their strong assumptions'. This sentence suggests there is a theoretical contribution to this work, which I don't see. So I would remove that sentence.  Also, the second paragraph of section 6 is incomplete.  *A list of pros and cons (reasons to accept/reject)* The pros are: - attacks an important, very hard problem - goes significantly beyond the current literature on zero-shot learning - some of the results are pretty impressive  The cons are: - model is a bit simple and builds quite a bit on previous work on image classification [6] and unsupervised learning of word representation [15]    (but frankly, that's really not such a big deal)",0,5015
"This paper introduces a group-gated Boltzmann machine for learning the transformations between a pair of images more efficiently than with a standard gated Boltzmann machine. Experiments show the model learns phase invariant complex cells-like units grouped by frequency and orientation. These groups can also be manipulated to include overlapping neighbors in which case the model learns topographic pinwheel layouts of orientation, frequency and phase. The paper also mentions how the model is related to squared-pooling used in other learning methods.  Pros Interesting idea to add an additional connectivity matrix to the factors to enforce grouping behavior in a gated RBM. This is shown to be beneficial for learning translation invariant groups which are stable for frequency and orientation.  Good classification of rotations and scale are reported in Table 1, unfortunately these appear to be on toy, not natural, images. Impressive grouping of complex transformations such as translations and rotations are shown in Figure 3.  Figure 2 is a great figure. Clearly shows how a GRBM can represent all forms of frequency and orientation and combine these to represent translations. In general the paper was well written and has good explanatory figures.  Cons While the gabors learned on natural image patches are interesting it is hard to judge them without knowing how large they are. These details seem to be omitted from the paper.  It is not immediately obvious what applications would benefit from this type of model. It also seems like it could be relatively expensive computationally, and there was no mention of timing versus the standard gated Boltzmann machine model.  Novelty and Quality: This extension to gated Boltzmann machines is novel in that it allows grouping of features and increases the modelling power because the model no longer needs multiple feature to do simple translations. The paper was well written overall.The model presented in this paper is an extension of a previous model that extracts features from images, and these featuers are multiplied together to extract motion information (or other relation between two images). The novelty is to connect each feature of one image to several features of other image. This reuses the features. Futher, these connections are made in groups, and the features in the group will learn to have related properties. With overlaping groups one obtains pinwheel patterns observed in visual cortex. This is a different mechanism then previous ones.   - Please write the formulas for the full model that you train, not just the encoding. Even though they exist in other papers, they are not so complicated to write them down here.  - You say that the pinwheel patterns don't appear in rodents because they don't have a binocular vision. However you haven't actually obtained the pinwheels from binocularity but from video.  - The formula (9) is unclear and should be fixed. For example, how come the f index appears only once?  - Figure 3: What is index of parameter set? In text you talk about different datasizes - where are the results for these?This paper proposes a novel generalization of the Gated Boltzmann Machine. Unlike a traditional GBM, this model is constrained in a way that hidden units that are grouped together (groupings defined a priori) can gate each other's connections. The model is shown to produce group structure in the learned representations (topographic feature maps) as well as frequency and orientation consistency of the filters within each group.  This paper is well written, presents a novel learning paradigm and is of interest to the representation learning community, especially those researchers interested in higher-order RBMs and transformation learning.    Positive points of the paper:   * Novelty   * Readability   * Treatment of an area (transformation learning) that is, in my opinion, worthy of more attention in the representation learning community   * Makes connections to the 'group sparse coding' literature (where other papers have proposed encouraging the squared responses of grouped filters to be similar)   * Makes a good effort to explain the observed phenomena (e.g. in discussing the filter responses)  Negative points of the paper:   * Targets somewhat of a 'niche audience'; may be less accessible to the general representation learning community   * Presents a lot of qualitative but not quantitative results  Overall, it's a nice paper.  Some specific comments:  Fig 2: It's difficult to read/understand the frequency scale (left, bottom); it seems that frequency has been discretized; what do these bins represent, and how are they constructed?  In section 2.1, could you be more explicit about what you mean by 'matching' the input filters (and the output filters). I assume the matching is referring to the connectivity by connecting filters to mapping units? Matching comes up again in Section 3, so it would help to clarify this early on.  Check equation 8: what happened to C - should it not show up there?",0,5016
"Summary: The paper proposes to replace the final stages of Coates and Ng's CIFAR-10 classification pipeline. In place of the hand-designed 3x3 mean pooling layer, the paper proposes to learn a pooling layer. In place of the SVM, the paper proposes to use softmax regression jointly trained with the pooling layer.  The most similar prior work is Jia and Huang's learned pooling system. Jia and Huang use a different means of learning the pooling layer, and train a separate logistic regression classifier for each class instead of using one softmax model.  The specific method proposed here for learning the pooling layer is to make the pooling layer a densely connected linear layer in an MLP and train it jointly with the softmax layer.  The proposed method doesn't work quite as well as Jia and Huang's on the CIFAR-10 dataset, but does beat them on the less-competitive CIFAR-100 benchmark.  Pros: -The method is fairly simple and straightforward -The method improves on the state of the art of CIFAR-100 (at the time of submission, there are now two better methods known to this reviewer)  Cons: -I think it's somewhat misleading to call this operation pooling, for the following reasons: 1) It doesn't allow learning how to max-pool, as Jia and Huang's method does. It's sort of like mean pooling, but since the weights can be negative it's not even really a weighted average. 2) Since the weights aren't necessarily sparse, this loses most of the computational benefit of pooling, where each output is computed as a function of just a few inputs. The only real computational benefit is that you can set the hyperparameters to make the output smaller than the input, but that's true of convolutional layers too. -A densely connected linear layer followed by a softmax layer is representationally equivalent to a softmax layer with a factorized weight matrix. Any improvements in performance from using this method are therefore due to regularizing a softmax model better. The paper doesn't explore this connection at all. -The paper doesn't do proper controls. For example, their smoothness prior might explain their entire success. Just applying the smoothness prior to the softmax model directly might work just as well as factoring the softmax weights and applying the smoothness prior to one factor. -While the paper says repeatedly that their method makes few assumptions about the geometry of the pools, their 'pre-pooling' step seems to make most of the same assumptions as Jia and Huang, and as far as I can tell includes Coates and Ng's method as a special case.This paper proposes a method to jointly train a pooling layer and a classifier in a supervised way.  The idea is to first extract some features and then train a 2 layer neural net by backpropagation (although in practice they use l-bfgs). The first layer is linear and the parameters are box constrained and regularized to be spatially smooth. The authors propose also several little tricks to speed up training (divide the space into smaller pools, partition the features, etc.).  Most relevant work related to this method is cited but some references are missing. For instance, learning pooling (and unappealing) regions was also proposed by Zeiler et al. in an unsupervised setting: Differentiable Pooling for Hierarchical Feature Learning Matthew D. Zeiler and Rob Fergus arXiv:1207.0151v1 (July 3, 2012) See below for other missing references.  The overall novelty is limited but sufficient. In my opinion the most novel piece in this work is the choice of the regularizer that enforces smoothness in the weights of the pooling. This regularization term is not new per se, but its application to learning filters certainly is. The overall quality is fair. The paper lacks clarity in some parts and the empirical validation is ok but not great. I wish the authors stressed more the importance of the weight regularization and analyzed that part a bit more in depth instead of focussing on other aspects of their method which seem less exciting actually.  PROS + nice idea to regularize weights promoting spatial smoothness + nice visualization of the learned parameters  CONS - novelty is limited and the overall method relies on heuristics to improve its scalability - empirical validation is ok but not state of the art as claimed - some parts of the paper are not clear - some references are missing  Detailed comments: - The notation in sec. 2.2 could be improved. In particular, it seems to me that pooling is just a linear projection subject to constraints in the parameterization. The authors mentions that constraints are used just for interpretability but I think they are actually important to make the system 'less unidentifiable' (since it is the composition of two linear stages).  Regarding the box  constraints, I really do not understand how the authors modified l-bfgs to account for these box constraints since this is an unconstrained optimization method. A detailed explanation is required for making this method reproducible. Besides, why not making the weights non-negative and sum to one instead? - The pre-pooling step is unsatisfying because it seems to defeat the whole purpose of the method. Effectively, there seem to be too many other little tricks that need to be in place to make this method competitive. - Other people have reported better accuracy on these datasets. For instance,  Practical Bayesian Optimization of Machine Learning Algorithms Jasper Snoek, Hugo Larochelle and Ryan Prescott Adams Neural Information Processing Systems, 2012 - There are lots of imprecise claims:   - convolutional nets before HMAX and SPM used pooling and they actually learned weights in the average pooling/subsampling step   - 'logistic function' in pag. 3 should be 'softmax function'   - the contrast with the work by Le et al. on pag.4 is weak since although pooling regions can be trained in parallel but the classifier trained on the top of them has to be done afterwards. This sequential step makes the whole procedure less parallelizable.   - second paragraph of sec. 3.2 about 'transfer pooling regions' is not clear.The paper presents a method for training pooling regions in image classification pipelines (similar to those that employ bag-of-words or spatial pyramid models).  The system uses a linear pooling matrix to parametrize the pooling units and follows them with a linear classifier.  The pooling units are then trained jointly with the classifier.  Several strategies for regularizing the training of the pooling parameters are proposed in addition to several tricks to increase scalability.  Results are presented on the CIFAR10 and CIFAR100 datasets.  The main idea here appears to be to replace the 'hard coded' average pooling stage + linear classifier with a trainable linear pooling stage + linear classifier.  Though I see why this is natural, it is not clear to me why using two linear stages is advantageous here since the combined system is no more powerful than connecting the linear classifier directly to all the features.  The two main advantages of competing approaches are that they can dramatically reduce dimensionality or identify features to combine with nonlinear pooling operations.  It could be that the performance advantage of this approach (without regularization) comes from directly learning the linear classifier from all the feature values (and thus the classifier has lower bias).  The proposed regularization schemes applied to the pooling units potentially change the picture.  Indeed the authors found that a 'smoothness' penalty (which enforces some spatial coherence on the pooling weights) was useful to regularize the system, which is quite similar to what is achieved using hand-coded pooling areas.  The advantage is that the classifier is given the flexibility to choose other weights for all of the feature values while retaining regularization that is similar to hand-coded pooling.  How useful this effect is in general seems worth exploring in more detail.  Pros: (1)  Potentially interesting analysis of regularization schemes to learn weighted pooling units. (2)  Tricks for pre-training the pooling units in batches and transferring the results to other datasets.  Cons: (1)  The method does not appear to add much power beyond the ability to specify prior knowledge about the smoothness of the weights along the spatial dimensions. (2)  The results show some improvement on CIFAR-100, but it is not clear that this could not be achieved simply due to the greater number of classifier parameters (as opposed to the pooling methods proposed in the paper.)",0,5017
"This paper introduces a dictionary learning technique that incorporates time delays or shifts on the learned dictionary, called JADL, to better account for this structure in multi-trial neuroelectric signals. The algorithm uses the previous dictionary learning framework and non-convex optimization, but adds a selection step over possible shifts for each atom (for each point), framed as an l-0 optimization. This objective is the main contribution of the paper, which enables better performance for time-delayed data as well as potentially useful temporal structure to be extracted from the data.  The paper introduces a novel objective for addressing the time shift problem (e.g in M/EEG data), but frames a typical coordinate descent approach for solving the resulting non-convex problem.  The main difference in the optimization is (1) ensuring that the coefficients, a, for the dictionary, D, block-wise satisfy the l-0 constraint by disabling updates to all but one coefficient within a block and (2) modifying the gradient update in block coordinate descent on the dictionary, D, which now has a shift operator around D. Taking this obvious solution route leads to a non-convex optimization and potentially lengthy computation (as the delta set size increases). The quality of writing and experiments is high.  Pros 1. The proposed JADL algorithm facilitates application of dictionary learning techniques to M/EEG data, which is an important application. Moreover, as a secondary benefit, it allows time-delay structure to be learned.  2. The writing is mostly clear and the paper is well organized.  3. Experimental results are comprehensive and include important details of their experimental procedure.  Cons 1. The computational requirements of this algorithm are not explored, though the larger dictionary in JADL (due to the addition of delta shifts) could significantly slow learning.  2. For clarity: (a) Include examples of shifts, Delta, in the problem statement (such as the ones used in the experiments). (b) Include examples of the types of data that could benefit from this framework, to better justify the importance of framing the problem with time-shifts and better explain what is meant by 'features [being] well-aligned across signals'.  (c ) The explanation of how to enforce constraint (7) should be improved, e.g. 'block all other coefficients a_j^{S,i}' should probably be 'block all other coefficients in segment a_j^{S,i}', but the meaning is actually significantly different and this was quite confusing.  3. The comment that the parameter, lambda, is no longer important because sparsity is induced by the constraint in (7) suggests that as the size of the set of delta increases, this problem formulation no longer learns a sparse solution over the chosen dictionary. I would suggest that this is not the case, but rather that the datasets used in this paper had a small dictionary and did not require the coefficients to be sparse. The constraint in (7) simply ensures that only one delta is chosen per atom, but does not guarantee that the final solution over the delta-shifted dictionary will be sparse. Therefore, if the number of atoms is large, the regularizer || a_j ||_1 should still be important. It is true that constraint (7) ensures the solution is sparse over all possible delta-shifted dictionaries; this is however an unfair comparison to other dictionary learning techniques which have a much smaller dictionary space to weight over.  4. Con (3) suggests that learning with a very large dictionary (the size of the delta-shifted dictionary set) and setting the lambda parameter large might have more comparable performance to the algorithm suggested in this paper and should be included. Of course, this highly regularized approach on a large dictionary would not explicitly provide the time shift structure in the data as does JADL, but would be an interesting and more fair comparison. However, if the time-shift structure is not actually useful (and is simply used to improve learning), then DL with a large dictionary and large regularization parameter, lambda, could be all that is needed to deal with this problem for EEG data. The authors should clarify this difference and contribution more clearly.   Minor Comments: 1. For a reference on convex solution to the dictionary learning problem, see 'Convex sparse matrix factorizations', F. Bach, J. Mairal and J. Ponce. 2008; and 'Convex Sparse Coding, Subspace Learning and Semi-Supervised Extensions', X. Zhang, Y. Yu, M. White, R. Huang and D. Schuurmans. 2011. 2. There should be citations for the claim: 'This issue is currently the biggest challenge in M/EEG multi-trial analysis.' 3. Bottom of page 3: 'which allows to solve it' -> 'which allows us to solve it' 4. Page 5: 'property allows to' -> 'property allows us to'This paper introduces a sparse coding variant called 'jitter-adaptive' sparse coding, aimed at improving the efficiency of sparse coding by augmenting a dictionary with temporally shifted elements. The motivating use case is EEG data, where neural activity can arise at any time, in atoms that span multiple recording channels. Ideally these motifs would be recognized as the dictionary components by a dictionary-learning algorithm.  EEG data has been analyzed with sparse coding before, as noted by the authors, and the focus of this paper is the use of jitter-adaptive dictionary learning to achieve a more useful signal decomposition.  The use of jitter adaptive dictionary learning is indeed an intuitive and effective strategy for recovering the atoms of synthetic and actual data.  One weakness of this paper is that the technique of augmenting a dictionary by a time-shifting operator is not entirely novel, and the authors should compare and contrast their approach with e.g.:  - Continuous Basis Pursuit - Deconvolutional Networks- Charles Cadieu's PhD work - The Statistical Inefficiency of Sparse Coding for Images (http://arxiv.org/abs/1109.6638)     Pro(s) - jitter-adaptive learning is an effective strategy for applying sparse coding    to temporal data, particularly EEG   Con(s) - paper would benefit from clarification of contribution relative to previous   workThe paper proposes a method for learning shiftable dictionary elements - i.e., each dictionary is allowed to shift to its optimal position to model structure in a signal.  Results on test data show a significant improvement over regular sparse coding dictionary learning for recovering structure in data, and results on LFP data provide a more interpretable result.  This seems like a sensible approach and the results are pretty convincing.  The choice of data seems a bit odd - all the LFP waveforms look the same, perhaps it would be worthwhile to expand the waveform so we can see more structure than just a spike.    This approach could be easily confused for a convolution model.  The difference here is that the coefficients are mutually exclusive over shift.  the authors may want to point out the similarities and differences to a convolution sparse coding model for the reader.",1,5018
"- A brief summary of the paper's contributions, in the context of prior work.  This paper proposes a new energy function (or scoring function) for ranking pairs of entities and their relationship type. The energy function is based on a so-called Neural Tensor Network, which essentially introduces a bilinear term in the computation of the hidden layer input activations of a single hidden layer neural network. A favorable comparison with the energy-function proposed in Bordes et al. 2011 is presented.  - An assessment of novelty and quality.  This work follows fairly closely the work of Border et al. 2011, with the main difference being the choice of the energy/scoring function. This is an advantage in terms of the interpretability of the results: this paper clearly demonstrates that the proposed energy function is better, since everything else (the training objective, the evaluation procedure) is the same. This is however a disadvantage in terms of novelty as this makes this work somewhat incremental.  Bordes et al. 2011 also proposed an improved version of their model, using kernel density estimation, which is not used here. However, I suppose that the proposed model in this paper could also be similarly improved.  More importantly, Bordes and collaborators have more recently looked at another type of energy function, in 'Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing' (AISTATS 2012), which also involves bilinear terms and is thus similar (but not the same) as the proposed energy function here. In fact, the Bordes et al. 2012 energy function seems to outperform the 2011 one (without KDE), hence I would argue that the former would have been a better baseline for comparisons.  - A list of pros and cons (reasons to accept/reject).  Pros: Clear demonstration of the superiority of the proposed energy function over that of Bordes et al. 2011.  Cons: No comparison with the more recent energy function of Bordes et al. 2012, which has some similarities to the proposed Neural Tensor Networks.  Since this was submitted to the workshop track, I would be inclined to have this paper accepted still. This is clearly work in progress (the submitted paper is only 4 pages long), and I think this line of work should be encouraged. However, I would suggest the authors also perform a comparison with the scoring function of Bordes et al. 2012 in future work, using their current protocol (which is nicely setup so as to thoroughly compare energy functions).This paper proposes a new model for modeling data of multi-relational knowledge bases such as Wordnet or YAGO. Inspired by the work of (Bordes et al., AAAI11), they propose a neural network-based scoring function, which is trained to assign high score to plausible relations. Evaluation is performed on Wordnet.  The main differences w.r.t. (Bordes et al., AAAI11) is the scoring function, which now involves a tensor product to encode for the relation type and the use of a non-linearity. It would be interesting if the authors could comment the motivations of their architecture. For instance, what does the tanh could model here?  The experiments raise some questions: - why do not also report the results on the original data set of (Bordes et al., AAAI11)? Even, is the data set contains duplicates, this stills makes a reference point. - the classification task is hard to motivate. Link prediction is a problem of detection: very few positive to find in huge set of negative examples. Transform that into a balanced classification problem is a non-sense to me.  There have been several follow-up works to (Bordes et al., AAAI11) such as (Bordes et al., AISTATS12) or (Jenatton et al., NIPS12), that should be cited and discussed (some of those involve tensor for coding the relation type as well). Besides, they would also make the experimental comparison stronger.  It should be explained how the pre-trained word vectors trained by the model of Collobert & Weston are use in the model. Wordnet entities are senses and not words and, of course, there is no direct mapping from words to senses. Which heuristic has been used?  Pros: - better experimental results  Cons: - skinny experimental section - lack of recent references      Semantic Word Vectors",0,5019
"The paper by Rose & Arel entitled 'Gradient Driven Learning for Pooling in Visual  Pipeline Feature Extraction Models' describes a new approach for optimizing hyper parameters in spatial pyramid-like architectures.  Specifically, an architecture is presented which corresponds to a spatial pyramid where a  two-layer neural net replaces the SVM in the final classification stage. The key contribution is to formulate the pooling operation in the spatial pyramid as a weighted sum over inputs which enables learning of the pooling receptive fields via back-propagation.  Pros: The paper addresses an important problem in the field of computer vision. Spatial pyramids are currently quite popular in the computer vision community and optimizing the many free parameters which are normally tuned by hand is a key problem.  Cons: The contribution of the present work remains very limited both in terms of the actual problem formulation and the empirical evaluation (no comparison to alternative approaches such as the recent work by Jia et al [ref 5] is shown). The overall 0.5% improvement in accuracy over non-optimized hyper-parameters is quite disappointing. In future work, the authors should compare their approach with alternative approaches in addition to suggesting significant improvement over non-optimized/standard parameters.  Additional comments: Additional references that could be added, discussed and/or used for benchmark. Y Boureau and J Ponce. A theoretical analysis of feature pooling in visual recognition. In ICML, 2010 and Pinto N, Doukhan D, DiCarlo JJ, Cox DD (2009). A High-Throughput Screening Approach to Discovering Good Forms of Biologically-Inspired Visual Representation. PLoS Computational Biology 5(11): e1000579. doi:10.1371/journal.pcbi.1000579.The paper proposes to learn the weights of the pooling region in a neural network for recognition.  The idea is a good one, but the paper is a bit terse.  Its not really clear what we are looking at in Figure 1b - the different quadrants and so forth - but I would guess the red blobs are the learned pooling regions.  Its kind of what you expect, so it also begs the question of whether this teaches us anything new. But still it seems like a sensible approach and worth reporting.   I suppose one can view it as a validation of the pooling envelope that is typically assumed.",1,5020
"Summary and general overview: ---------------------------------------------- The paper introduces Discriminative Recurrent Sparse Auto-Encoders, a new model, but more importantly a careful analysis of the behaviour of this model. It suggests that the hidden layers of the model learn to differentiate into a hierarchical structure, with part units at the bottom and categorical units on top.   Questions and Suggestions ---------------------------------------- 1. Given equation (2) it seems that model is very similar to recurrent neural network with rectifier units as the one used for e.g. in [1]. The main difference would be how the model is being trained (the pre-training stage as well as the additional costs and weight norm constraints). I think this observation could be very useful, and would provide a different way of understanding the proposed model. From this perspective, the differentiation would be that part units have weak recurrent connections and are determined mostly by the input (i.e. behave as mlp units would), while categorical units have strong recurrent connections. I'm not sure if this parallel would work or would be helpful, but I'm wondering if the authors explored this possibility or have any intuitions about it.  2. When mentioning that the model is similar to a deep model with tied weights, one should of course make it clear that additionally to tied weights, you feed the input (same input) at each layer. At least this is what equation (2) suggests. Is it the case? Or is the input fed only at the first step ?   2. As Yoshua Bengio pointed out in his comment, I think Recurrent Networks, and hence DrSAE, suffer more from the vanishing gradient problem than deep forward models (contrary to the suggestion in the introduction). The reason is the lack of degrees of freedom RNNs have due to the tied weights used at each time step. If W for an RNN is moved such that its largest eigenvalue becomes small enough, the gradients have to vanish. For a feed forward network, all the W_i of different layers need to change such to have this property which seems a less likely event. IMHO, the reason for why DrSAE seem not to suffer too much from the vanishing gradient is due to (a) the norm constraint, and (b) the reconstruction cost found at each step which provide additional error signal. One could also say that 11 steps might not be a high enough number for vanishing gradient to make learning prohibitive.   3. Could the authors be more specific when they talk about bounding the column-wise norm of a matrices. Is this done through a soft constraint added to the cost? Is it done, for e.g.,  by scaling D if the norm exceeds the chosen bound ? Is there a projection done at each SGD step ? It is not clear from the text how this works.   4. The authors might have expected this from reviewers, but anyway. Could the authors validate this model (in a revision of the paper) on different datasets, beside MNIST? It would be useful to know that you see the same split of hidden units for more complex datasets (say CIFAR-10)  5.  The model had been run with only 2 temporal steps. Do you still get some kind of split between categorical and part hidden units ? Did you attempt to see how the number of temporal steps affect this division of units ?   References: [1] Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu, Advances in Optimizing Recurrent Networks, arXiv:1212.0901Authors propose an interesting idea to use deep neural networks with tied weights (recurrent architecture) for image classification. However, I am not familiar enough with the prior work to judge novelty of the idea.  On the critical note, the paper is not easy to read without good knowledge of prior work, and is pretty long. I would recommend authors to consider following to make their paper more accessible:  - the description should be shorter, simpler and self-contained - try to avoid the ad-hoc constants everywhere - run experiments on something larger and more difficult than MNIST - current experiments are not convincing to me; together with many hand-tuned constants, I would be afraid that this model might not work at all on more realistic tasks (or that a lot of additional manual work would be needed) - when you claim that accuracy degrades from 1.21% to 1.49% if 2 instead of 11 time steps are used, you are comparing models with much different computational complexity: try to be more fair - also, it would be interesting to show results for the larger model (400 neurons) with less time steps than 11  Still, I consider the main idea interesting, and I believe it would lead to interesting discussions at the conference.SUMMARY:  The authors describe a discriminative recurrent sparse auto-encoder, which is essentially a recurrent neural network with a fixed input and linear rectifier units. The auto-encoder is initially trained to reproduce digits of MNIST, while enforcing a sparse representation. In a later phase it is trained in a discriminative (supervised) fashion to perform classification.  The authors discuss their observations. Most prominently they describe the occurrence of two types of nodes: part-units, and categorical units The first are units that encode low-level features such as pen-strokes, whereas the second encode specific digits within the MNIST set. It is shown that before the discriminative training, the image reconstruction happens mostly by combining pen-strokes, whereas after the discriminative training, image reproduction happens mainly by the combination of a prototype digit of the corresponding class, which is subsequently transformed by adding pen-stroke-like features. The authors state that this observation is consistent with the underlying hypothesis of auto-encoders that the data lies on low-dimensional manifolds, and the auto-encoder learns to split the representation of a digit into a categoric prototype and a set of transformations.  GENERAL OPINION  The paper and the suggested network architecture is interesting and, as far as I know, quite original. It is also compelling to see the unique ways in which the unsupervised and supervised training contribute to the image reconstruction. Overall I believe this paper is a suited contribution to this conference. I have some questions and remarks that I will list here.   QUESTIONS   - From figure 5 I get the impression that the states dynamics are convergent; for sufficiently large T, the internal state of the nodes (z) will no longer change. This begs the question: is the ideal situation that where T goes to infinity? If so, could you consider the following scenario: We somehow compute the fixed, final state $z(infty)$ (maybe this can be performed faster than by simply iterating the system). Once we have it, we can perform backpropagation-through-time on a sequence where each step in time, the states are identical (the fixed-point state). This would be an interesting scenario, as you might be able to greatly accelerate the training process (all Jacobians are identical, error backpropagation has an analytical solution), and you explicitly train the system to perform well on this fixed point, transient effects are no longer important. Perhaps I'm missing some crucial detail here, but it seems like an interesting scenario to discuss.   - On a related note: what happens if - after training - the output (image reconstruction and classification) is constructed using the state from a later/earlier point in time? How would performance degrade as a function of time?  REMARKS   - In both the abstract and the introduction the following sentence appears: 'The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters'. I believe this is an dangerous statement, as tied weights will also impose a severe restriction on representational power (so they will not have 'all the power of deep networks'). I would agree with a rephrasing of this sentence that says something along the lines of: 'The depth implicit in the temporally-unrolled form allows the system to exhibit far more representational power, while keeping the number of trainable parameters fixed'.    - I agree with the Yoshua's remark on the vanishing gradient problem. Tied weights cause every change in parameter space to be exponentially amplified/dampened (save for nonlinear effects), making convergence harder. The authors should probably rewrite this sentence.  - I deduce from the text that the system is only trained to provide output (image reconstruction and classification) at the T-th iteration. As such, the backpropagated error only is 'injected' at this point in time. This is distinctly different form the 'common' BPTT setup, where error is injected at each time step, and the authors should maybe explicitly mention this. Apparently reviewer 'Anonymous 8ddb' has interpreted the model as if it was to provide output at each time step ('the reconstruction cost found at each step which provide additional error signal'), so definitely make this more clear.  - The authors mention that they trained the DrSAE with T=11, so 11 iterations. I suspect this number emerges from a balance between computational cost and the need for a sufficient amount of iterations? Please explicitly state this in your paper.  - As a general remark, the comparison to ISTA and LISTA is interesting, but the authors go to great lengths to finding detailed analogies, which might not be that informative. I am not sure whether the other reviewers would agree with me, but maybe the distinction between categorical and part-units can be deduced without this complicated and not easy-to-understand analysis. It took me some time to figure out the content of paragraphs 3.1 and 3.2.   - I also agree with other reviewers that it is unfortunate that only MNIST has been considered. Results on more datasets, and especially other kinds of data (audio, symbolic?) might be quite informativeThe paper describes the following variation of an autoencoder: An encoder (with relu nonlinearity) is iterated for 11 steps, with observations providing biases for the hiddens at each step. Afterwards, a decoder reconstructs the data from the last-step hiddens. In addition, a softmax computes class-labels from the last-step hiddens. The model is trained on labeled data using the sum of reconstruction and classification loss. To perform unsupervised pre-training the classification loss can be ignored initially.  It is argued that training the architecture causes hiddens to differentiate into two kinds of unit (or maybe a continuum): part-units, which mainly try to perform reconstruction, and categorical units, which try to perform classification. Various plots are shown to support this claim empirically.  The idea is interesting and original. The work points towards a direction that hasn't been explored much, and that seems relevant in practice and from the point of view of how classification may happen in the brain. Some anecdotal evidence is provided to support the part-categorical separation claim. The evidence seems interesting. Though I'm pondering still whether there may be other explanations for those plots. Training does seem to rely somewhat on finely tuned parameter settings like individual learning rates and weight bounds.  It would be nice to provide some theoretical arguments for why one should expect the separation to happen. A more systematic study would be nice, too, eg. measuring how many recurrent iterations are actually required for the separation to happen. To what degree does that separation happen with only pre-training vs. with the classification loss? And in the presence of classification loss, could it happen with shallow model, too? The writing and organization of the paper seems preliminary and could be improved. For example, it is annoying to jump back-and-forth to refer to plots, and some plots could be made more informative (see also comments below).   The paper seems to suggest that the model gradually transforms an input towards a class-template. I'm not sure if I agree, that this is the right view given that the input is clamped (by providing biases via E) so it is available all the time. Any comments?   It may be good to refer to 'Learning continuous attractors in recurrent networks', Seung, NIPS 1998, which also describes a recurrent autoencoder (though that model is different in that it iterates encoder+decoder not just encoder with clamped data).   Questions/comments:  - It would be much better to show the top-10 part units and the top-10 categorical units instead of figure 2, which shows a bunch of filters for which it is not specified to what degree they're which (except for pointing out in the text that 3 of them seem to be more like categorical units).  - What happens if the magnitude of the rows of E is bounded simply by 1/T instead of 1.25/(T-1) ? (page 3 sentence above Eq. 4) Are learning and classification results sensitive to that value?  - Last paragraph of section 1: 'through which the prototypes of categorical-units can be reshaped into the current input': Don't you mean the other way around?  - Figure 4 seems to suggest that categorical units can have winner-takes-all dynamics that disfavor other categorical units from the same class.  Doesn't that seem strange?   - Section 3.2 (middle) mentions why S-I is plotted but S-I is shown and referred to before (section 3.1) and the explanation should instead go there.  - What about the 2-step model result with 400 hiddens (end of section 4)?",1,5021
"The paper provides a theoretical analysis of Restricted Boltzmann Machines with multivalued discrete units, with the emphasis on representation capacity of such models.  Discrete RBMs are a special case of exponential family harmoniums introduced by Welling et al. [1] and have been known under the name of multinomial or softmax RBMs. The parameter updates given in the paper, which are its only not purely theoretical contribution, are not novel and have been known for some time. Though the authors claim that their analysis can serve as a starting point for developing novel machine learning algorithms, I am unable to see how that applies to any of the results in the paper. Thus the only contributions of the paper are theoretical.  Unfortunately, those theoretical contributions do not seem particularly interesting, at least from the machine learning perspective, appearing to be direct generalizations of the corresponding results for binary RBMs. The biggest problem with the paper, however, is presentation. The paper is clearly not written for a machine learning audience. The presentation is extremely technical and even the 'non-technical' outline in Section 4 is difficult to follow. Given that the only novel contribution of the paper is the results proved in it, it is unreasonable to put all the proof in the supplementary material where they are unlikely to receive the necessary attention. The fact that the proofs will not fit in the paper due to the ICLR page limit, simply highlights the fact that this paper should be submitted to a journal.  [1] Welling, M., Rosen-Zvi, M., & Hinton, G. (2005). Exponential family harmoniums with an application to information retrieval. Advances in Neural Information Processing Systems, 17, 1481-1488.This paper presents a comprehensive theoretical discussion on the approximation properties of discrete restricted Boltzmann machines. The paper is clearly written. It provides a contextual introduction to the theoretical results by reviewing approximation results for Naive Bayes models and binary restricted Boltzmann machines. Section 4 of the paper lists the theoretical contributions, while proofs are are delayed to the appendix.  Notably, the first result gives conditions, based on the number of hidden and visible units together with their cardinalities, for the joint RBM to be a universal approximator of distributions over the visible units. The theorem provides an extension to previous results for binary RBMs. The second result shows that discrete RBMs can represent distributions with a number of strong modes that is exponential in the number of hidden units, but not necessarily exponential in the number of parameters. The third result shows that discrete RBMs can approximate any mixture of product distributions, with disjoint supports, arbitrarily well.  Proposition 10 is a nice result showing that a discrete RBM is a Hadamard product of mixtures of product distributions. These decompositions often help with the design of inference algorithms. Lemma 25 provides useful connections between RBMs and mixtures. Subsequently theorem 27 discusses the relation to exponential families. Theorem 29 provides a very nice approximation bound for the KL divergence between the RBM and a distribution in the set of all distributions over the discrete state space, and so on. The paper also presents a geometry analysis but I did not follow all the appendix details about these.  Finally the appendices discuss interactions within layers and training. With regard to the first issue, I think the authors should consult  H. J. Kappen. Deterministic learning rules for Boltzmann machines. Neural Networks, 8(4):537-548, 1995  which discusses these lateral connections and approximation properties. With regard to training,  I recommend the following expositions to the authors. The last one considers a different aspect of the theory of RBMS, namely statistical efficiency of the estimators:  Marlin, Benjamin, Kevin Swersky, Bo Chen, and Nando de Freitas. 'Inductive principles for restricted boltzmann machine learning.' In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 509-516. 2010.  Tieleman, Tijmen, and Geoffrey Hinton. 'Using fast weights to improve persistent contrastive divergence.' In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 1033-1040. ACM, 2009.  Marlin, Benjamin, and Nando de Freitas. 'Asymptotic Efficiency of Deterministic Estimators for Discrete Energy-Based Models'. UAI 2011.   The above provide a more clear picture of stochastic maximum likelihood as well as deterministic estimators.  Minor: Why does your paper end with a b?  In remark 5. It might be easier to simply use x throughout instead of v.This paper reviews properties of the Naive Bayes models and Binary RBMs before moving on to introducing discrete RBMs for which they extend universal approximation and other properties.  I think such a review and extensions are extremely interesting for the more theoretical fields such as algebraical geometry. As it is, the paper does not cater to a machine learning crowd as it's mostly a sequence of mathematical definitions and theorems statements. I advise the authors to either: - submit it to an algebraic geometry venue - give as many intuitions as possible to help the reader get a full grasp on the results presented.  For the latter point, I advise against using sentences such as 'In algebraic geometrical terms this is a Hadamard product of a collection of secant varieties of the Segre embedding of the product of a collection of projective spaces'. Though it sounds incredibly intelligent, I didn't get anything from it, despite my fair knowledge of RBMs.  This work of explaining the results is done fairly well in the Results section, especially for the universal approximation property and the approximation error. This is a good target for the review part of the paper.",0,5022
"Summary and general overview: ---------------------------------------------- The paper tries to explore an online regime for Hessian Free as well as using drop outs. The new method is called Stochastic Hessian Free and is tested on a few datasets (MNIST, USPS and Reuters).  The approach is interesting and it is a direction one might need to consider in order to scale to very large datasets.   Questions: --------------- (1) An aesthetic point. Stochastic Hessian Free does not seem as a suitable name for the algorithm, as it does not mention the use of drop outs. I think scaling to a stochastic regime is an orthogonal issue to using drop outs, so maybe Drop-out Stochastic Hessian Free would be more suitable, or something rather, that makes the reader aware of the use of drop-outs.  (2) Page 1, first paragraph. Is not clear to me that SGD scales well for large data. There are indications that SGD could suffer, for e.g., from under-fitting issues (see [1]) or early over-fitting (see [2]). I'm not saying you are wrong, you are probably right, just that the sentence you use seems a bit strong and we do not yet have evidence that SGD scales well to very large datasets, especially without the help of things like drop-outs (which might help with early-overfitting or other phenomena).   (3) Page 1, second paragraph. Is not clear to me that HF does not do well for classification. Is there some proof for this somewhere? For e.g. in [3] a Hessian Free like approach seem to do well on classification (note that the results are presented for Natural Gradient, but the paper shows that Hessian Free is Natural Gradient due to the use of Generalized Gauss-Newton matrix).  (4) Page 3, paragraph after the formula. R-operator is only needed to compute the product of the generalized Gauss-Newton approximation of the Hessian with some vector `v`. The product between the Hessian and some vector 'v' can easily be computed as d sum((dC/dW)*v)/dW (i.e. without using the R-op).   (5) Page 4, third paragraph. I do not understand what you mean when you talk about the warm initialization of CG (or delta-momentum as you call it). What does it mean that hat{M}_	heta is positive ? Why is that bad? I don't understand what this decay you use is suppose to do? Are you trying to have some middle ground between starting CG from 0 and starting CG from the previous found solution? I feel a more detailed discussion is needed in the paper.   (6) Page 4, last paragraph. Why does using the same batch size for the gradient and for computing the curvature results in lambda going to 0? Is not obvious to me. Is it some kind of over-fitting effect? If it is just an observation you made through empirical experimentation, just say so, but the wording makes it sound like you expect this behaviour due to some intuitions you have.   (7) Page 5, section 4.3. I feel that the affirmation that drop-outs do not require early stopping is too strong. I feel the evidence is too weak at the moment for this to be a statement. For one thing, eta_e goes exponentially fast to 0. eta_e scales the learning rate, and it might be the reason you do not easily over-fit (when you reach epoch 50 or so you are using a extremely small learning rate). I feel is better to make this as an observation. Also could you maybe say something about this decaying learning rate, is my understanding of eta_e correct?    (8) I feel a important comparison would be between your version of stochastic HF with drop-outs vs stochastic HF (without the drop outs) vs just HF. From the plots you give, I'm not sure what is the gain from going stochastic, nor is it clear to me that drop outs are important. You seem to have the set-up to run this additional experiments easily.    Small corrections: -------------------------- Page 1, paragraph 1, 'salable` -> 'scalable' Page 2, last paragraph. You wrote : 'B is a curvature matrix suc as the Hessian'. The curvature of a function `f` at theta is the Hessian (there is no choice) and there is only one curvature for a given function and theta. There are different approximations of the Hessian (and hence you have a choice on B) but not different curvatures. I would write only 'B is an approximation of the curvature matrix` or `B is the Hessian`.  References:  [1] Yann N. Dauphin, Yoshua Bengio, Big Neural Networks Waste Capacity,  arXiv:1301.3583 [2] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent and Samy Bengio, Why Does Unsupervised Pre-training Help Deep Learning? (2010), in: Journal of Machine Learning Research, 11(625--660) [3] Razvan Pascanu, Yoshua Bengio, Natural Gradient Revisited, arXiv:1301.3584This paper makes an attempt at extending the Hessian-free learning work to a stochastic setting.  In a nutshell, the changes are:  - shorter CG runs - cleverer information sharing across CG runs that has an annealing effect - using differently-sized mini-batches for gradient and curvature estimation (former sizes being larger) - Using a slightly modified damping schedule for lamdba than Martens' LM criteria, which encourages fewer oscillations.  Another contribution of the paper is the integration of dropouts into stochastic HF in a sensible way. The authors also include an exponentially-decaying momentum-style term into the parameter updates.  The authors present but do not discuss results on the Reuters dataset (which seem good). There is also no comparison with the results from [4], which to me would be a natural thing to compare to.  All in all, a series of interesting tricks for making HF work in a stochastic regime, but there are many questions which are unanswered. I would have liked to see more discussion *and* experiments that show which of the individual changes that the author makes are responsible for the good performance. There is also no discussion on the time it takes the stochastic HF method to make on step / go through one epoch / reach a certain error.   SGD dropout is a very competitive method because it's fantastically simple to implement (compared to HF, which is orders of magnitude more complicated), so I'm not yet convinced by the insights of this paper that stochastic HF is worth implementing (though it seems easy to do if one has an already-running HF system).This paper looks at designing an SGD-like version of the 'Hessian-free' (HF) optimization approach which is applied to training shallow to moderately deep neural nets for classification tasks.  The approach consists of the usual HF algorithm, but with smaller minibatches and with CG terminated after only 3-5 iterations.   As advocated in [20], more careful attention is paid to the 'momentum-constant' gamma.  It is somewhat interesting to see a very data intensive method like HF made 'lighter' and more SGD-like, since this could perhaps provide benefits unique to both HF and SGD, but it's not clear to me from the experiments if there really is an advantage over variants of SGD that would perform some kind of automatic adaptation of learning rates (or even a fixed schedule!).   The amount of novelty in the paper isn't particularly high since many of these ideas have been proposed before ([20]), although perhaps in less extreme or less developed forms.     Pros: - takes the well-known approach HF in a different (if not entirely novel) direction - seems to achieves performance competitive with versions of SGD used in [3] with dropout Cons: - experiments don't look at particularly deep models and aren't very thorough - comparisons to other versions of SGD are absent (this is my primary issue with the paper)  ----   The introduction and related work section should probably clarify that HF is an instance of the more general family of methods sometimes known as 'truncated-Newton methods'.  In the introduction, when you state:  'HF has not been as successful for classification tasks', is this based on your personal experience, particularly negative results in other papers, or lack of positive results in other papers?  Missing from your review are papers that look at the performance of pure stochastic gradient descent applied to learning deep networks, such as [15] did, and the paper by Glorot and Bengio from AISTATS 2010.   Also, [18] only used L-BFGS to perform 'fine-tuning' after an initial layer-wise pre-training pass.   When discussing the generalized Gauss-Newton matrix you should probably cite [7].  In section 4.1, it seems like a big oversimplification to say that the stopping criterion and overall convergence rate of CG depend on mostly on the damping parameter lambda.  Surely other things matter too, like the current setting of the parameters (which determine the local geometry of the error surface).  A high value of lambda may be a sufficient condition, but surely not a necessary one for CG to quickly converge.  Moreover, missing from the story presenting in this section is the fact that lambda *must* decrease if the method is to ever behave like a reasonable approximation of a Newton-type method.  The momentum interpretation discussed in the middle of section 4, and overall the algorithm discussed in this paper, sounds similar to ideas discussed in [20] (which were perhaps not fully explored there).   Also, a maximum iteration for CG is was used in the original HF paper (although it only appeared in the implementation, and was later discussed in [20]).  This should be mentioned.  Could you provide a more thorough explanation of why lambda seems to shrink, then grow, as optimization proceeds?  The explanation in 4.2 seems vague/incomplete.  The networks trained seem pretty shallow (especially Reuters, which didn't use any hidden layers).  Is there a particular reason why you didn't make them deeper?  e.g. were deeper networks overfitting more, or perhaps underfitting due to optimization problems, or simply not providing any significant advantage for some other reasons?  SGD is already known to be hard to beat for these kinds of not-very-deep classification nets, and while it seems plausible that the much more SGD-like HF which you are proposing would have some advantage in terms of automatic selection of learning rates, it invites comparison to other methods which do this kind of learning rate tuning more directly (some of which you even discuss in the paper).  The lack of these kinds of comparisons seems like a serious weakness of the paper.  And how important to your results was the use of this 'delta-momentum' with the particular schedule of values for gamma that you used?  Since this behaves somewhat like a regular momentum term, did you also try using momentum in your SGD implementation to make the comparison more fair?  The experiments use drop-out, but comparisons to implementations that don't use drop-out, or use some other kind of regularization instead (like L2) are noticeably absent.  In order understand what the effect of drop-out is versus the optimization method in these models it is important to see this.  I would have been interested to see how well the proposed method would work when applied to very deep nets or RNNs, where HF is thought to have an advantage that is perhaps more significant/interesting than what could be achieved with well tuned learning rates.",0,5023
"Segmentation with multi-scale max pooling CNN, applied to indoor vision, using depth information. Interesting paper! Fine results.  Question: how does that compare to multi-scale max pooling CNN for a previous award-winning application, namely, segmentation of neuronal membranes (Ciresan et al, NIPS 2012)?This work applies convolutional neural networks to the task of RGB-D indoor scene segmentation. The authors previously evaulated the same multi-scale conv net architecture on the data using only RGB information, this work demonstrates that for most segmentation classes providing depth information to the conv net increases performance.   The model simply adds depth as a separate channel to the existing RGB channels in a conv net. Depth has some unique properties e.g. infinity / missing values depending on the sensor. It would be nice to see some consideration or experiments on how to properly integrate depth data into the existing model.   The experiments demonstrate that a conv net using depth information is competitive on the datasets evaluated. However, it is surprising that the model leveraging depth is not better in all cases. Discussion on where the RGB-D model fails to outperform the RGB only model would be a great contribution to add. This is especially apparent in table 1. Does this suggest that depth isn't always useful, or that there could be better ways to leverage depth data?   Minor notes: 'modalityies' misspelled on page 1  Overall: - A straightforward application of conv nets to RGB-D data, yielding fairly good results - More discussion on why depth fails to improve performance compared to an RGB only model would strengthen the experimental findingsThis work builds on recent object-segmentation work by Farabet et al., by augmenting the pixel-processing pathways with ones that processes a depth map from a Kinect RGBD camera. This work seems to me a well-motivated and natural extension now that RGBD sensors are readily available.  The incremental value of the depth channel is not entirely clear from this paper. In principle, the depth information should be valuable. However, Table 1 shows that for the majority of object types, the network that ignores depth is actually more accurate.  Although the averages at the bottom of Table 1 show that depth-enhanced segmentation is slightly better, I suspect that if those averages included error bars (and they should), the difference would be insignificant.  In fact, all the accuracies in Table 1 should have error bars on them.  The comparisons with the work of Silberman et al. are more favorable to the proposed model, but again, the comparison would be strengthened by discussion of statistical confidence.  Qualitatively, I would have liked to see the ouput from the convolutional network of Farabet et al. without the depth channel, as a point of comparison in Figures 2 and 3. Without that point of comparison, Figures 2 and 3 are difficult to interpret as supporting evidence for the model using depth.  Pro(s) - establishes baseline RGBD results with convolutional networks     Con(s) - quantitative results lack confidence intervals - qualitative results missing important comparison to non-rgbd network",1,5024
"This paper introduces a new regularization technique based on inexpensive approximations to model averaging, similar to dropout. As with dropout, the training procedure involves stochasticity but the trained model uses a cheap approximation to the average over all possible models to make a prediction.  The paper includes empirical evidence that the model averaging effect is happening, and uses the method to improve on the state of the art for three datasets.  The method is simple and in principle, computationally inexpensive.  Two criticisms of this paper: -The result on CIFAR-10 was not in fact state of the art at the time of submission; it was just slightly worse than Snoek et al's result using Bayesian hyperparameter optimization. -I think it's worth mentioning that while this method is computationally inexpensive in principle, it is not necessarily easy to get a fast implementation in practice. i.e., people wishing to use this method must implement their own GPU kernel to do stochastic pooling, rather than using off-the-shelf implementations of convolution and basic tensor operations like indexing.  Otherwise, I think this is an excellent paper. My colleagues and I have made a slow implementation of the method and used it to reproduce the authors' MNIST results. The method works as advertised and is easy to use.The authors introduce a stochastic pooling method in the context of convolutional neural networks, which replaces the traditionally used average or max pooling operators. In the stochastic pooling a multinomial distribution is created from input activations and used to select the index of the activation to pass to the next layer of the network. On first read, this method resembled that of 'probabilistic max pooling' by Lee et. al in 'Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations', however the context and execution are different.  During testing, the authors employ a separate pooling function that is a weighted sum of the input activations and their corresponding probabilities that would be used for index selection during training. This pooling operator is speculated to work as a form of regularization through model averaging. The authors substantiate this claim with results averaging multiple samples at each pool of the stochastic architectures and visualizations of images obtained from reconstructions using deconvolutional networks.  Moreover, test set accuracies for this method are given for four relevant datasets where it appears stochastic pooling CNNs are able to achieve the best known performance on three. A good amount of detail has been provided allowing the reader to reproduce the results.  As the sampling scheme proposed may be combined with other regularization techniques, it will be exciting to see how multiple forms of regularization can contribute or degrade test accuracies.  Some minor comments follow:  - Mini-batch size for training is not mentioned.  - Fig. 2 could be clearer on first read, e.g. if boxes were drawn around (a,b,c), (e,f), and (g,h) to indicate they are operations on the same dataset.  - In Section 4.2 it is noted that stochastic pooling avoids over-fitting unlike averaging and max pooling, however in Fig. 3 it certainly appears that the average and max techniques are not severely over-fitting as in the typical network training case (with noticeable degradation in test set performance). However, the network does train to near zero error on the training set. It may be more accurate to state that stochastic pooling promotes better generalization yet additional training epochs may make the over-fitting argument clearer.  - Fig. 3 also suggests that additional training may improve the final reported test set error in the case of stochastic pooling. The reference to state-of-the-art performance on CIFAR-10 is no longer current.  - Section 4.8, sp 'proabilities'Regularization methods are critical for the successful applications of neural networks.  This work introduces a new dropout-inspired regularization method named stochastic pooling.  The method is simple, applicable applicable to convolutional neural networks with positive nonlinearites, and achieves good performance on several tasks.  A potentially severe issue is that the results are no longer state of the art, as maxout networks get better results.  But this does not strongly suggest that stochastic pooling is inferior to maxout, since the methods are different and can therefore be combined, and, more importantly, maxout networks may have used a more thorough architecture and hyperparameter search, which would explain their better performance.  The main problem with the paper is that the experiments are lacking in that there is no proper comparison to dropout.  While the results on CIFAR-10 are compared to the original dropout paper and result in an improvement, the paper does not report results for the remainder of the datasets with dropout and with the same architecture (if the architecture is not the same in all experiments, then performance differences could be caused by architecture differences).  It is thus possible that dropout would achieve nearly identical performance on these tasks if given the same architecture on MNIST, CIFAR-100, and SVHN.  What's more, when properly tweaked, dropout outperforms the results reported here on CIFAR-10 as reported in Snoek et al. [A] (sub 15% test error); and it is conceivable that Bayesian-optimized stochastic pooling would achieve worse results.   In addition to dropout, it is also interesting to compare to dropout that occurs before max-pooling.  This kind of dropout bears more resemblance to stochastic pooling, and may achieve results that  are similar (or better -- it cannot be ruled out).  Finally, a minor point.  The paper emphasizes the fact that stochastic pooling averages 4^N models while dropout averages 2^N models, where N is the number of units.  While true, this is not relevant, since both quantities are vast, and the performance differences between the two  methods will stem from other sources.   To conclude, the paper presented an interesting and elegant technique for preventing overfitting that may become widely used.  However, this paper does not convincingly demonstrate its superiority over dropout.   References  ---------- [A] Snoek, J. and Larochelle, H. and Adams, R.P., Practical Bayesian Optimization of Machine Learning Algorithms, NIPS 2012    Networks",0,5025
"The paper describes a Natural Gradient technique to train Boltzman machines.  This is essentially the approach of Amari et al (1992) where the Fisher information matrix is expressed in which the authors estimate the Fisher information matrix L with examples sampled from the model distribution using a MCMC approach with multiple chains.  The gradient g is estimated from minibatches, and the weight update x is obtained by solving Lx=g with an efficient truncated algorithm.  Doing so naively would be very costly because the matrix L is large.  The trick is to express L as the covariance of the Jacobian S with respect to the model distribution and take advantage of the linear nature of the sample average to estimate the product Lw in a manner than only requires the storage of the Jacobien for each sample.   This is a neat idea.  The empirical results are preliminary but show promise.  The proposed algorithm requires less iterations but more wall-clock time than SML.  Whether this is due to intrinsic properties of the algorithm or to deficiencies of the current implementation is not clear.This paper presents a natural gradient algorithm for deep Boltzmann machines. The authors must be commended for their extremely clear and succinct description of the natural gradient method in Section 2. This presentation is particularly useful because, indeed, many of the papers on information geometry are hard to follow. The derivations are also correct and sound. The derivations in the appendix are classical statistics results, but their addition is likely to improve readability of the paper.  The experiments show that the natural gradient approach does better than stochastic maximum likelihood when plotting estimated likelihood against epochs. However, per unit computation, the stochastic maximum likelihood method still does better.   I was not able to understand remark 4 about mini-batches. Why are more parallel chains needed? Why not simply use a single chain but have longer memory. I strongly think this part of the paper could be improved if the authors write down the pseudo-code for their algorithm. Another suggestion is to use automatic algorithm configuration to find the optimal hyper-parameters for each method, given that they are so close.  The trade-offs of second order versus first order optimization methods are well known in the deterministic case. There is is also some theoretical guidance for the stochastic case. I encourage the authors to look at the following papers for this:  A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets. N. Le Roux, M. Schmidt, F. Bach. NIPS, 2012.   Hybrid Deterministic-Stochastic Methods for Data Fitting. M. Friedlander, M. Schmidt. SISC, 2012.   'On the Use of Stochastic Hessian Information in Optimization Methods for Machine Learning' R. Byrd, G. Chin and W. Neveitt, J. Nocedal. SIAM J. on Optimization, vol 21, issue 3, pages 977-995 (2011).  'Sample Size Selection in Optimization Methods for Machine Learning' R. Byrd, G. Chin, J. Nocedal and Y. Wu. to appear in Mathematical Programming B (2012).  In practical terms, given that the methods are so close, how does the choice of implementation (GPUs, multi-cores, single machine) affect the comparison? Also, how data dependent are the results. I would be nice to gain a deeper understanding of the conditions under which the natural gradient might or might not work better than stochastic maximum likelihood when training Boltzmann machines.  Finally, I would like to point out a few typos to assist in improving the paper:  Page 1: litterature should be literature Section 2.2 cte should be const for consistency. Section 3: Avoid using x instead of grad_N in the linear equation for Lx=E(.)  This causes overloading. For consistency with the previous section, please use grad_N instead. Section 4: Add a space between MNIST and [7]. Appendix 5.1: State that the expectation is with respect to p_{	heta}(x). Appendix 5.2: The expectation with respect to q_	heta should be with respect to p_{	heta}(x) to ensure consistency of notation, and correctness in this case. References: References [8] and [9] appear to be duplicates of the same paper by J. Martens.This paper introduces a new gradient descent algorithm that combines is based on Hessian-free optimization, but replaces the approximate Hessian-vector product by an approximate Fisher information matrix-vector product. It is used to train a DBM, faster than the baseline algorithm in terms of epochs needed, but at the cost of a computational slowdown (about a factor 30).  The paper is well-written, the algorithm is novel, although not fundamentally so.   In terms of motivation, the new algorithm aims to attenuate the effect of ill-conditioned Hessians, however that claim is weakened by the fact that the experiments seem to still require the centering trick. Also, reproducibility would be improved with pseudocode (including all tricks used) was provided in the appendix (or a link to an open-source implementation, even better).   Other comments:  * Remove the phrase 'first principles', it is not applicable here.  * Is there a good reason to limit section 2.1 to a discrete and bounded domain X?  * I'm not a big fan of the naming a method whose essential ingredient is a metric 'Metric-free' (I know Martens did the same, but it's even less appropriate here).  * I doubt the derivation in appendix 5.1 is a new result, could be omitted.  * Hyper-parameter tuning is over a small ad-hoc set, and finally chosen values are not reported.  * Results should be averaged over multiple runs, and error-bars given.  * The authors could clarify how the algorithm complexity scales with problem dimension, and where the computational bottleneck lies, to help the reader judge its promise beyond the current results.  * A pity that it took longer than 6 weeks for the promised 'next revision', I had hoped the authors might resolve some of the self-identified weaknesses in the meanwhile.",1,5026
"The paper introduces a new approach to supervised metric learning. The setting is somewhat similar to the information-theoretic approach of Davis et al. (2007). The main difference is that here the parameterized Mahalanobis distance is tuned by optimizing a new information-theoretical criterion, based on a matrix functional inspired by Renyi's entropy. Eqs. (5), (11) and (19) and their explanations are basically enough to grasp the basic idea. In order to reach the above goal, several mathematical technicalities are necessary and well developed in the paper. A key tool are infinitely divisible matrices.    + New criterion for information-theoretic learning   + The mathematical development is sound +/- The Renyi-inspired functional could be useful in other contexts     (but details remain unanswered in the paper)    - The presentation is very technical and goes bottom-up making it     difficult to get the 'big picture' (which is not too complicated)     until Section 4 (also it's not immediately clear which parts     convey the essential message of the paper and which parts are just     technical details, for example Section 2.1 could be safely moved     into an appendix mentioning the result when needed).    - Experiments show that the method works. I think this is almost     enough for a conference paper. Still, it would improve the paper     to see a clear direct comparison between this approach and     KL-divergence where the advantages outlined in the conclusions     (quote: 'The proposed quantities do not assume that the density of     the data has been estimated, which avoids the difficulties related     to it.') are really appreciated. Perhaps an experiment with     artificial data could be enough to complete this paper but real     world applications would be nice to see in the future.  Minor:  Section 2. Some undefined symbols: $M_n$, $sigma(A)$ (spectrum of A?)  Page 3: I think you mean     'where $n_1$ of the entries are 1$ -> $where $n_1$ of the entries of $mathbf{1}$ are 1$This paper introduces new entropy-like quantities on positive semi definite matrices. These quantities can be directly calculated from the Gram matrix of the data, and they do not require density estimation. This is an attractive property, because density estimation can be difficult in many cases. Based on this theory, the authors propose a supervised metric learning algorithm which achieves competitive results.  Pros: The problem studied in the paper is interesting and important. The empirical results are promising.  Cons:  i) Although I believe that there are many great ideas in the paper, in my opinion the presentation of the paper needs significant improvement. It is very difficult to asses what exactly the novel contributions are in the paper, because the authors didn't separate their new results well enough from the existing results. For example, Section 3 is about infinitely divisible matrices, but I don't know what exactly the new results are in this section.   ii) The introduction and motivation could be improved as well. The main message and its importance is a bit vague to me. I recommend revising Section 1. The main motivation to design new entropy like quantities was that density estimation is difficult and we might need lots of sample points to get satisfactory results. That's true that the proposed approach doesn't require density estimation, but it is still not clear if the proposed approach works better than those algorithms that use density estimators.  The empirical results seem very promising, so maybe I would emphasize them more.  iii) There are a few places in the text where the presented idea is simple, but it is presented in a complicated way and therefore it is difficult to understand. For example Section 4.1 and 4.2 seem more difficult than they should be. The definition of function F is not clear either.  iv) There are  a few typos and grammatical mistakes in the paper that also need to be fixed before publication. For example, on Page 1: Page 1: know as --> known as Page 1: jlearningThis paper proposes a new type of information measure for positive semidefinte matrices, which is essentially the logarithm of the sum of powers of eigenvalues. Several entropy-like properties are shown based on properties of spectral functions. A notion of joint entropy is then defined through Hadamard products, which leads to conditional entropies.  The newly defined conditional entropy is finally applied to metric learning, leading naturally to a gradient descent procedure. Experiments show that the performance of the new procedure exceeds the state of the art (e.g., LMNN).  I did not understand the part on infinitely divisible matrices and why Theorem 3.1 leads to a link with maximum entropy.  To the best of my knowledge, the ideas proposed in the paper are novel. I like the approach of trying to defining measures that have similar properties than entropies without the computational burden of computing densities. However, I would have like more discussion of the effect of alpha (e.g., why alpha=1.01 in experiments? does it make a big difference to change alpha? what does it corresponds to for alpha =2, in particular in relation fo HSIC?).  Pros: -New information measure with attractive properties -Simple algorithm for metric learning  Cons: -Lack of comparison with NCA which is another non-convex approach (J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov. (2005) Neighbourhood Component Analysis. Advances in Neural Information Processing Systems. 17, 513-520. -Too little discussion on the choice of alpha",1,5027
"Many unsupervised representation-learning algorithms are based on minimizing reconstruction error. This paper aims at addressing the important questions around what these training criteria actually learn about the input density.   The paper makes two main contributions: it first makes a link between denoising autoencoders (DAE) and contractive autoencoders (CAE), showing that the DAE with very small Gaussian corruption and squared error is actually a particular kind of CAE (Theorem 1). Then, in the context of the contractive training criteria, it answers the question 'what does an auto-encoder learn about the data-generating distribution': it estimates both the first and second derivatives of the log-data generating density (Theorem 2) as well as other various local properties of this log-density. An important aspect of this work is that, compared to previous work that linked DAEs to score matching, the results in this paper do not require the reconstruction function of the AE to correspond to the score function of a density, making these results more general.  Positive aspects of the paper:   * A pretty theoretical paper (for representation learning) but well presented in that most of the heavy math is in the appendix and the main text nicely presents the key results   * Following the theorems, I like the way in which the various assumptions (perfect world scenario) are gradually pulled away to show what can still be learned about the data-generating distribution; in particular, the simple numerical example (which could be easily re-implemented) is a nice way to connect the abstractness of the result to something concrete  Negative aspects of the paper:   * Since the results heavily rely on derivatives with respect to the data, they only apply to continous data (extensions to discrete data are mentioned as future work)  Comments, Questions --------  It's interesting that in the classical CAE, there is an implicit contractive effect on g() via the side effect of tying the weights whereas in the form of the DAE presented, g() is explicitly made contractive via r(). Have you investigated the effective difference?  Minor comments, typos, etc -------------------------- Fig 2 - green is not really green, it's more like turquoise       - 'high-capcity' -> 'high-capacity'       - the figure makes reference to lambda but at this point in the paper, lambda is yet to be defined  objective function for L_DAE (top of p4) - last term o() coming from the Taylor expansion is explicitly discussed in appendix (and perhaps obvious here) but is not explicitly defined in the main text  Right before 3.2.4 'high dimensional <data> (such as images)'   Although in the caption, you mention the difference between upper/lower and left/right subplots in Fig 4, I would prefer those (model 1/model 2) to be labeled directly on the subplots, it would just make for easier parsing.This paper shows that we can relate the solution of specific autoencoder to the data generating distribution. Specifically solving for general reconstruction function with regularizer that is the L2 penalty of reconstruction contraction relates the reconstruction function derivative of the data probability log likelihood. This is in the limit of small regularization. The paper also shows that in the limit of small penalty this autoencoder is equivalent to denoising autoencoder with small noise.  Section 3.2.3: You get similar attractive behavior using almost any autoencoder with limited capacity. The point of your work is that with the specific form of regularization - square norm of contraction of r - the r(x)-x relates to derivative of log probability (proof seem to require it - it would be interesting to know what can be said about other regularizers). It would be good to compare these plots with other regularizers and show that getting log(p) for contractive one is somehow advantageous. Otherwise this section doesn't support this paper in any way.  As authors point out, it would be good to know something not in the limit of penalty going to zero. At least have some numerical experiments, for example in 1d or 2d.   Figure 4. - 'Top plots are for one model and bottom plots for another' - what are the two models? It would be good to specify this in the figure, e.g. denosing autoencoders with different initial conditions and parameter settings.  Section 3.2.5 is important and should be written a little more clearly.   I would suggest deriving (13) in the appendix directly from (11) without having the reader recall or read about Euler-Lagrange equations, and it might actually turn out to be simpler. Differentiating the first term with r(x) gives r(x)-x. For the second term one moves the derivative to the other size using integration by parts (and droping the boundary term) and then just applying it to the product p(x)dr/dx resulting in (13).  Minor - twice you say in the appending that the proof is in the appendinx (e.g. after statement of theorem 1)  The second last sentence in the abstract is uncomfortable to read.  This is probably not important, but can we assume that r given by (11) actually has a taylor expansion in lambda? (probably, but in the spirit of prooving things).  You don't actually derive formulas the second moments in the appendix like you do for the first moment, you mean they can similarly be derived?The paper presents a method to analyse how and what the auto-encoder models that use reconstruction error together with a regularisation cost, are learning with respect to the underlying data distribution. The paper focuses on contractive auto-encoder models and also reformulates denoising auto-encoder as a form of contractive auto-encoder where the contraction is achieved through regularisation of the derivative of reconstruction error wrt to the input data. The rest of the paper presents a theoretical analysis of this form of auto-encoders and also provides couple of toy examples showing empirical support.  The paper is easy to read and the theoretical analysis is nicely split between the main paper and appendices. The details in the main paper are sufficient for the reader to understand the concept that is presented in the paper.  The theory and empirical data show that one can recover the true data distribution if using contractive auto-encoders of the given type. I think this is quite an important result. even though limited to this specific type of model, quantitative analysis of generative capabilities of auto-encoders have been limited.  I find the experiment shown in Figure 4 somewhat confusing. The text suggests that the only difference between the two models is their initial conditions and optimisation hyper parameters. Is the main reason due to initial conditions or hyper parameters? Which hyper parameters? Is the difference in initial condition just a different random seed or different type of initialisation of the network? I think this requires more in depth explanation. Is it normal to expect such different solutions depending on initial conditions?  Section 3.2.4. I am not clear what is the importance of this section. It seems to state the relationship between the score and reconstruction derivative.  Is it possible to link these results and theory to other forms of auto-encoders, such as sparse auto-encoders or with different type of non-linear activation functions? It would be very useful to have similar analysis for more general types of auto-encoders too.",1,5028
"The paper describe a compositional object models that take the form of a hierarchical generative models. Both object and part models provide (1) a set of part models, and (2) a generative model essentially describing how parts are composed.  A distinctive feature of this model is the ability to support 'part sharing' because the same part model can be used by multiple objects and/or in various points of the object hierarchical description. Recognition is then achieved with a Viterbi search. The central point of the paper is to show how part sharing provides opportunities to reduce the computational complexity of the search because computations can be reused.   This is analogous to the complexity gain of a large convolutional network over a sliding window recognizer of similar architecture. Although I am not surprised by this result, and although I do not see it as very novel, this paper gives a self-contained description of a sophisticated and conceptually sound object recognition system. Stressing the complexity reduction associated with part sharing is smart because the search complexity became a central issue in computer vision. On the other hand, the unsupervised learning of the part decomposition is not described in this paper (reference [19]) and could have been relevant to ICLR.This paper presents a complexity analysis of certain inference algorithms for compositional models of images based on part sharing.  The intuition behind these models is that objects are composed of parts and that each of these parts can appear in many different objects;  with sensible parallels (not mentioned explicitly by the authors) to typical sampling sets in image compression and to renormalization concepts in physics via  model high-level executive summaries.  The construction of hierarchical part dictionaries is an important and in my appreciation challenging prerequisite, but this is not the subject of the paper.   The authors discuss an approach for object detection and object-position inference exploiting part sharing and dynamic programming,  and evaluate its serial and parallel complexity. The paper gathers interesting concepts and presents intuitively-sound theoretical results that could be of interest to the ICLR community.This paper explores how inference can be done in a part-sharing model and the computational cost of doing so. It relies on 'executive summaries' where each layer only holds approximate information about the layer below. The authors also study the computational complexity of this inference in various settings.  I must say I very much like this paper. It proposes a model which combines fast and approximate inference (approximate in the sense that the global description of the scene lacks details) with a slower and exact inference (in the sense that it allows exact inference of the parts of the model). Since I am not familiar with the literature, I cannot however judge the novelty of the work.  Pros: - model which attractively combines inference at the top level with inference at the lower levels - the analysis of the computational complexity for varying number of parts and objects is interesting  - the work is very conjectural but I'd rather see it acknowledged than hidden under toy experiments. Cons:    Part Sharing",0,5029
"Deep predictive coding networks  This paper introduces a new model which combines bottom-up, top-down, and temporal information to learning a generative model in an unsupervised fashion on videos. The model is formulated in terms of states, which carry temporal consistency information between time steps, and causes which are the latent variables inferred from the input image that attempt to explain what is in the image.  Pros: Somewhat interesting filters are learned in the second layer of the model, though these have been shown in prior work.  Noise reduction on the toy images seems reasonable.  Cons: The explanation of the model was overly complicated. After reading the the entire explanation it appears the model is simply doing sparse coding with ISTA alternating on the states and causes. The gradient for ISTA simply has the gradients for the overall cost function, just as in sparse coding but this cost function has some extra temporal terms.  The noise reduction is only on toy images and it is not obvious if this is what you would also get with sparse coding using larger patch sizes and high amounts of sparsity. The explanation of points between clusters coming from change in sequences should also appear in the clean video as well because as the text mentions the video changes as well. This is likely due to multiple objects overlapping instead and confusing the model.  Figure 1 should include the variable names because reading the text and consulting the figure is not very helpful currently.  It is hard to reason what each of the A,B, and C is doing without a picture of what they learn on typical data. The layer 1 features seem fairly complex and noisy for the first layer of an image model which typically learns gabor-like features.  Where did z come from in equation 11?  It is not at all obvious why the states should be temporally consistent and not the causes. The causes are pooled versions of the states and this should be more invariant to changes at the input between frames.  Novelty and Quality: The paper introduces a novel extension to hierarchical sparse coding method by incorporating temporal information at each layer of the model. The poor explanation of this relatively simple idea holds the paper back slightly.This paper attempts to capture both the temporal dynamics of signals and the contribution of top down connections for inference using a deep model.  The experimental results are qualitatively encouraging, and the model structure seems like a sensible direction to pursue.  I like the connection to dynamical systems.  The mathematical presentation is disorganized though, and it would have been nice to see some sort of benchmark or externally meaningful quantitative comparison in the experimental results.  More specific comments:  You should state the functional form for F and G!!  Working backwards from the energy function, it looks as if these are just linear functions?  In Eq. 1 should F( x_t, u_t ) instead just be F( x_t )?  Eqs. 3 and 4 suggest it should just be F( x_t ), and this would resolve points which I found confusing later in the paper.  The relationship between the energy functions in eqs. 3 and 4 is confusing to me.  (this may have to do with the (non?)-dependence of F on u_t)  Section 2.3.1, 'It is easy to show that this is equivalent to finding the mode of the distribution...': You probably mean MAP not mode.  Additionally this is non-obvious.  It seems like this would especially not be true after marginalizing out u_t.  You've never written the joint distributions over p(x_t, y_t, x_t-1), and the role of the different energy functions was unclear.  Section 3.1: In a linear mapping, how are 4 overlapping patches different from a single larger patch?  Section 3.2: Do you do anything about the discontinuities which would occur between the 100-frame sequences?A brief summary of the paper's contributions, in the context of prior work. The paper proposes a hierarchical sparse generative model in the context of a dynamical system. The model can capture temporal dependencies in time-varying data, and top-down information (from high-level contextual/causal units) can modulate the states and observations in lower layers.   Experiments were conducted on a natural video dataset, and on a synthetic video dataset with moving geometric shapes. On the natural video dataset, the learned receptive fields represent edge detectors in the first layer, and higher-level concepts such as corners and junctions in the second layer. In the synthetic sequence dataset, hierarchical top-down inference is used to robustly infer about “causal” units associated with object shapes.   An assessment of novelty and quality. This work can be viewed as a novel extension of hierarchical sparse coding to temporal data. Specifically, it is interesting to see how to incorporate dynamical systems into sparse hierarchical models (that alternate between state units and causal units), and how the model can perform bottom-up/top-down inference. The use of Nestrov’s method to approximate the non-smooth state transition terms in equation 5 is interesting.  The clarity of the paper needs to be improved. For example, it will be helpful to motivate more clearly about the specific formulation of the model (also, see comments below).   The experimental results (identifying high-level causes from corrupted temporal data) seem quite reasonable on the synthetic dataset. However, the results are all too qualitative. The empirical evaluation of the model could be strengthened by directly comparing the DPCN to related works on non-synthetic datasets.   Other questions and comments: - In the beginning of the section 2.1, please define P, D, K to improve clarity. - In section 2.2, little explanation about the pooling matrix B is given. Also, more explanations about equation 4 would be desirable. - What is z_{t} in Equation 11? - In Section 2.2, it’s not clear how u_hat is computed.    A list of pros and cons (reasons to accept/reject). Pros: - The formulation and the proposed solution are technically interesting.  - Experimental results on a synthetic video data set provide a proof-of-concept demonstration.  Cons: - The significance of the experiments is quite limited. There is no empirical comparison to other models on real tasks. - Inference seems to be complicated and computationally expensive.  - Unclear presentation",1,5030
"This papers show the effects of under-fitting in a neural network as the size of a single neural network layer increases. The overall model is composed of SIFT extraction, k-mean, and this single hidden layer neural network. The paper suggest that this under-fitting problem is due to optimization problems with stochastic gradient descent.  Pros For a certain configurations of network architecture the paper shows under-fitting remains as the number of hidden units increases.  Cons This paper makes many big assumptions: 1) that the training set of millions of images is labelled correctly. 2) training on sift features followed by kmeans retains enough information from the images in the training set to allow for proper learning to proceed. 3) a single hidden layer network is capable of completely fitting (or over-fitting) Imagenet.  While the idea seems novel, it does appear to be a little rushed. Perhaps more experimentation with larger models and directly on the input image would reveal more.The net gets bigger, yet keeps underfitting the training set. Authors suspect that gradient descent is the culprit. An interesting study!",1,5031
"summary: This is a 3-page abstract only. It proposes a low-dimensional representation of data in order to impose a tree structure. It relates to other mixed-norm approaches previously proposed in the literature. Experiments on a binarized MNIST show how it becomes robust to added noise.  review: I must say I found the abstract very hard to read and would have preferred a longer version to better understand how the model is different from prior work. It's not clear for instance how the proposed approach compares to other denoising methods. It's not clear neither what is the relation between tree-based decomposition and noise in MNIST. Finally, I didn't understand why the model was restricted to binary representations. All this simply says I failed to capture the essence of the proposed approach.The paper extends the widely known idea of tree-structured sparse coding to the Hamming space. Instead for each node being represented by the best linear fit of the corresponding sub-space, it is represented by the best sub-cube. The idea is valid if not extremely original.   I’m not sure it has too many applications, though. I think it is more frequent to encounter raw data residing some Euclidean space, while using the Hamming space for representation (e.g., as in various similarity-preserving hashing techniques). Hence, I believe a more interesting setting would be to have W in R^d, while keeping Z in H^K, i.e., the dictionary atoms are real vectors producing best linear fit of corresponding clusters with binary activation coefficients. This will lead to the construction of a hash function. The out-of-sample extension would happen naturally through representation pursuit (which will now be performed over the cube).  Pros:  1.	A very simple and easy to implement idea extending tree dictionaries to binary data 2.	For binary data, it seems to outperform other algorithms in the presented recovery experiment.   Cons:  1.	The paper reads more like a preliminary writeup rather than a real paper. The length might be proportional to its contribution, but fixing typos and putting a conclusion section wouldn’t harm. 2.	The experimental result is convincing, but it’s rather andecdotal. I might miss something, but the author should argue convincingly that representing binary data with sparse tree-structured dictionary is interesting at all, showing a few real applications. The presented experiment on binarized MNIST digit is very artificial.",0,5032
"The authors aim to introduce a new method for training deep Boltzmann machines. Inspired by inference procedure they turn the model into two hidden layers autoencoder with recurrent connections. Instead of reconstructing all pixels from all (perhaps corrupted) pixels they reconstruct one subset of pixels from the other (the complement).  Overall this paper is too preliminary and there are too few experiments and most pieces are not new. However with better analysis and experimentation this might turn out to be very good architecture, but at this point is hard to tell.  The impainting objective is similar to denoising - one tries to recover original information from either subset of pixels or from corrupted image. So this is quite similar to denoising autoencoders. It is actually exactly the same as the NADE algorithm, which can be equivalently trained by the same criterion (reconstructing one set of pixels from the other - quite obvious) instead of going sequentially through pixels. The architecture is an autoencoder but a more complicated one then standard single layer - it has two (or more) hidden layers and is recurrent. In addition there is the label prediction cost. The idea of a more complicated encoding function, including recurrence, is interesting but certainly not new and neither is combining unsupervised and supervised criterion in one criterion. However if future exploration shows that this particular architecture is a good way of learning features, or that is specifically trains well the deep bolzmann machines, or it is good for some other problems then this work can be very interesting. However as presented, it needs more experiments.This breaking-news paper proposes a new method to jointly train the layers of a DBM. DBM are usually 'pre-trained' in a layer-wise manner using RBMs, a conceivably suboptimal procedure. Here the authors propose to use a deterministic criterion that basically turns the DBM into a RNN. This RNN is trained with a loss that resembles that one of denoising auto-encoders (some inputs at random are missing and the task is to predict their values from the observed ones).  The view of a DBM as special kind of RNN is not new and the inpainting criterion is not new either, however their combination is. I am very curious to see whether this will work because it may introduce a new way to train RNNs that can possibly work well for image related tasks. I am not too excited about seeing this as a way to improve DBMs as a probabilistic model, but that's just my personal opinion.   Overall this work can be moderately original and of good quality.  Pros -- clear motivation -- interesting model -- good potential to improve DBM/RNN training -- honest writing about method and its limitation (I really like this and it is so much unlike most of the work presented in the literature). Admitting current limitations of the work and being explicit about what is implemented helps the field making faster progress and becoming less obscure to outsiders.   Cons -- at this stage this work seems preliminary --  formulation is unclear  More detailed comments: The notation is a bit confusing: what's the difference between Q^*_i and Q^*? Is the KL divergence correct? I would expect something like: KL(DBM probability of (v_{S_i} | v_{-S_i}) || empirical probability of ( v_{S_i} | v_{-S_i}) ). I do not understand why P(h | v_{-S_i}) shows up there.  It would be nice to relate this method to denoising autoencoders. In my understanding this is the analogous for RNN-kind of networks.  Doesn't CG make the training procedure more prone to overfitting on the minibatch? How many steps are executed?  Important details are missing. Saying that error rate on MNIST is X% does not mean much if the size of the network is not given.  Overall, this is a good breaking news paper.",1,5033
"This is a followup paper for reference [1] which describes a parameter free adaptive method to set learning rates for SGD.  This submission cannot be read without first reading [1].  It expands the work in several directions: the impact of minibatches, the impact of sparsity and gradient orthonormality, and the use of finite difference techniques to approximate curvature. The proposed methods are justified with simple theoretical considerations under simplifying assumptions and with serious empirical studies. I believe that these results are useful.  On the other hand, an opportunity has been lost to write a more substantial self-contained paper. As it stands, the submission reads like three incremental contributions stappled together.summary: The paper proposes a new variant of stochastic gradient descent that is fully automated (no hyper-parameter to tune) and is robust to various scenarios, including mini-batches, sparsity, and non-smooth gradients. It relies on an adaptive learning rate that takes into account a moving average of the Hessian. The result is a single algorithm that takes about 4x memory (with respect to the size of the model) and is easy to implement. The algorithm is tested on purely artificial tasks, as a proof of concept.  review. - The paper relies on some previous algorithm (bbprop) that is not provided here and only explained briefly on page 5, while first used on page 2. It would have been nice to provide more information about it earlier.  - The 'parallelization trick' using mini-batches is good for a single-machine approach, where one can use multiple cores, but is thus limited by the number of cores. Also, how would this 'interfere' with Hogwild type of updates, which also uses efficiently multi-core approaches for SGD?  - Obviously, results on real large datasets would have been welcomed (I do think experiments on artificial datasets are very useful as well, but they may hide the fact that we have not fully understood the complexity of real datasets).This is a paper that builds up on the adaptive learning rate scheme proposed in [1], for choosing learning rate when optimizing a neural network.  The first result (eq. 3) is that of figuring out an optimal learning rate schedule for a given mini-batch size n (a very realistic scenario, when one cannot adapt the size of the mini-batch during training because of computational and architectural constraints).  The second interesting result is that of setting the learning rates in those cases where one has sparse gradients (rectified linear units etc) -- this results in an effective rescaling of the rates by the number of non-zero elements in a given minibatch.  The third nice result is the observation that in a sparse situation the gradient update directions are mostly orthogonal. Taking this intuition to the logical conclusion, the authors thus induce a re-weighing scheme that essentially encourages the gradient updates to be orthogonal to each other (by weighing them proportionally to 1/number of times they interfere with each other). While the authors claim that this can be computationally expensive generally speaking, for problems of realistic sizes (d is in the tens of millions and n is a few dozen examples), this can be quite interesting.  The final interesting result is that of adapting the curvature estimation to the fact that with the advent of rectified linear units we are often faced with optimizing non-smooth loss functions. The authors propose a method that is  based on finite differences (with some robustness improvements) and is vaguely similar to what is done in SGD-QN.  Generally this is a very well-written paper that proposes a few sensible and relatively easy to implement ideas for adaptive learning rate schemes. I expect researchers in the field to find these ideas valuable. One disappointing aspect of the paper is the lack of real-world results on things other than simulated (and known) loss functions.    non-smooth gradients",0,5034
"The paper introduces a natural extension to the nested Chinese Restaurant process, where the main limitation was that a single path for the tree (from the root to a leaf) is chosen for each individual document. In this work, a document specific tree is drawn (with associated switching probabilities) which is then used to generate words in the document. Consequently, the words can represent very different topics not necessarily associated with the same path in the tree.   Though the work is clearly interesting and important for the topic modeling community, the workshop paper could potentially be improved.  The main problem is clearly the length of the submission which does not provide any kind of details (less than 2 pages of content). Though additional information can be found in the cited arxiv paper, I think it would make sense to include in the workshop paper at least the comparison in terms of perplexity (showing that it substantially outperforms nCRP) and maybe some details on efficiency of inference.  Conversely, the page-long Figure 2 could be reduced or removed to fit the content.  Overall, the work is quite interesting and seems to be a perfect fit for the conference. Given that an extended version is publicly available, I do not think that the above comments are really important.  Pros: -- a natural extension of the previous model which achieves respectable results on standard benchmarks (though results are not included in the submissions) Cons: -- a little more information about the model and its performance could be included even in a 3-page workshop paper.This paper presents a novel variant of the NCRP process that overcomes the latter's main limitation, namely, that a document necessarily has to use topics from a specific path in the tree. This is accomplished by combining ideas from HDP with the NCRP process, where the entire nCRP tree is replicated for each document where a sample from each DP at each node of the original tree is used as a shared base distribution for each document's own DP.  The idea is novel and is an important contribution in the area of unsupervised large scale text modeling.  Although the paper is strong on novelty, it seems to be incomplete in terms of presenting any evidence that the model actually works and is better than the original NCRP model. Does it learn better topics than nCRP? Is the new model a better predictor of text? Does it produce a better hierararchy of topics than the original model? Does the better representation of documents translate into better performance on any extrinsic task? Without any preliminary answers to these questions, in my mind, the work is incomplete at best.",0,5035
"This paper is by the group that did the first large-scale speech recognition experiments on deep neural nets, and popularized the technique.  It contains various analysis and experiments relating to this setup.   Ultimately I was not really sure what was the main point of the paper.  There is some analysis of whether the network amplifies or reduces differences in inputs as we go through the layers; there are some experiments relating to features normalization techniques (such as VTLN) and how they interact with neural nets, and there were some experiments showing that the neural network does not do very well on narrowband data unless it has been trained on narrowband data in addition to wideband data; and also showing (by looking at the intermediate activations) that the network learns to be invariant to wideband/narrowband differences, if it is trained on both kinds of input.   Although the paper itself is kind of scattered, and I'm not really sure that it makes any major contributions, I would suggest the conference organizers to strongly consider accepting it, because unlike (I imagine) many of the other papers, it comes from a group who are applying these techniques to real world problems and is having considerable success.  I think their perspective would be valuable, and accepting it would send the message that this conference values serious, real-world applications, which I think would be a good thing.  -- Below are some suggestions for minor fixes to the paper.  eq. 4, prime ( ') missing after sigma on top right.  sec. 3.2, you do not explain the difference between average norm and maximum norm. What type of matrix norm do you mean, and what are the average and maximum taken over?  after 'narrowband input feature pairs', one of your subscripts needs to be changed.The paper presents an analysis of performance of DNN acoustic models in tasks where there is a mis-match between training and test data. Most of the results do not seem to be novel, and were published in several papers already. The paper is well written and mostly easy to follow.  Pros: Although there is nothing surprising in the paper, the study may motivate others to investigate DNNs.  Cons: Authors could have been more bold in ideas and experiments.  Comments:  Table 1: it would be more convincing to show L x N for variable L and N, such as N=4096, if one wants to prove that many (9) hidden layers are needed to achieve top performance (I'd expect that accuracy saturation would occur with less hidden layers, if N would increase); moreover, one can investigate architectures that would have the same number of parameters, but would be more shallow - for example, first and last hidden layers can have N=2048, and the hidden layer in between can have N=8192 - this would be more fair to show if one wants to claim that 9 hidden layers are better than 3 (as obviously, adding more parameters helps and the current comparison with 1-hidden layer NN is completely unfair as input and output layers have different dimensionality, but one can apply other tricks there to reduce complexity - for example hierarchical softmax in the output layer etc.)  'Note that the magnitude of the majority of the weights is typically very small' - note that this is also related to sizes of the hidden layers; if hidden layers were very small, the weights would be larger (output of neuron is non-linear function of weighted sum of inputs; if there are 2048 inputs that are in range (0,1), then we can naturally expect the weights to be very small)  Section 3 rather shows that neural networks are good at representing smooth functions, which is the opposite to what deep architectures were proposed for. Another reason to believe that 9 hidden layers are not needed.  The results where DNN models perform poorly on data that were not seen during training are not really striking or novel; it would be actually good if authors would try to overcome this problem in a novel way. For example, one can try to make DNNs more robust by allowing some kind of simple cheap adaptation during test time. When it comes to capturing VTLN / speaker characteristics, it would be interesting to use longer-context information, either through recurrence, or by using features derived from long contexts (such as previous 2-10 seconds).  Table 4 compares relative reductions of WER: however, note that 0% is not reachable on Switchboard. If we would assume that human performance is around 5-10% WER, then the difference in relative improvements would be significantly smaller. Also, it is very common that the better the baseline is, the harder it is to gain improvements (as many different techniques actually address the same problems).  Also, it is possible that DNNs can learn some weak VTLN, as they typically see longer context information; it would be interesting to see an experiment where DNN would be trained with limited context information (I would expect WER to increase, but also the relative gain from VTLN should increase).* Comments ** Summary    The paper uses examples from speech recognition to make the    following points about feature learning in deep neural networks:    1. Speech recognition performance improves with deeper networks,       but the gain per layer diminishes.    2. The internal representations in a trained deep network become       increasingly insensitive to small perturbations in the input       with depth.    3. Deep networks are unable to extrapolate to test samples that are       substantially different from the training samples.     The paper then shows that deep neural networks are able to learn    representations that are comparatively invariant to two important    sources of variability in speech:  speaker variability and    environmental distortions.  ** Pluses    - The work here is an important contribution because it comes from      the application of deep learning to real-world problems in speech      recognition, and it compares deep learning to classical      state-of-the-art approaches including discriminatively trained      GMM-HMM models, vocal tract length normalization, feature-space      maximum likelihood linear regression, noise-adaptive training,      and vector Taylor series compensation.    - In the machine learning community, the deep learning literature      has been dominated by computer vision applications.  It is good      to show applications in other domains that have different      characteristics.  For example, speech recognition is inherently a      structured classification problem, while many vision applications      are simple classification problems.  ** Minuses    - There is not a lot of new material here.  Most of the results      have been published elsewhere.  ** Recommendation    I'd like to see this paper accepted because    1. it makes important points about both the advantages and       limitations of current approaches to deep learning, illustrating       them with practical examples from speech recognition and       comparing deep learning against solid baselines; and    2. it brings speech recognition into the broader conversation on       deep learning.  * Minor Issues   - The first (unnumbered) equation is correct; however, I don't     think that viewing the internal layers as computing posterior     probabilities over hidden binary vectors provides any useful     insights.   - There is an error in the right hand side of the unnumbered     equation preceding Equation 4:  it should be sigma prime (the     derivative), not sigma.   - 'Senones' is jargon that is very specific to speech recognition     and may not be understood by a broader machine learning audience.   - The VTS acronym for vector Taylor series compensation is never     defined in the paper.  * Proofreading   the performance of the ASR systems -> the performance of ASR systems    By using the context-dependent deep neural network -> By using context-dependent deep neural network    the feature learning interpretations of DNNs -> the feature learning interpretation of DNNs    a DNN can interpreted as -> a DNN can be interpreted as    whose senone alignment label was generated -> whose HMM state alignment labels were generated    the deep models consistently outperforms the shallow -> the deep models consistently outperform the shallow    This is reflected in right column -> This is reflected in the right column    3.2 DNN learns more invariant features -> 3.2 DNNs learn more invariant features    is that DNN learns more invariant -> is that DNNs learn more invariant    since the differences needs to be -> since the differences need to be    that the small perturbations in the input -> that small perturbations in the input    with the central frequency of the first higher filter bank at 4 kHz -> with the center frequency of the first filter in the higher filter bank at 4 kHz    between p_y|x(s_j|x_wb) and p_y|x(s_j|x_nb -> between p_y|x(s_j|x_wb) and p_y|x(s_j|x_nb)    Note that the transform is applied before augmenting neighbor frames. -> Note that the transform is applied to individual frames, prior to concatentation.    demonstrated through a speech recognition experiments -> demonstrated through speech recognition experiments    Tasks",1,5036
"The authors propose a bipartite, undirected graphical model for multiview learning, called structure-adapting multiview harmonimum (SA-MVH). The model is based on their earlier model called multiview harmonium (MVH) (Kang&Choi, 2011) where hidden units were separated into a shared set and view-specific sets. Unlike MVH which explicitly restricts edges, the visible and hidden units in the proposed SA-MVH are fully connected to each other with switch parameters s_{kj} indicating how likely the j-th hidden unit corresponds to the k-th view.  It would have been better if the distribution of s_{kj}'s (or sigma(s_{kj})) was provided. Unless the distribution has clear modes near 0 and 1, it would be difficult to tell why this approach of learning w^{(k)}_{ij} and s_{kj} separately is better than just learning 	ilde{w}^{(k)}_{ij} = w^{(k)}_{ij} sigma s_{kj} all together (as in dual-wing harmonium, DWH). Though, the empirical results (experiment 2) show that the features extracted by SA-MVH outperform both MVH and DWH.  The visualizations of shared and view-specific features from the first experiment do not seem to clearly show the power of the proposed method. For instance, it's difficult to say that the filters of roman digits from the shared features do seem to have horizontal noise. It would be better to try some other tasks with the trained model. Would it be possible to sample clean digits (without horizontal or vertical noise) from the model if the view-speific features were forced off? Would it be possible to denoise the corrupted digits? and so on..  Typo:  - Fig. 1 (c): sigma(s_{1j}) and sigma(s_{2j})The paper introduces an new algorithm for simultaneously learning a hidden layer (latent representation) for multiple data views as well as automatically segmenting that hidden layer into shared and view-specific nodes. It builds on the previous multi-view harmonium (MVH) algorithm by adding (sigmoidal) switch parameters that turn a connection on or off between a view and hidden node and uses gradient descent to learn those switch parameters. The optimization is similar to MVH, with a slight modification on the joint distribution between views and hidden nodes, resulting in a change in the gradients for all parameters and a new switch variable to descend on.  This new algorithm, therefore, is somewhat novel; the quality of the explanation and writing is high; and the experimental quality is reasonable.  Pros  1. The paper is well-written and organized.  2. The algorithm in the paper proposes a way to avoid hand designing shared and private (view-specific) nodes, which is an important contribution.  3. The experimental results indicate some interesting properties of the algorithm, in particular demonstrating that the algorithm extracts reasonable shared and view-specific hidden nodes.  Cons 1. The descent directions have W and the switch parameters, s_kj, coupled, which might make learning slow. Experimental results should indicate computation time.  2. The results do not have error bars (in Table 1), so it is unclear if they are statistically significant (the small difference suggests that they may not be).  3. The motivation in this paper is to enable learning of the private and shared representations automatically. However, DWH (only a shared representation) actually seems to perform generally better that MVH (shared and private). The experiments should better explore this question. It might also be a good idea to have a baseline comparison with CCA.   4. In light of Con (3), the algorithm should also be compared to multi-view algorithms that learn only shared representations but do not require the size of the hidden-node set to be fixed (such as the recent relaxed-rank convex multi-view approach in 'Convex Multiview Subspace Learning', M. White, Y. Yu, X. Zhang and D. Schuurmans, NIPS 2012). In this case, the relaxed-rank regularizer does not fix the size of the hidden node set, but regularizes to set several hidden nodes to zero. This is similar to the approach proposed in this paper where a node is not used if the sigmoid value is < 0.5.  Note that these relaxed-rank approaches do not explicitly maximize the likelihood for an exponential family distribution; instead, they allow general Bregman divergences which have been shown to have a one-to-one correspondence with exponential family distributions (see 'Clustering with Bregman divergences' A. Banerjee, S. Merugu, I. Dhillon and J. Ghosh, JMLR 2005). Therefore, by selecting a certain Bregman divergence, the approach in this paper can be compared to the relaxed-rank approaches.    Harmoniums",1,5037
"Summary: this paper uses the common 2-step procedure to first eliminate most of unlikely detection windows (high recall), then use a network with higher capacity for better discrimination (high precision). Deep learning (in the unsupervised sense) helps having features optimized for each of these 2 different tasks, adapt them for different situations (different robotics grippers) and beat hand-designed features for detection of graspable areas, using a mixture of inputs (depth + rgb + xyz).  Novelty: deep learning for detection is not as uncommon as the authors suggest (pedestrians detection by [4] and imagenet 2012 detection challenge by Krizhevsky), however its application to robotics grasping detection is indeed novel. And detecting rotations (optimal grasping detection) while not completely novel is not extremely common.  Quality: the experiments are well conducted (e.g. proper 5-fold cross validation).  Pros: - Deep learning successfully demonstrated in a new domain. - Goes beyond the simpler task of classification. - Unsupervised learning itself clearly learns interesting 3D features of graspable areas versus non-graspable ones. - Demonstrates superior results to hand-coded features and automatic adaptability to different grippers. - The 2-pass shows improvements in quality and ~2x speedup.  Cons: - Even though networks are fairly small, the system is still far from realtime. Maybe explaining what the current bottlenecks are and further work would be interesting. Maybe you want to use convolutional networks to speed-up detection (no need to recompute each window's features, a lot of them are shared in a detection setting).This paper uses a two-pass detection mechanism with sparse autoencoders for robotic grasp detection, a new application of deep learning. The methods used are fairly standard by now (two pass and autoencoders), so the main novelty of the paper is its nice application. It shows good results, which are well presented and hold the promise of future extensions in this area.   The main issue I have with the paper is that it seems 'unfinished'; text wise I would have liked to see a proper conclusion and some more details on training; regarding its methods, I have the feeling this is work in its early stages.   pros: - novel and successful application - expert implementation of deep learning  cons: - 'unfinished' early work - this is an application paper, not a novel method (admittedly not necessarily a 'con')",1,5038
"The authors propose two log-linear language models for learning real-valued vector representations of words. The models are designed to be simple and fast and are shown to be scalable to very large datasets. The resulting word embeddings are evaluated on a number of novel word similarity tasks, on which they perform at least as well as the embeddings obtained using a much slower neural language model.  The paper is mostly clear and well executed. Its main contributions are a demonstration of scalability of the proposed models and a sensible protocol for evaluating word similarity information captured by such embeddings. The experimental section is convincing.  The log-linear language models proposed are not quite as novel or uniquely scalable as the paper seems to imply though. Models of this type were introduced in [R1] and further developed in [15] and [R2]. The idea of speeding up such models by eliminating matrix multiplication when combining the representations of context words was already implemented in [15] and [R2]. For example, the training complexity of the log-linear HLBL model from [15] is the same as that of the Continuous Bag-of-Words models. The authors should explain how the proposed log-linear models relate to the existing ones and in what ways they are superior. Note that Table 3 does contain a result obtained by an existing log-bilinear model, HLBL, which according to [18] was the model used to produce the 'Mhih NNLM' embeddings. These embeddings seem to perform considerably better then the 'Turian NNLM' embeddings obtained with a nonlinear NNLM on the same dataset, though of course not as well as the embeddings induced on much larger datasets. This result actually strengthens the authors argument for using log-linear models by suggesting that even if one could train a slow nonlinear model on the same amount of data it might not be worth it as it will not necessarily produce superior word representations.  The discussion of techniques for speeding up training of neural language models is incomplete, as the authors do not mention sampling-based approaches such as importance sampling [R3] and noise-contrastive estimation [R2].  The paper is unclear about the objective used for model selection. Was it a language-modeling objective (e.g. perplexity) or accuracy on the word similarity tasks?  In the interests of precision, it would be good to include the equations defining the models in the paper.  In Section 3, it might be clearer to say that the models are trained to 'predict' words, not 'classify' them.  Finally, in Table 3 'Mhih NNLM' should probably read 'Mnih NNLM'.  References: [R1] Mnih, A., & Hinton G. (2007). Three new graphical models for statistical language modelling. ICML 2007. [R2] Mnih, A., & Teh, Y. W. (2012). A fast and simple algorithm for training neural probabilistic language models. ICML 2012. [R3] Bengio, Y., & Senecal, J. S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713-722.The paper studies the problem of learning vector representations for words based on large text corpora using 'neural language models' (NLMs). These models learn a feature vector for each word in such a way, that the feature vector of the current word in a document can be predicted from the feature vectors of the words that precede (and/or succeed) that word. Whilst a number of studies have developed techniques to make the training of NLMs more efficient, scaling NLMs up to today's multi-billion-words text corpora is still a challenge.  The main contribution of the paper comprises two new NLM architectures that facilitate training on massive data sets. The first model, CBOW, is essentially a standard feed-forward NLM without the intermediate projection layer (but with weight sharing + averaging  before applying the non-linearity in the hidden layer). The second model, skip-gram, comprises a collection of simple feed-forward nets that predict the presence of a preceding or succeeding word from the current word. The models are trained on a massive Google News corpus, and tested on a semantic and syntactic question-answering task. The results of these experiments look promising.  Whilst I think this line of research is interesting and the presented results look promising, I do have three main concerns with this paper:  (1) The choice for the proposed models (CBOW and skip-gram) are not clearly motivated. The authors' only motivation appears to be for computational reasons. However, the experiments do not convincingly show that this indeed leads to performance improvements on the task at hand. In particular, the 'vanilla' NLM implementation of the authors actually gives the best performance on the syntactic question-answering task. Faster training speed is mainly useful when you can throw more data at the model, and the model can effectively learn from this new data (as the authors argue themselves in the introduction). The experiments do not convincingly show that this happens. In addition, the comparisons with the models by Collobert-Weston, Turian, Mnih, Mikolov, and Huang appear to be unfair: these models were trained on much smaller corpora. A fair experiment would re-train the models on the same data to show that they learn slower (which is the authors' hypothesis), e.g., by showing learning curves or by showing a graph that shows performance as a function of training time.  (2) The description of the models that are developed is very minimal, making it hard to determine how different they are from, e.g., the models presented in [15]. It would be very helpful if the authors included some graphical representations and/or more mathematical details of their models. Given that the authors still almost have one page left, and that they use a lot of space for the (frankly, somewhat superfluous) equations for the number of parameters of each model, this should not be a problem.  (3) Throughout the paper, the authors assume that the computational complexity of learning is proportional to the number of parameters in the model. However, their experimental results show that this assumption is incorrect: doubling the number of parameters in the CBOW and skip-gram models only leads to a very modest increase in training time (see Table 4).   Detailed comments =============== - The paper contains numerous typos and small errors. Specifically, I noticed a lot of missing articles throughout the paper.  - 'For many tasks, the amount of … focus on more advanced techniques.' -> This appears to be a contradiction. If speech recognition performance is largely governed by the amount of data we have, than simply scaling up the basic techniques should help a lot!  - 'solutions were proposed for avoiding it' -> For avoiding what? Computation of the full output distribution over words of length V?  - 'multiple degrees of similarities' -> What is meant by degrees here? Different dimensions of similarity? (For instance, Fiat is like Ferrari because they're both Italian but unlike Ferrari because it's not a sports car.) Or different strengths of the similarity? (For instance, Denmark is more like Germany than like Spain.) What about the fact that semantic similarities are intransitive? (Tversky's famous example of the similarity between China and North Korea.)  - 'Moreover, we discuss hyper-parameter selection … millions of words in the vocabulary.' -> I fail to see the relation between hyperparameter selection and training speed. Moreover, the paper actually does not say anything about hyperparameter selection! It only states the initial learning rate is 0.025, and that is linearly decreased (but not how fast).  - Table 2: It appears that the performance of the CBOW model is still improving. How does it perform when D = 1000 or 2000? Why not make a learning curve here (plot performance as a function of D or of training time)?  - Table 3: Why is 'our NNLM' so much better than the other NNLMs? Just because it was trained on more data? What model is implemented by 'our NNLM' anyway?   - Tables 3 and 4: Why is the NNLM trained on 6 billion examples and the others on just 0.7 or 1.6 billion examples? The others should be faster, so easier to train on more data, right?  - It would be interesting if the authors could say something about how these models deal with intransitive semantic similarities, e.g., with the similarities between 'river', 'bank', and 'bailout'. People like Tversky have advocated against the use of semantic-space models like NLMs because they cannot appropriately model intransitive similarities.  - Instead of looking at binary question-answering performance, it may also be interesting to look whether a hitlist of answers contains the correct answer.  - The number of self-citations seems somewhat excessive.  - I tried to find reference [14] to see how it differs from the present paper, but I was not able to find it anywhere.This paper introduces a linear word vector learning model and shows that it performs better on a linear evaluation task than nonlinear models. While the new evaluation experiment is interesting the paper has too many issues in its current form.  One problem that has already been pointed out by the other reviewers is the lack of comparison and proper acknowledgment of previous models. The log-linear models have already been introduced by Mnih et al. and the averaging of context vectors (though of a larger context) has already been introduced by Huang et al. Both are cited but the model similarity is not mentioned.  The other main problem is that the evaluation metric clearly favors linear models since it checks for linear relationships. While it is an interesting finding that this holds for any of the models, this phenomenon does not necessarily need to lead to better performance. Other non-linear models may have encoded all this information too but not on a linear manifold. The whole new evaluation metric is just showing that linear models have more linear relationships. If this was combined with some performance increase on a real task then non-linearity for word vectors would have been convincingly questioned. Do these relationships hold for even simpler models like LSA or tf-idf vectors?  Introduction: Many very broad and general statements are made without any citations to back them up.  The motivation talks about how simpler bag of words models are not sufficient anymore to make significant progress... and then the rest of the paper introduces a simpler bag of words model and argues that it's better. The intro and the first paragraph of section 3 directly contradict themselves.  The other motivation that is mentioned is how useful for actual tasks word vectors can be. I agree but this is not shown. This paper would have been significantly stronger if the vectors from the proposed (not so new) model would have been compared on any of the standard evaluation metrics that have been used for these words. For instance: Turian et al used NER, Huang et al used human similarity judgments, the author himself used them for language modeling. Why not show improvements on any of these tasks?   LDA and LSA are missing citations.  Citation [14] which is in submission seems an important paper to back up some of the unsubstantiated claims of this paper but is not available.  The hidden layer in Collobert et al's word vectors is usually around 100, not between 500 to 1000 as the authors write.  Section 2.2 is impossible to follow for people not familiar with this line of work.  Section 4: Why cosine distance? A comparison with Euclidean distance would be interesting, or should all word vectors be length-normalized?  The problem with synonyms in the evaluation seems somewhat important but is ignored.  The authors claim that their evaluation metric 'should be positively correlated with' 'certain applications'. That's yet another unsubstantiated claim that could be made much stronger with showing such a correlation on the above mentioned tasks.  Mnih is misspelled in table 3.  The comparisons are lacking consistency. All the models are trained on different corpora and have different dimensionality. Looking at the top 3 previous models (Mikolov 2x and Huang) there seems to be a clear correlation between vector size and overall performance. If one wants to make a convincing argument that the presented models are better, it would be important to show that using the same corpus.  Given that the overall accuracy is around 50%, the examples in table 5 must have been manually selected? If not, it would be great to know how they were selected.",1,5039
"This paper proposes new algorithms to minimize the Non-negative Matrix Factorization (NMF) reconstruction error in Frobenius norm subject to additional sparseness constraints (NMFSC) as originally proposed by [R1]. The original method from [R1] to minimize the reconstruction error is a projected gradient descent. While in [R1] a geometrically inspired method is used to compute the projection onto the sparseness constraints, this paper proposes to use Lagrange multipliers instead. To solve the NMFSC problem, the authors propose to update the basis vectors one at a time (therefore their method is called Sequential Sparse NMF or SSNMF), while in ordinary NMF/NMFSC the entire matrix with the basis vectors is updated at once. Experiments are reported that show that SSNMF is one order of magnitude faster compared to the algorithm of [R1].  The paper may only propose more efficient algorithms to solve a known optimization problem instead of proposing new learnable representations, but the approach is interesting and the results are promising. There are however some major issues with the paper:  (1) The sparseness projection of [R1] is essentially a Euclidean projection onto the intersection of a scaled probabilistic simplex (L1 sphere intersected with positive cone) and the scaled unit sphere (in L2 norm). The method of [R1] to compute this projection is an alternating projection algorithm (similar to the Dykstra algorithm for convex sets). The method was proven correct by [R2], and additionally it was shown that the projection is unique almost everywhere. Therefore, the method of [R1] and Algorithm 2 of the paper (Sparse-opt) should almost always compute the same result. In the paper, however, the sparseness projection of [R1] is denoted the 'projection-heuristic' while Sparse-opt is called 'exact', and when the projection of [R1] is used in the SSNMF algorithm instead of Sparse-opt the reconstruction error is no more monotonically decreasing as optimization proceeds. As both projection algorithms should compute the same, the plot should be identical for them when using the same starting points. Section 5.2 of the paper should be enhanced to verify whether both algorithms actually compute the same result and to find the bug that causes this contradiction.  (2) The proposed Algorithm 2 can be considered a (non-trivial) extension of the projection onto a scaled probabilistic simplex as described by [R3] and is a valuable contribution. In the paper, there is however a bug in the execution (which may explain the discrepancies described in Issue (1)): There are no multipliers that enforce the entries of the projection to be non-negative, as would be required by Problem (5) in the paper. Analogously, in Algorithm 2 there is no check in the loop of Line 2 to guarantee the values for lambda and mu produce a feasible (that is non-negative) solution. I implemented the algorithm in Matlab and compared it to the sparseness projection of [R1] (which is freely available on the author's homepage). In the algorithm as given in the paper, p_star always equals m after line 3 and no correct solution to Problem (5) is found in general. If I add the check for a feasible solution, both Sparse-opt and the sparseness projection of [R1] compute numerically equal results. I first suspected there was a typo in the manuscript, but that still would not explain the contradictory results from Section 5.2 of the paper.  On the positive side, I did check the expressions for lambda, mu and obj as given in Algorithm 2, and found them correct. Further, the algorithm is empirically faster than that of [R1], and its run-time is guaranteed theoretically to be at most quasilinear.  Based on the bugfix, I realized that the method from [R4] could be adapted to Sparse-opt to further enhance its run-time efficiency: Set p_star to m before the for loop of line 2 (in case all elements of the projection will be non-zero). Then, after computation of lambda and mu (obj does not need to be computed anymore with this modification), check if a_p < -mu(p) holds. If it does, set p_star to p - 1 and break the for loop. Line 3 of the algorithm should then be omitted. This modification fixes the algorithm, and additionally obj is not needed, and for lambda and mu simple scalars are sufficient to store at most two values of each.  (3) As noted by Paul Shearer and confirmed by the first author of the paper (see public comments), the proof of Theorem 3 is flawed as the arguments there would only apply if the sparseness constraints would induce a convex set (which they don't). I wouldn't have any objections if Theorem 3 and its proof were withdrawn and removed from the manuscript.  Moreover, I verified Algorithm 3 from the paper and found no obvious bugs. I implemented all algorithms and ran them on the ORL face data set and found that SSNMF computes a sparse representation. I did not check what happens without the bugfix for Algorithm 2, though. The authors should definitely fix the major issues and repeat the experiments before publication (judging from the run-time given in Figure 3 and Figure 4 this shouldn't take too long).  There are some minor issues too: - It should be briefly discussed whether SSNMF could benefit from a multi-threaded implementation as NMF/NMFSC do (in the experiments, the number of threads was set to one). - Figures should be enlarged and improved such that a difference between the plots is also noticeable when printed in black and white on letter size paper. - The references should be polished to achieve a consistent style (remove the URLs and ISSNs, don't use two different styles for JMLR ([7] and [10]) and NIPS ([6] and [17]), fix the page of [11], add volume and issue to [15] and [23], add the journal version of [21] unless that citation is withdrawn with Theorem 3, etc.). - Always cite using numbers in the main text ('cite{}') instead of using only the author names ('citeauthor{}') (e.g. Hoyer, Kim and Park, etc.), because now [9], [10] and [13], [14] could be confused. - The termination criteria should be described more elaborately for Algorithms 1, 3, and 4. - Page 2, just after Expression (1): This is only a convex combination if the rows of H are normed (wrt. L1), otherwise it's a conical combination. - Page 2, just after Expression (2): We use *subscripts* to denote... (missing s). Please also define what H_j^T would mean (is it (H_j)^T or (H^T)_j or something else?). - It would be nice to add line numbers to all algorithms (some have ones, some don't). - In Algorithm 3, Line 7: This should probably read G_j^T, as i is not defined here? - Mention the number of images for the sMRI data set in Section 5.1, and use ' ' or a footnote for the URL there. - Cite [7] in the third bullet point in Section 2.2.  References: [R1] Hoyer. Non-negative Matrix Factorization with Sparseness Constraints. JMLR, 2004, vol. 5, pp. 1457-1469. [R2] Theis et al. First results on uniqueness of sparse non-negative matrix factorization. EUSIPCO, 2005, vol. 3, pp. 1672-1675. [R3] Duchi et al. Efficient Projections onto the l1-Ball for Learning in High Dimensions. ICML, 2008, pp. 272-279. [R4] Chen & Ye. Projection Onto A Simplex. arXiv:1101.6081v2, 2011.Summary:  The paper presents a new optimization algorithm for solving NMF problems with the Euclidean norm as fitting cost and subject to sparsity constraints. The sparsity is imposed explicitly by adding an equality constraint to the optimization problem, imposing the sparsity measure proposed in [10] (referred as L1/L2 measure) of the columns of the matrix factors to be equal to a pre-defined constant. The contribution of this paper is to propose a more efficient optimization procedure for this problem. This is obtained mainly due to two variations on the original method introduced in [10]: (i) a block-coordinate descent strategy (ii) a fast algorithm for minimizing the subproblems involved in the obtained block coordinate scheme. Experimental evaluations show that the proposed algorithm runs substantially faster than previous works proposed in [7] and [10]. The paper is well written and the problem is clearly presented.  Pros:  - The paper presents an algorithm to solve an optimization problem that is significantly faster than available alternatives.  Cons:  - it is not clear why this particular formulation is better than other similar alternatives that can be efficiently optimized - the proposed approach seems limited to work with the L2 norm as fitting cost. - the convergence results for the block coordinate scheme is not applicable to the proposed algorithm  General comment:  1.  The measure used for sparsifying the NMF is an L1/L2 measure proposed in [10] (based on the relationship between the L1 and L2 norm of a given vector). The authors list interesting properties of this measure to justify its use and it seems a good option.  I understand that it is not the purpose of this paper to study or compare different regularizers. However, I believe that the authors should provide clear examples where this precise formulation (with the equality constraint) is better. Maybe even empirical evaluation (or a reference to a paper performing this study). Having a hard constrain in the sparsity level for every data code (or dictionary atom) seems too restrictive.  This is a very relevant issue, since explicitly imposing the sparsity constraint leads to a harder optimization problem with slower optimization algorithms (as explained by the authors). An important modeling advantage is required to justify the increase in complexity.  In the work:  Berry, M. W., et al. 'Algorithms and applications for approximate nonnegative matrix factorization.' Computational Statistics & Data Analysis 52.1 (2007): 155-173.  the authors adopt the sparsity measure form [10] but include it on a Lagrangian formulation. This implicit way of imposing sparsity can be combined with other fitting terms (e.g. beta divergences) and it is easier to optimize.  This was done with a very similar sparsity measure in the work:  V, Tuomas. 'Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria.' Audio, Speech, and Language Processing, IEEE Transactions on 15.3 (2007): 1066-1074.  The author proposes to add to the cost function a sparsity regularization term also of the form L1/L2 and was later used for audio source separation in:  W. Felix, J. Feliu, and B. Schuller. 'Supervised and semi-supervised suppression of background music in monaural speech recordings.' Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012.  2.  The strategy proposed in this paper alternatively fixes one matrix and minimizes over the other in a block coordinate fashion. In contrasts with [10], in which only a descent direction is searched. Maybe here there is another reason for the speed-up?  When the minimization is performed on a matrix factor that is subject to the sparsity constraint the authors employ a block coordinate descent strategy, referred to as sequential-pass. The authors empirically demonstrate that this strategy leads to a significant improvement in time.  The  minimization over each block (or columns) leads to a linear maximization problem subject to constraining the L1 and L2 norm to be constant, referred as sparse-opt. The authors propose an algorithm for exactly solving this subproblem. This problem also appears in [10] but in a slightly different way. In [10] the author proposes a heuristic method for projecting a vector to meet the sparsity constraints (sort of a proximal projection but to a non-convex set).  In Theorem 3, the authors present a convergence result for a relaxed version of the sequential-pass. Specifically, they relax the constraint on the L2 norm to be an inequality (instead of an equality). In this new setting, imposing the L1 norm to be constant no longer implies that the sparsity measure is constant. The quotient L1/L2 should be used instead, but this is no longer coincides with the sparse-opt problem.   Other minor comments:  - In Section 2.2 and later in Section 6, the authors refer to the properties that a good sparsity measure should have, according to [12]. I think that it would help the clarity of the presentation to briefly list these properties in Section 2.2 instead of defining them within the text of Section 6.   - In Section 3.1, the equation for the Lagrangian of the problem (5) should also include the term corresponding to the non-negativity constraint on y. This does not affect the derivation, since the multipliers for that constraint would be zero when the y_i are not, thus the obtained values of lambda, mu and obj would remain the same.This paper considers a dictionary learning algorithm for positive data. The sparse NMF approach imposes sparsity on the atoms, and positivity on both atoms and decomposition coefficients.   The formulation is standard, i.e., applying a contraint on the L1-norm and L2-norm of the atoms. The (limited) novelty comes in the optimization formulation: (a) with respect to the atoms, block coordinate descent is used with exact updates which are based on an exact projection into a set of constraints (this projection appears to be novel, though the derivation is standard), (b) with respect to the decomposition coefficients, multiplicative updates are used.  Running-time comparisons are done, showing that the new formulation outperforms some existing approaches (from 2004 and 2007).  Pros: -Clever use of the structure of the problem for algorithm design  Cons: -The algorithm is not compared to the state of the art (there has been some progress in sparse PCA and dictionary learning since 2007). In particular, the SPAMS toolbox of [19] allows sparse dictionary learning with positivity constraints. A comparison with this toolbox would help to assess the significance of the improvements. -Limited novelty.",1,5040
"* A brief summary of the paper's contributions, in the context of prior work.  This paper suggests an improvement over the LDA topic model with class labels of Fei-Fei and Perona [6], which consists in the incorporation of a prior that encourages the class conditional topic distributions to either be specific to a particular class or to be 'shared' across classes. Experiments suggest that this change to the original LDA model of [6] yields topics that are sharply divided into class-specified or shared topics and that are jointly more useful as a discriminative latent representation.  * An assessment of novelty and quality.  I like the motivation behind this work: designing models that explicitly try to separate the class-specific and class-invariant factors of variation is certainly an important goal and makes for a particularly appropriate topic at a conference on learning representations.  The novelty behind this paper is not great, since it adds a small component to a known model. But I wouldn't see this as a strong reason for not accepting this paper. There are, however, other issues which are more serious.  First, I find the mathematical description of the model to be imprecise. The main contribution of this work lies in the specification of a class-dependent prior over topics. It corresponds to the product of the regular prior from [6] and a new prior factor p(	heta | kappa), which as far as I know is not explicitly defined anywhere. The authors only describe how this prior affect learning, but since no explicit definition of p(	heta | kappa) is given, we can't verify that the learning algorithm is consistent with the definition of the prior. Given that the learning algorithm is somewhat complicated, involving some annealing process, I think a proper, explicit definition of the model is important, since it can't be derived easily from the learning algorithm.  I also find confusing that the authors refer to h(k) (Eq. 3) as an entropy. To be an entropy, it would need to involve a sum over k, not over c. Even the plate graphic representation of the new model is hard to understand, since 	heta is present in two separate plates (over M).  Finally, since there are other alternatives than [6] to supervised training of an LDA topic model, I think a comparison with these other alternatives would be in order. In particular, I'm thinking of the two following alternatives:  Supervised Topic Models, by Blei and McAuliffe, 2007 DiscLDA: Discriminative Learning for Dimensionality Reduction and Classiﬁcation, by Lacoste-Julien, Sha and Jordan, 2008  I think these alternatives should at least be discussed, and one should probably be added as a baseline in the experiments.   As a side comment (and not as a strong criticism of this paper), I'd like to add that I don't think the state of the art for scene classification (or object recognition in general) is actually based on LDA. My understanding is that approaches based on sparse coding + max pooling + linear SVM are better. I still think it's OK for some work to focus on improving a particular class of models. But at one point, perhaps a comparison to these other approaches should be considered.  * A list of pros and cons (reasons to accept/reject).  |Pros| - attacks an important problem, that of discovering and separating the factors of variation of a data distribution that are either class-dependent or class-shared, in the context of a topic model  |Cons| - somewhat incremental work - description of the model is not enough detailed - no comparison with alternative supervised LDA modelsThis paper presents an extension of Latent Dirichlet Allocation (LDA) which explicitly factors data into a structured noise part (varation shared among classes) and a signal part (variation within a specific class). The model is shown to outperform a baseline of LDA with class labels (Fei-Fei and Perona). The authors also show that the model can extract class-specific and class-shared variability by examining the learned topics.  The authors show that the new model can outperform standard LDA on classification tasks, however, it's not clear to me why one would necessarily use an LDA-based topic model (or topic models in general) if they're just interested in classification. In the introduction, the paper motivates the use of generative models (all well known reasons - learning from sparse data, handling missing observations, and providing estimates of data uncertainty, etc.) But none of these situations are explored in the experiments. So in the end, the paper shows that a model that it not really necessarily good at classification being improved but not to the point where it's better than discriminative models and not in a context of where a generative model would really be helpful.   Positive points:   * The method seems sound in its motivation and construction   * The model is shown to work on different modalities (text and images)   * The model outperforms classical LDA for classification  Negative points   * As per my comments above, there may be situations in which one would want to use this type of model for classification, but they haven't been explored in this paper   * The argument that the model produces sparser topic representations could be more convincing: in 4.3, the paper claims that the class-specific topic space effectively used for classification consists of 8 topics, where 12 topics are devoted to modeling structured noise; however the 12 noise topics still form part of the representation. Is the argument that the non-class topics would be thrown away after learning while the class-specific topics are retained and/or stored?  Specific comments:  In the third paragraph of the introduction, I'm not sure about choosing SIFT as the feature extraction step of this example of inference in generative models. I can't think of examples where SIFT has been used as part of a generative model -- it seems to be a classical feature extraction step for discriminative methods. Therefore, why not use an example of features that are learned generatively for this example?  In Section 2, the paper begins to talk about 'views' without really defining what is meant by a 'view'. Earlier, the paper discussed 'class-dependent' and 'class-independent' variance, and now 'view-dependent' and 'view-independent' variance - but they are not the same thing (though connected, as in the next paragraph the paper describes providing class labels as an additional 'view'). Perhaps it's best just to define up-front what's meant by a 'view'. If 'views' are just being used as a generalization of classes here in describing related work, just state that. Maybe the generalization for purposes of this discussion is not even necessary and the concept of 'view' just adds confusion.  Section 2: 'only the private class topics contain the relevant _____ for class inference'.   End of section 3: 'we associate *low*-entropy topics as class-dependent while *low*-entropy topics are considered as independent' ? (change second low to high)This paper introduces a new prior for topics in LDA to disentangle general variance and class specific variance.  The other reviews already mentioned the lack of novelty and some missing descriptions. Concretely, the definition of p(	heta | kappa), which is central to this paper, is not clear. Instead of defining these A(k) functions in figure 7, couldn't you just use a beta distribution as a prior?  In order to publish yet another variant of LDA, more comparisons are needed to the many other LDA-based models already published.  In particular, this paper tackles common issues that have been addresses many times by other authors. In order to have a convincing argument for introducing another LDA-like model, some form of comparison would be needed to a subset of these: - the supervised topic models of Blei et al.,  - DiscLDA from Lacoste,  - partially labeled LDA from Ramage et al.,  - Factorial LDA: Sparse Multi-Dimensional Text Models by Paul and Dredze, - Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model by Chemudugunta et al. - sparsity inducing topic models of  Chong Wang et al.   Unless the current model outperforms at least a couple of the above related models it is hard to argue for acceptance.",0,5041
"In [10], the authors had previously proposed modifying the network parametrization, in order to ensure zero-mean hidden unit activations across training examples (activity centering) and zero-mean derivatives (slope centering). This was achieved by introducing skip-connections between layers l-1 and l+1 and adding linear components to the non-linearity of layer l: these new parameters aren't learnt however, but instead are adjusted deterministically to enforce activity and slope centering.  These ideas had initially been proposed by Schraudolph in earlier work, with [10] showing that these tricks significantly improved convergence of deep networks while also making the connection to second order methods.  In this work, the authors proposed adding an extra scaling parameter to the  non-linearity, which is adjusted in order to make the digonal terms of the  Hessian / Fisher Information matrix closer to unity. The authors study the  effect of these 3 transformations by:  (1) measuring properties of the Hessian matrix with and without transformations, as well as angular distance of the resulting gradients to 2nd order gradients; (2) comparing the overall classification convergence speed for a 2 and 3 layer MLPs on MNIST and finally; (3) studying its effect on a deep auto-encoder.  While I find this research direction particularly interesting, I find the  overlap between this paper and [10] to be rather troubling. While their analysis of slope / activity centering is new (and a more direct test of their  hypothesis), I feel that the case for these transformations had already been made in [10]. More importantly, evidence for the 3rd transformation is rather weak: it seems to slightly help convergence of 3-layer models and also helps in making the diagonal elements of the Hessian more unimodal. However, including  gamma seem to rotate gradients *away* from 2nd order gradients. Also, their method did not seem to help in the deep auto-encoder setting: using gamma in the encoder network did not improve convergence speed, while using gamma in both encoders/decoders led to gamma either blowing-up or going to zero. While you would expect a diagonal approximation to a second-order method to help with the problem of dead-units, adding gamma did not seem to help in this respect.   Similarities between this paper and [10] are also evident in the writing itself. Large portions of Sections 1, 2 and 3 appear verbatim in [10]. This needs to be addressed prior to publication. The math of Section 3 could also be simplified by writing out gradients of log p (for each parameter 	heta) and then simply stating the general form of the FIM as E_eps[ dlogp/dtheta^T dlogp / dtheta]. As it stands Eqs. (12-17) are slightly inaccurate, as elements of the FIM should include an expectation over epsilon.  Summary: I find the direction promising but the conclusion to be somewhat confusing / disappointing. The premise for gamma seemed well motivated and I expected more concrete evidence explaining the need for this transformation. Unfortunately, I am left wondering where things went wrong: some missing theoretical insight, wrong update rule on gamma or other ?  Other: * Authors should consider using df/dx instead of the more ambiguous f' notation. * Could the authors clarify what they mean by: 'transforming the model instead of the gradient makes it easier to generalize to other contexts such as variational Bayes ?'  One downside I see to transforming the model instead of the gradients is that it obfuscates the link to second order methods and might thus hide useful insights. * Section 4: 'algorith' -> algorithm* A brief summary of the paper's contributions, in the context of prior work.  This paper extends the authors' previous work on making sure that the hidden units in a neural net have zero output and slope on average, by also using direct connections that model explicitly the linear dependencies. The extension introduces another transformation which changes the scale of the outputs of the hidden units: essentially, they try to normalize both the scale and the slope of the outputs to one. This is done (essentially) by introducing a regularization parameter that encourages the geometric mean of the scale and the slope to be one.  The paper's contributions are also to give a theoretical analysis of the effect of the proposed transformations. The already proposed tricks are shown to make the non-diagonal elements of the Fisher information matrix closer to zero. The new transformation makes the diagonal elements closer to each other in scale, which is interesting as it's similar to what natural gradient does.  The authors also provide an empirical analysis of how the proposed method is close to what a second-order method would do (albeit on a small neural net). The experiment with the angle between the gradient and the second-order update is quite nice (I think such an experiment should be part of any paper that proposes new optimization tricks for training neural nets).  * An assessment of novelty and quality.  Generally, this is a well-written and clear paper that extends naturally the authors' previous work. I think that the analysis is interesting and quite readable. I don't think that these particular transformations have been considered before in the literature and I like that they are not simply fixed transformations of the data, but something which integrates naturally into the learning algorithm.  * A list of pros and cons (reasons to accept/reject).  The proposed scaling transformation makes sense in theory, but I'm not sure I agree with the authors' statement (end of Section 5) that the method's complexity is 'minimal regularization' compared to dropouts (maybe in theory, but honestly implementing dropout in a neural net learning system is considerably easier). The paper also doesn't show significant improvements (beyond analytical ones) over the previous transformations; based on the empirical results only I wouldn't necessarily use the scaling transformation.This paper builds on previous work by the same authors that looks at performing dynamic reparameterizations of neural networks to improve training efficiency.  The previously published approach is augmented with an additional parameter (gamma) which, although it is argued should help in theory, doesn't seem to in practice.  Theoretical arguments for why the standard gradient computed under this reparameterization will be closer to a 2nd-order update are made, and experiments are conducted.   While the theoretical arguments are pretty weak in my opinion (see detailed comments below), the experiments that looks at eigenvalues of the Hessian are somewhat more convincing, although they indicate that the originally published approach, without the gamma modification, is doing a better job.   Pros: - reasonably well written - experiments looking at eigenvalue distributions are interesting  Cons: - actual method is similar to authors' previous work in [10] and the older method of Schraudolph [12] - the new modification doesn't seem to improve training efficiency, and even makes the eigenvalue distribution worse - there seem to be problems with the theoretical analysis (maybe the authors can address this in their response?)   ///// Detailed comments \\  Because it sounds similar to what you're doing, I think it would be helpful to give a slightly more detailed description of Schraudolph's 'gradient factor centering'.   Does it correspond exactly to what you are doing in the case of neural nets?  And if so, could you give an interesting example of how to apply your method to other models where Schraudolph's method would no longer apply?   I don't understand what you mean by 'many competing paths' at the bottom of page 2.    And when talking about 'linear dependencies' from x to y, what exactly do you mean?  Do you mean the 1st-order components of the Taylor series of the true mapping or something else?   Also, you might want to use affine when discussing functions that are linear + constant to be more technically precise.  Can the arguments in section 3 be applied to network with more than 1 hidden layer?  A concern I have with the analysis in section 3 is that, while assuming uncorrelated hidden unit outputs might be somewhat sensible (although I feel that our intuitions about how neural networks model certain mappings - such as 'representing different things' may be inaccurate), it seems less reasonable to assume that inputs (x) are uncorrelated with the outputs of the units, which seems to be needed to show that off-diagonal terms are zero (other than for eqn 12).   You also seem to assume that certain 1st-derivatives of unit outputs are uncorrelated with various quantities (inputs, other unit outputs, and unit derivatives), which I don't think follows from the assumptions about the outputs of the units being uncorrelated with each other (but if this is indeed true, you should prove it or provide a reference).  I think you should apply more rigor to these arguments for them to be convincing.  I would recommend using an exact method to compute the Hessian.  For example, you can compute it using n matrix-vector products, and tools for computing these automatically for any computational graph are widely available, as are particular formulae for neural networks.  Such a method would be no more costly than what you are doing now, which involves n gradient computations.  The discussion surrounding equation 19 is an somewhat inaccurate and oversimplified account of the role that a constant like mu has in a second-order update rule like eqn. 19.  This is a well studied and highly complex problem which doesn't really have to do with issues surrounding the inversion of the Hessian 'blowing up' so much as the problems of break-downs in model trust that occur when computing proposals based on local quadratic models of the objective.   Your experiments seem to suggest that the eigenvalues are more even when you leave out the gamma parameter.  How do you reconcile this with your theoretical analysis?  Why do you show a histogram of diagonal elements as opposed to eigenvalues in figure 2?  I would argue that the concentration of the eigenvalues is a much better indicator of how close the Hessian matrix is to the identity (and hence how close the gradient is to being the same as a 2nd-order update) than what the diagonal entries look like.  The diagonal entries of a highly non-diagonal matrix aren't particularly meaningful to look at.  Also, since your analysis was done using the Fisher, why not examine this matrix instead of the Hessian in your experiments?",1,5042
"rac{P(	ilde{v}|v)P(v)}{P(	ilde{v})}propto P(	ilde{v}|v)P(v)$.  Page 6: Some high-resolution natural image data sets (ImageNet or Berkeley Segmentation Benchmark) could be more proper than CIFAR-10 for this denoising task. Page 6: The authors should describe how to construct training set in detail.This paper is an empirical comparison of the different models (Boltzmann Machines and Denoising Autoencoders) on the task of image denoising. Based on the experiments the authors claimed the increasing model depth improves the denoising performances when the level of noise is high.  PROS + Exploring DBMs for images denosing is indeed interesting and important.  CONS - There is little novelty in this paper.  - The experiments could be not easily reproduced since some important details of the experimental setting are not provided (see below). - The proposed models were not compared with any state-of-the-art denoising method.  Detailed comments Page 4: The authors should explicitly specify how they constructed the matrix D. Page 4: There could be some  mistakes in equation 5. Please list the detailed derivation.  Page 4: Equation 5 is not a standard routine. You should firstly make an assumption about noise, for example, $	ilde{v}=v+n,nsim mathcal{N}(mu,,sigma^2)$. Then $P(	ilde{v}|v)=The paper conducts an empirical performance comparison, on the task of image denoising, where the denoising of large images is based on combining densoing of small patches. In this context, the study compares usign, as small patch denoisers, deep denoising autoencoders (DAE) versus deep Boltzmann machines with a Gaussian visible layer (GDBM, which correspond to GRBM for a single hidden layer). Compared to recent work on deep DAE for image denoising shown to be competitive with state-of-the-art methods (Burger et al. CVPR'2012; Xie et al. NIPS'2012) this work rather considers *blind* denoising tasks (test noise kind and level not the same as that used during training). For the DBM part, the work builds on the author's authors' GDBM (presented at NIPS 2011 workshop on deep learning), and performs denoising as the expectation of visibles given inferred expected first layer hidden obtained through varitional approximation.   The paper essentially draws the following observations  a) GRBM / GDBM can be equally successful at image denoising as deep DAEs,  b) increased depth seems to help denoising, particularly at higher noise levels.  c) interestingly a GRBM (single layer) appears often competitive compared to a GDBN with more layers (while deeper DAEs more systematically improve over single layer DAE).  Pros: + I find it is a worthy empirical comparison study to make. + it reasonably supports observation a), which is not too surprising (also there's no clear winner). + the observation I find most interesting, and worthy of further *digging*  is c) as it could be, as suggested by the authors, a concrete effect of the limitations of the variational approximation in the GDBN.  Cons: - empirical performance comparison of similar models, but does not yield much insight regarding wherefrom differences may arise (no other sensitivity analysis except final denoising perofrmance) - while I would a priori be inclined to believe in b), I find the methodology lacking here. It seems a single fixed hiddden layer size has been considered, the same for all layers, so that deeper networks had necessarily more parameters. Proper layer-sizes cross validation should be performed before we can hope to draw a scientific conclusion with respect to the benefit of depth.  - mathematical notation is often a little sloppy or buggy: Eq 4: if D is n x d as claimed Dx will be n x 1, so it cannot correspond to n 'patches' as claimed (unless your patches are but 1 pixel).  Eq 5: I belive last p(	ilde{v}|h) should be p(v|h) Next eq: p(v | h=mu) is an abuse since h are binary. In eq of hat{v}_i : p(h|	ilde{v}) is problematic, since there's no bound value for h. Shouldn't it rather be E(h|	ilde{v}) ?A brief summary of the paper's contributions, in the context of prior work. The paper proposed to use Gaussian deep Boltzmann machines (GDBM) for image denoising tasks, and it empirically compared the denoising performance to another state-of-the-art method based on stacked denoising autoencoders (Xie et al.). From empirical evaluations, the author confirms that deep learning models (DBM or DAE) achieve good performance in image denoising. Although DAE performs better than GDBM in many cases, GDBM can be still useful for image denoising since it doesn’t require prior knowledge on the types or levels of noise.   An assessment of novelty and quality. The main contribution of the paper is the use of Gaussian DBM for denoising. It also provides comparison against existing models (stacked denoising autoencoders). Although, technical novelty is limited, it is still interesting that GRBM without the knowledge of specific noise (in target tasks) can perform well for image denoising.   One major problem is that the paper fails to compare against a closely related work on robust Boltzmann machines (Tang et al., CVPR 2012), which is specifically designed for denoising tasks.   Conclusions drawn from empirical evaluation seem fairly reasonable, but not very surprising. Also, the results look somewhat random. More thorough analysis and better training might clean up the results and make the conclusion more convincing.   Other comments: How did you tune the hyperparameters (l2 regularization, learning rate, number of hidden nodes, etc.) of the model? The trained model is sensitive to these hyperparameters, so it should have been tuned to some validation task.   A list of pros and cons (reasons to accept/reject). pros: - Empirical evaluation of two deep models on image denoising tasks seems to confirm the usefulness of deep learning methods for image denoising. - It’s very interesting that models trained from natural images (CIFAR-10) work well for unrelated images.   cons: - The main contribution of the paper is the use of GRBM/DBM for denoising. However, it’s not clear whether GRBM/DBMs are better than DAE(4). - There is no comparison against robust Boltzmann machines (Tang et al., CVPR 2012).  - It would have been nice to make the results comparable to other published work (e.g., Xie et al.). The results in the paper raise questions about whether the authors faithfully implemented Xie et al.’s method.",1,5043
"This paper develops a new iterative optimization algorithm for performing non-negative matrix factorization, assuming a standard 'KL-divergence' objective function.  The method proposed combines the use of a traditional updating scheme ('multiplicative updates' from [1]) in the initial phase of optimization, with a diagonal Newton approach which is automatically switched to when it will help.  This switching is accomplished by always computing both updates and taking whichever is best, which will typically be MU at the start and the more rapidly converging (but less stable) Newton method towards the end.  Additionally, the diagonal Newton updates are made more stable using a few tricks, some of which are standard and some of which may not be.  It is found that this can provide speed-ups which may be mild or significant, depending on the application, versus a standard approach which only uses multiplicative updates.  As pointed out by the authors, Newton-type methods have been explored for non-negative matrix factorization before, but not for this particularly objective with a diagonal approximation (except perhaps [17]?).  The writing is rough in a few places but okay overall.  The experimental results seem satisfactory compared to the classical algorithm from [1], although comparisons to other potentially more recent approaches is conspicuously absent.  I'm not an experiment on matrix factorization or these particular datasets so it's hard for me to independently judge if these results are competitive with state of the art methods.  The paper doesn't seem particularly novel to me, but matrix factorization isn't a topic I find particularly interesting, so this probably biases me against the paper somewhat.    Pros: - reasonably well presented - empirical results seem okay Cons: - comparisons to more recent approaches is lacking - it's not clear that matrix factorization is a problem for which optimization speed is a primary concern (all of the experiments in the paper terminate after only a few minutes) - writing is rough in a few places    Detailed comments:  Using a KL-divergence objective seems strange to me since there aren't any distributions involved, just matrices, whose entries, while positive, need not sum to 1 along any row or column.  Are the entries of the matrices supposed to represent probabilities?  I understand that this is a formulation used in previous work ([1]), but it should be briefly explained.  You should explain the connection between your work and [17] more carefully.  Exactly how is it similar/different?  Has a diagonal Newton-type approach ever been used for the squared error objective?  'the smallest cost' -> 'leading to the greatest reduction in d_{KL}(V,Z)'  'the variables required to compute' -> 'the quantities required to compute'  You should avoid using two meanings of the word 'regularized' as this can lead to confusion.  Maybe 'damped' would work better to refer to the modifications made to the Newton updates that prevent divergence?  Have you compared to using damped/'regularized' Newton updates instead of your method of selecting the best between the Newton and MU updates?  In my experience, damping, along the lines of the LM algorithm or something similar, can help a great deal.  I would recommend using '	op' to denote matrix transposition instead of what you are doing.  Section 2 needs to be reorganized.  It's hard for me to follow what you are trying to say here.  First, you introduce some regularization terms.  Then, you derive a particular fixed-point update scheme.  When you say 'Minimizing [...] is achieved by alternative updates...' surely you mean that this is just one particular way it might be done.  Also, are these derivation prior work (e.g. from [1])?  If so, it should be stated.  It's hard to follow the derivations in this section.  You say you are applying the KKT conditions, but your derivation is strange and you seem to skip a bunch of steps and neglect to use explicit KKT multipliers (although the result seems correct based on my independent derivation).  But when you say: 'If h_r = 0, the partial derivative is positive. Hence the product of h_r and the partial derivative is always zero', I don't see how this is a correct logical implication.  Rather, the product is zero for any solution satisfying complementary slackness.  And I don't understand why it is particularly important that the sum over equation (6) is zero (which is how the normalization in eqn 10 is justified).  Surely this is only a (weak) necessary condition, but not a sufficient one, for a valid optimal solution.  Or is there some reason why this is sufficient (if so, please state it in the paper!).  I don't understand how the sentence on line 122 'Therefor...' is not a valid logical implication.  Did you actually mean to use the word 'therefor' here?  The lower bound is, however, correct.  'floor resp. ceiling'??Summary:  The paper presents a new algorithm for solving L1 regularized NMF problems in which the fitting term is the Kullback-Leiber divergence. The strategy combines the classic multiplicative updates with a diagonal approximation of Newton's method for solving the KKT conditions of the NMF optimization problem. This approximation results in a multiplicative update that is computationally light. Since the objective function might increase under the Newton updates, the author proposes to simultaneously compute both multiplicative and Newton updates and choose the one that produces the largest descent. The algorithm is tested on several datasets, generally producing improvements in both number of iterations and computational time with respect to the standard multiplicative updates.  I believe that the paper is well written. It proposes an efficient optimization algorithm for solving a problem that is not novel but very important in many applications. The author should highlight the strengths of the proposed approach and the differences with recent works presented in the literature.  Pros.:  - the paper addresses an important problem in matrix factorization, extensively used in audio processing applications - the experimental results show that the method is more efficient than the multiplicative algorithm (which is the most widely used optimization tool), without significantly increasing the algorithmic complexity  Cons:  - experimental comparisons against related approaches is missing - this approach seems limited to only work for the Kullback-Leiber divergence as fitting cost.   General comments:  I believe that the paper lacks of experimental comparisons with other accelerated optimization schemes for solving the same problem. In particular, I believe that the author should include comparisons with [17] and the work,  C.-J. Hsieh and I. S. Dhillon. Fast coordinate descent methods with variable selection for non-negative matrix factorization. In Proceedings of the 17th ACM SIGKDD, pages 1064–1072, 2011.  which should also be cited.  As the author points out, the approach in [17] is very similar to the one proposed in this paper (they have code available online).  The work by Hsieh and Dhillon is also very related to this paper. They propose a coordinate descent method using Newton's method to solve the individual one-variable sub-problems. More details on the differences with these two works should be provided in Section 1.  The experimental setting itself seems convincing. Figures 2 and 3 are never cited in the paper.Overview:  This paper proposes an element-wise (diagonal Hessian) Newton method to speed up convergence of the multiplicative update algorithm (MU) for NMF problems. Monotonic progress is guaranteed by an element-wise fall-back mechanism to MU. At a minimal computational overhead, this is shown to be effective in a number of experiments.   The paper is well-written, the experimental validation is convincing, and the author provides detailed pseudocode and a matlab implementation.   Comments:  There is a large body of related work outside of the NMF field that considers diagonal Hessian preconditioning of updates, going back (at least) as early as Becker & LeCun in 1988.  Switching between EM and Newton update (using whichever is best, element-wise) is an interesting alternative to more classical forms of line search: it may be worth doing a more detailed comparison to such established techniques.  I would appreciate a discussion of the potential of extending the idea to non KL-divergence costs.",0,5044
"Stochastic neighbour embedding (SNE) is a sound, probabilistic method for dimensionality reduction. One of its limitations is that  its complexity is O(N^2), where N is the, typically large, number of data points. To surmount this limitation, the this paper proposes computational methods to reduce the computational cost to O(NlogN), while only incurring an O(N) memory cost.  In the SNE variant discussed in this paper, the kernel in high dimensions is Gaussian, while the similarity in low dimensions is governed by a t-distribution. The proposed method consists of two components. First, the exponential decay of Gaussian measures is used to carry out truncation and construct a vantage-point tree for the data in high dimensions. This enables the authors to carry our nearest neighbour search in O(NlogN). The second component addreses the efficient computation of the gradient of SNE. Here, the paper proposes a 2D Barnes-hut algorithm to approximate the gradient in O(NlogN) steps. The Barnes-Hut algorithm is a well known method in N-body simulation, but it has not been used in this context previously to the best of my knowledge.  The paper is very well written. The contribution is correct and sound. Not surprisingly, the experiments show great improvements in computational performance, thus allowing for a good dimensionality reduction technique to become more broadly applicable.  The author ought to be commended for making the code available. He should also be commended for making the limitations of the approach very clear in the concluding remarks, namely that the current version is only for 2D-embeddings and that the method does not offer a way of controlling the error (e.g. via error bounds).  Minor typo in Page 2 last line: to slow should be too slow.  I believe the paper makes a good contribution. However, it has one crucial shortcoming that must be addressed by the author.  Specifically, there is a great body of literature on N-body methods for machine learning problems that the author does not seem to be aware of. I think this work should be placed in this context and that appropriate references and comparisons (for which I will point the author to online software) should be included in the final form in this paper. The relevant work includes:  1. All the dual-tree approximations developed by Alex Gray at http://www.fast-lab.org/ In particular note that his methods apply to nearest neighbour search and the type of kernel density estimates required in the computation of the gradient. Dual trees also allow for the use of error bounds. For publications, see e.g. Gray, Alexander G., and Andrew W. Moore. 'N-Body'problems in statistical learning.' Advances in neural information processing systems (2001): 521-527. Liu, Ting, Andrew W. Moore, Alexander Gray, and Ke Yang. 'An investigation of practical approximate nearest neighbor algorithms.' Advances in neural information processing systems 17 (2004): 825-832.  2. The multipole methods developed in Ramani Duraiswami lab, including: Yang, Changjiang, Ramani Duraiswami, Nail A. Gumerov, and Larry Davis. 'Improved fast gauss transform and efficient kernel density estimation.' In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, pp. 664-671. IEEE, 2003.  3. The algorithms for fast kernel density estimates from Nando de Freitas' lab. See e.g., Mahdaviani, Maryam, Nando de Freitas, Bob Fraser, and Firas Hamze. 'Fast computational methods for visually guided robots.' In IEEE International Conference on Robotics and Automation, vol. 1, p. 138. IEEE; 1999, 2005. Lang, Dustin, Mike Klaas, and Nando de Freitas. 'Empirical testing of fast kernel density estimation algorithms.' UBC Technical repor 2 (2005). One of his papers does, in fact, discuss multipole methods for SNE and presents results using the fast Gauss transform: De Freitas, Nando, Yang Wang, Maryam Mahdaviani, and Dustin Lang. 'Fast Krylov methods for N-body learning.' Advances in neural information processing systems 18 (2006): 251. The code is available here: http://www.cs.ubc.ca/~awll/nbody_methods.html  4. The cover tree for nearest neighbour search, introduced in: Beygelzimer, Alina, Sham Kakade, and John Langford. 'Cover trees for nearest neighbor.' In MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-, vol. 23, p. 97. 2006. For code, see the Wikipedia entry: http://en.wikipedia.org/wiki/Cover_tree  5. FLANN - Fast Library for Approximate Nearest Neighbors developed by Marius Muja. This is a powerful library of methods including randomized kd-trees and k-means methods for fast nearest neighbour search. It is extremely popular in computer vision. For code and more info see: http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN You could use this code easily to replace the nearest neighbour search and compare performance.  Finally, there is something very interesting in this paper that is worth studying further. Assume we use an N-body method in the computation of the gradient, which has error bounds. Then, it seems to stand to reason that one ought to use loose bounds in the beginning of the gradient iterations and increase the precision as the algorithm progresses. This could allow for further improvements in computation. Moreover, using theoretical tools for studying the convergence of optimization algorithms, one could possibly address the theoretical analysis of this algorithm.The submitted paper proposes a more efficient implementation of the Student-t distributed version of SNE. t-SNE is O(n^2), and the proposed implementation is O(nlogn). This offers a substantial improvement in the efficiency, such that very large datasets may be embedded. Furthermore, the speed increase is obtained through 2 key approximations without incurring a penalty on accuracy of the embedding.  There are 2 approximations that are described. First, the input space nearest neighbors are approximated by building a vantage-point tree. Second, the approximation of the gradient of KL divergence is made by splitting the gradient into attractive and repulsive components and applying a Barnes-Hut algorithm to estimate the repulsive component. The Barnes Hut algorithm uses a hierarchical estimate of force. A quad-tree provides an efficient, hierarchical spatial representation.   The submission is well-written and seems to be accurate. The results validate the claim: the error of the embedding does not increase, and the computation time is decreased by an order of magnitude. The approach is tested on MNIST, NORB, TIMIT, and CIFAR. Overall, the contribution of the paper is fairly small, but the benefit is real, given the popularity of SNE. In addition, the topic is relevant for the ICLR audience.The paper addresses the problem of low-dimensional data embedding for visualization purposes via stochastic neighbor embedding, in which Euclidean dissimilarities in the data space are modulated by the Gaussian kernel, and a configuration of points in the low-dimensional embedding space is found such that the new dissimilarities in the embedding space obtained via the Student-t kernel match the original ones as closely as possible in the sense of the KL divergence. While the original algorithm is O(n^2), the authors propose to use a fast multipole technique to reduce complexity to O(nlogn). The idea is original and the reported results are very convincing. I think it is probably one of the first instances in which an FMM technique is used to accelerate local embeddings.  Pros:  1.	The idea is simple and is relatively easy to implement. The authors also provide code. 2.	The experimental evaluation is large-scale, and the results are very convincing.  Cons:  1.	No controllable tradeoff between the embedding error and acceleration. 2.	In its current setting, the proposed approach is limited to local similarities only. Can it be extended to other settings in which global similarities are at least as important as the local ones? In other words, is it possible to apply a similar scheme for MDS-type global embedding algorithms?",0,5045
"Semantic Matching Energy Function for Learning with Multi-Relational Data  Paper Summary  This paper deals with learning an energy model over 3-way relationships. Each entity in the relation is associated a low dimensional representation and a neural network associate a real value to each representation triplet. The learning algorithm relies on an online ranking loss. Two models are proposed a linear model and a bilinear model.  Review Summary  The paper is clear and reads well. Its use of the ranking loss function to this problem is an interesting proposition. It could give more details on the ranking loss and the training procedure. The experiments could also be more thorough. My main concern however is that references to a co-author own work have been omitted. This omission means that the authors pretend not to know that a model with better reported performance exists. This should be discouraged and I will recommend the rejection of the paper.  Review  This paper is part of the recent effort of using distributed representation and various loss function for learning relational models. Papers focusing on this line of research include work from A. Bordes, J. Weston and Y. Bengio: - A Latent Factor Model for Highly Multi-relational Data (NIPS 2012).   Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes and Guillaume Obozinski. - Learning Structured Embeddings of Knowledge Bases (AAAI 2011).    Antoine Bordes, Jason Weston, Ronan Collobert and Yoshua Bengio.  The variations among this paper mainly involves   - model regularization (low rank, parameter tying...) - loss function  Regarding regularization, your proposition, Jenatton et al, and RESCAL are highly related. Basically your bilinear model seems to introduce a rank constrain on the 3D tensor representing all the relation {R_k, forall k} as in RESCAL notation. Basically your bilinar model decomposes R_k = (E_{rel,k} W_l) (W_r E_{rel,k})^T while the NIPS2012 model decomposes R_k as a linear combination of rank one matrices shared accross relations. Like Jenatton et al, you brake the symetry of the left and right relations.  Regarding loss, RESCAL uses MSE, Jenatton et al uses logistic loss and you use a ranking loss.  This differences result in different AUCs. Jenatton et al is always better, RESCAL and your model are close. Given that Jenatton et al and RESCAL precede your submission. I feel it is necessary to check one thing at a time, i.e. training a model parameterized like RESCAL / Jenatton et al / yours with all three losses. This would give the best combination. This could give an empirical advantage to your ideas (either parameterization or ranking loss) over Jenatton et al.  Given that your model is worse in terms of AUC compared to Jenatton et al. I feel that you should at least explain why and maybe highlight some other advantages of your approach. I am disappointed that you do not refer to Jenatton et al: you know about this paper (shared co-author), the results on the same data are better and you do not even mention it.  Typos/Details  Intro: unlike in previous work -> put citation here. 2.2 (2) even if it remains low dimensional, nothing forces the dimension of ... -> barely understandable rephrase this sentence.The paper proposes two functions for assigning energies to triples of entities, represented as vectors.  One energy function essentially adds the vectors of the relations and the entities, while another energy function computes a tensor product of the relation and both entities.  The new energy functions appear to beat other methods.  The main weakness is in the relative lack of novelty. The paper proposes a slightly different neural network architecture for computing energies of object triplets from the ones that existed before, but its advantage over these architectures hasn't been demonstrated conclusively.  How does it compare to a simple tensor factorization? (or even a factorization that computes an energy with a 3-way inner product sum_i a_i R_i b_i? such a factorization embeds entities and relations in the same space) Without this comparison, the new energy function is merely a 'new neural network architecture' that is not shown to outperform other architectures.  And indeed, the the performance of a simple Tensor factorization method matches the results of the more sophisticated factorization method that is proposed here, on the datasets from [6] that overlap with the datasets here (namely, UML and kinship).  In general, new energy functions or architectures are worthwhile only when they reliably improve performance (like the recently introduced maxout networks) or when they have other desirable properties, such as interpretability or simplicity.  The energy function proposed here is more complex than a simple tensor factorization method which appears to work just as well.  Pros   - New energy function, method appears to work well Cons  - The architecture is not compared against simpler architectures,    and there is evidence that the simpler architectures achieve    identical performance.    Data",0,5046
"This paper introduces a model for sentiment analysis aimed at capturing blended, non-binary notions of sentiment. The paper uses a novel dataset of >1 million blog posts (livejournal) using 32 emoticons as labels. The model uses a Gaussian latent variable to embed bag of words documents into a vector space shaped by the emoticon labels. Experiments on the blog dataset demonstrate the latent vector representations of documents are better than bag of words for multiclass sentiment classification.  I'm a bit confused as to how the inference procedure works. The conditional distribution P(Z|X) is Gaussian as is P(Z|Y), but the graphical structure suggests P(Y|Z) needs to be given and a Gaussian doesn't make sense here as Y is a binary vector. More generally, given the description in the paper I don't understand how to implement the proposed model. A more detailed description of the model itself and the inference procedure could help here.  There is a lot of recent work on representation learning for sentiment. No discussion of related models or comparisons to other work are given. In particular, recursive neural networks (e.g. Socher et al EMNLP 2011) have been used to learn document vector representations for multi-dimensional sentiment. Additionally, Maas et al (ACL 2011) introduce a benchmark dataset and similar latent-variable graphical model for sentiment representations. Overall, I think substantially more discussion of previous work is necessary. The two citations given aren't reflective of much of the recent work on learning text representations or sentiment.  To summarize: - The proposed dataset sounds interesting and could advance representation learning for multi-dimensional sentiment analysis - The proposed model is very unclear. With the current explanation I am unable to verify its correctness - Practically no discussion of the large amount of previous work on learning representations for text and sentimentThis paper proposes a new method for sentiment analysis of text documents based on two phases: first, learning a continuous vector representation of the document (a projection on the mood manifold) and second, learning to map from this representation to the sentiment classes. The assumption behind this model is that such an intermediate smooth representation might help the classification, especially in the case where the number of sentiment classes is rather large (32) as it is studied here.  The idea of modeling the relationship between emotions labels (Y) and documents (X, encoded using bag-of-words) via an intermediate representation (Z) is appealing and seems to be a good direction to pursue.  The main idea of the present model is to build a kind of two-layers network(X->Z->Y), where each layers has its own architecture and learning process and is trained in a (weakly) supervised way. Unfortunately, it is not exactly clear how this training works. On one hand, the layer X->Z is trained via maximum likelihood, setting the supervision on Z via least-square regression for Z->Y (X and Y are known). But on the other hand, it is written that the layer Z->Y is obtained via MDS or kernel PCA. This is a bit puzzling.  I also think that the dimension of the manifold (l) should be given.  There is a lack of references (maybe due to the page limit) Still, (Glorot et al., ICML11), (Chen et al., ICML12) or (Socher et al., EMNLP11) should be discussed since all these papers presents neural network archiecture for sentiment analysis and basically learn an intermediate representation of documents.  Pros: - interesting setting with weak supervision - new data  Cons: - many unclear points - lack references",1,5047
"Matrix Approximation under Local Low-Rank Assumption  Paper summary  This paper deals with low-rank matrix approximation/completion. To reconstruct a matrix element M_{i,j}, the proposed method performs a weighted low rank matrix approximation which considers a similarity metric between matrix coordinates. More precisely, the weighting scheme emphasizes reconstruction errors close to the element {i,j} to reconstruct. As a computational speedup, the authors perform the low rank approximation only on a small set of coordinates and approximate the reconstruction for any coordinate through kernel estimation.  Review Summary  The core idea of the paper is interesting and could be helpful in many practical applications of low rank decomposition. The paper reads well and is technically correct. On the negative side, I feel that the availability of a meaningful similarity metric between coordinates should be discussed. The experimental section could be greatly improved. There is no reference to related work at all.  Review   In many application of matrix completion, the low rank decomposition algorithm is here to circumvent the fact that no meaningful similarity metric between coordinate pairs is available. For instance, if such a metric is available in a collaborative filtering scenario, one would simply take the input (customer, item) fetch its neighbors and average its ratings. Your algorithm presuppose the availability of such a metric, could you discuss this core aspect of your proposal in the paper?  Following on this idea, would you consider as baseline performing the Nadaraya-Watson kernel regression on the matrix itself and reports the result in your experimental section. This would be meaningful to quantify how much comes from the low rank smoothing and how much comes simply from the quality of the similarity metric.  Still in the experimental section,  - would you consider validating the kernel width? - discuss the influence of the L2 regularizer which is not even introduced in the previous sections - define clearly the d you use. To me d(s,s') compare two coordinate pairs and I do not know how to relate it to the arccos you are using, i.e. what are x,y? - could you measure the variance observed due to the sampling of anchor points and could you report whether the reconstruction error is greater further from anchor points? - how does Nadaraya-Watson smoothing compare with respect to solving the low rank problem for each point?  References: - you should at least refer to weighted low rank matrix approximation (Srebro & Jaakkola, ICML-03). It would be good to refer to prior work on expanding low rank matric approximation, given how fertile this field has been in the Netflix prize days (Maximum Margin Matrix Factorizations, RBM for matrix completion...).  Details along the text  - In Eq. 1, to unify notation, you could use the projection Pi here as well - Hoelder continuity: I do not understand how it relate to the smoothing kernel approach defined below. I believe this sentence could be removed.Approximation and completion of sparse matrices is a common task. As popularized by the Netflix prize, there are many possible approaches, and combinations of different styles of approach can lead to better predictions than individual methods. In this work, local prediction and low-rank factorization are combined as one coherent method.  This is a short paper, with an interesting idea, and some compelling results. It has the appealing property that one can almost guess what is going to come from the abstract. My key question while reading was how locality was going to be defined: one of the goals of low-rank learning is finding a space in which to represent entities. The paper uses a simple distance measure to local support points. I'm not sure if a value is missing from one row and not another whether it is ignored, or counted as zero. I wonder if an approach that finds a low-rank or factor model fit and uses that to define distances for local modelling might work better. Potentially one could iterate, after fitting the model, get improved distances and refit.  I find the large improvement over the Netflix prize winners surprising given the large effort invested over three years to get that result. Is one relatively simple method really sufficient to blow that away? I think open code and scrutiny would be required to be sure. (Honest mistakes are not unprecedented: http://arxiv.org/abs/1301.6659v2 ) It will be a great result if correct.  I found the complete lack of references distracting. There is clearly related work in this area. Some of it is even mentioned, just with no formal citations. This is a workshop submission for light touch review, but citations seem like a basic requirement for any scientific document.  Pros: neat idea, quick, to-the-point presentation. Cons: I'm suspicious of the results, and would like to see a reference section.",0,5048
"This paper focuses on multi-task learning across domains, where both the data generating distribution and the output labels can change between source and target domains. It presents a SVM-based model which jointly learns 1) affine hyperplanes that separate the classes in a common domain consisting of the source and the target projected to the source; and 2) a linear transformation mapping points from the target domain into the source domain.  Positive points  1) The method is dead simple and seems technically sound. To the best of my knowledge it's novel, but I'm not as familiar with the SVM literature - I am hoping that another reviewer comes from the SVM community and can better assess its novelty. 2) The paper is well written and understandable 3) The experiments seem thorough: several datasets and tasks are considered, the model is compared to various baselines. The model is shown to outperform contemporary domain adaption methods, generalize to novel test categories at test time (which many other methods cannot do) and can scale to large datasets.  Negative points  I have one major criticism: the paper doesn't seem really focused on representation learning - it's more a paper about a method for multi-task learning across domains which learns a (shallow, linear) mapping from source to target. I agree - it's a representation but there's no real analysis or focus on the representation itself - e.g. what is being captured by the representation. The method is totally valid, but I just get the sense that it's a paper that could fit well with CVPR or ICCV (i.e. a good vision paper) where the title says 'represention learning', and a few sentences highlight the 'representation' that's being learned, however the method nor the paper's focus is really on learning interesting representations. On one hand I question its suitability for ICLR and it's appeal to the community (compared to CVPR/ICCV, etc.) but on the other hand, I think it's great to encourage diversity in the papers/authors at the conference and having a more 'visiony'-feeling paper is not a bad thing.  Comments --------  Can you state up front what is meant by the asymmetry of the transform (e.g. when it's first mentioned)? Later on in the paper it becomes clear that it has to do with the source and target having different feature dimensions but it wasn't obvious to me at the beginning of the paper.   Just before Eq (4) and (5) it says that 'we begin by rewriting Eq 1-3 with soft constraints (slack)'. But where are the slack variables in Eq 4?This paper proposes to make domain adaptation and multi-task learning easier by jointly learning the task-specific max-margin classifiers and a linear mapping from a new target space to the source space; the loss function encourages the mapped features to lie on the correct side of the hyperplanes learned for each task of the hyperplanes of the max-margin classifiers. Experiments show that the mapping performs as well or better as existing domain adaptation methods, but can scale to larger problems while many earlier approaches are too costly.  Overall the paper is clear, well-crafted, and the context and previous work are well presented. The idea is appealing in its simplicity, and works well.  Pros: the idea is intuitive and well justified; it is appealing that the method is flexible and can tackle cases where labels are missing for some categories. The paper is clear and well-written. Experimental results are convincing enough; while the results are not outperforming the state of the art (results are within the standard error of previously published performance), the authors' argument that their method is better suited to cases where domains are more different seems reasonable and backed by their experimental results.  Cons: this method would work only in cases where a simple general linear rotation of features would do a good job placing features in a favorable space. The method also gives a privileged role to the source space, while methods that map features to a common latent space have more symmetry; the authors argue that it is hard to guess the optimal dimension of the latent space -- but their method simply constrains it to the size of the source space, so there is no guarantee that this would be any more optimal.The paper presents a new method for learning domain invariant image representations. The proposed approach simultaneously learns a linear mapping of the target features into the source domain and the parameters of a multi-class linear SVM classifier. Experimental evaluations show that the proposed approach performs similarly or better than previous art. The new algorithm presents computational advantages with respect to previous approaches.  The paper is well written and clearly presented. It addresses an interesting problem proposing that has received attention in recent years. The proposed method is considerably simpler than competitive approaches with similar (or better) performance (in the setting of the reported experiments). The method is not very novel but manages to improve some drawbacks of previous approaches.  Pros:  - the proposed framework is fairly simple and the provided implementation details makes it easy to reproduce - experimental evaluation is presented, comparing the proposed method with several competing approaches. The amount of empirical evidence seems sufficient to back up the claims.  Cons:  - Being this method general, I think that it would have been very good to include an example with more distinct source and target feature spaces (e.g. text categorization), or even better different modalities.   Comments:  In the work [15], the authors propose a metric that measures the adaptability between a pair of source and target domains. In this setting if several possible source domains are available, it selects the best one. How could this be considered in your setting?  In the first experimental setting (standard domain adaptation problem), I understand that the idea the experiment is to show how the labeled data in the source domain can help to better classify the data in the target domain. It is not clear to me how the SVM trained with training data, SVM_t, of the target domain. Is this done only with the limited set of labeled data in the target domain? What is the case for the SVM_s?  Looking to the last experimental setting, I suppose that the SVM_s (trained using source training data) also includes the transformed data from the target domain. Otherwise, I don't understand how the performance can increase by increasing the number of labeled target examples.",1,5049
"The paper provides an explicit connection between the linear-nonlinear-poisson (LNP) model of biological neural networks and the Boltzmann machine. The author proposes a semi-stochastic inference procedure on Boltzmann machines, with some tweaks, that can be considered equivalent to the inference of an LNP model.  Author's contributions: (1) Starting from the LNP neuron model the author, in detail, derives one (Eq. 5) that closely resembles a single unit in a Boltzmann machine.  (2) A semi-stochastic inference (Eq. 10) for a Boltzmann machine that combines Gibbs sampling and variational inference is introduced. (3) Several tweaks (Sec. 4) are proposed to the semi-stochastic inference (Eq. 10) to mimic Eq.5 as closely as possible.  Pros) As I am not an expert in biological neurons and their modeling, it is difficult for me to assess the novelty fully. Though, it is interesting enough to see that the inference in the biological neuronal network (based on the LNP model) corresponds to that in Boltzmann machines. Despite my lack of prior work and details in biological neuronal models, the reasoning seems highly detailed and understandable. I believe that not many work has explicitly shown the direct connection between them, at least, not on the level of a single neuron (Though, in a high level of abstraction, Reichert (2012) used a DBM as a biological model).   Cons) If I understood correctly, unlike what the title seems to claim, the network consisting of LNP neurons do 'not' perform the exact inference on the corresponding Boltzmann machine. Rather, one possible approximate inference (the semi-stochastic inference, in this paper) on the Boltzmann machine corresponds to the LNP neural network (again, in a form presented by the author).   I can't seem to understand how the proposed inference, which essentially samples from the variational posterior and use them to compute the variational parameters, differs much from the original variational inference, except for that the proposed method adds random noise in estimating variational params. Well, perhaps, it doesn't really matter much since the point of introducing the new inferernce scheme was to find the correspondance between the LNP and Boltzmann machine.      = References = Reichert, D. Deep Boltzmann Machines as Hierarchical Generative Models of Perceptual Inference in the Cortex. Ph.D. Thesis. 2012.This paper proposes a scheme for utilizing LNP model neurons to perform inference in Boltzmann Machines.  The contribution of the work is to map a Boltzmann Machine network onto a set of LNP model units and to demonstrate inference in this model.  The idea of using neural spiking models to represent probabilistic inference is not new (see refs. at end).  The primary contribution of this work is to take a learned deep Boltzmann machine from the literature, and to implement this network using LNP neurons, with the necessary modifications.  Therefore, the contribution is specific to the deep Boltzmann machine architecture.  The existing work in the literature often takes a different approach: taking realistic neural models and asking how these models can represent variables probabilistically.  Pros: Developing mappings between machine learning algorithms and neural responses is an important direction. To my knowledge, the implementation of a deep-BM with spiking neurons is novel.  Cons: The clarity of the text and presentation of the mathematics needs improvement. The resulting model suffers from some non-biological phenomenology. The empirical results are not very compelling. I would liked to have seen a comparison to the existing approaches for using spiking neurons to implement inference.  Particularly: [2-5]. Is there not a mapping from those models to the deep BM?  Why is the proposed mapping necessary, or what are the limitations of those existing proposals for a deep BM?   Other comments: The paper provides a lengthy introduction to LNP and inference.  I would encourage the author to justify the various details that are introduced, and those that are not directly relevant for the proposed network, should be left out.  In general, the exposition needs clarification.  The proposed network seems like a logical series of steps, but the end result leads to a biologically implausible network, (at least when considering known properties of cortex).  I think a broad approach might be warranted for this problem. For example, starting from the LNP model and using this model as an element in a Boltzmann machine.  A related note: Isn't it just more plausible to estimate a deep-network with positive only weights? (to deal with Dale's law) There is likely some work to be done there, but it seems this direction wouldn't require the paired neurons you have here.  Or a network with realistic excitatory-inhibitory ratio?  Why not start with a Poisson-unit Boltzmann machine, and examine its properties? see (Welling et al. 2005)  I found the empirical evaluation to be weak.  I don't understand how running the network is a demonstration of correct inference.  Wouldn't we expect each of these networks to diverge and sample different parts of the posterior?  The statistics in Figure 5 need more justification.  I did not understand why these are relevant, or what degree of variability should be acceptable.  There are, of course, a variety of biological issues that seem to be incongruent with the proposal. In cortex the distribution of excitatory to inhibitory neurons is 4:1.  The current proposal seems to require 1:1. The pairing of neuron weights seems unlikely, but maybe this could be solved through learning? What about mean firing rates?, are these consistent between model and cortical responses?  The title is a little misleading. I might suggest something more like: Networks of LNP neurons are capable of performing Bayesian inference on Boltzmann machines.  The work of Shi and Griffiths 2009 seems highly relevant, and addresses some of the questions posed by the author.  Note that Dale's law is not generally applicable, but I am not sure about any refuttaion in cortex, which I assume is where you would imagine the deep network. (see co-transmission)  [1] Welling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential family harmoniums with an application to information retrieval. Advances in Neural Information Processing Systems 17, pages 1481-1488. MIT Press, Cambridge, MA.  [2] Shi, L., & Griffiths, T. L. (2009). Neural implementation of hierarchical Bayesian inference by importance sampling. Advances in Neural Information Processing Systems 22  [3] Ma WJ, Beck JM, Pouget A (2008) Spiking networks for Bayesian inference and choice. Current Opinion in Neurobiology 18, 217-22.  [4] Pecevski D, Buesing L, Maass W (2011) Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons. PLoS Comput Biol 7(12): e1002294  [5] József Fiser, Pietro Berkes, Gergő Orbán, Máté Lengyel. Statistically optimal perception and learning: from behavior to neural representations', Trends Cogn Sci. 2010. 14(3):119-130This paper argues that inference in Boltzmann machines can be performed using neurons modelled according to the Linear Nonlinear-Poisson model. The LNP model is first presented, then one variant of inference procedure for Boltzmann machine is introduced and a section shows that LNP neurons can implement it. Experiments show that the inference procedure can produce reconstructions of handwritten digits.  Pros: the LNP model is presented at length and LNP neurons can indeed perform the operations needed for inference in the Boltzmann machine model. Cons: the issue of learning the network itself is not tackled here at all. While the mapping between the LNP model and the inference process in the machine is particularly detailed here, I did not find this particularly illuminating, given that restricted Boltzmann machines were designed with a simple inference procedure with only very simple operations. I find this paper provides too little new insight to warrant acceptance at the conference.",0,5050
"Summary: This paper proposes learning a pooling layer (not necessarily of a convolutional network) by using temporal coherence to learn the pools. Training is accomplished by minimizing a criterion that encourages the features to change slowly but have high entropy over all. Detailed comments: -The method demonstrates improvement over a spatial pooling baseline -The experiments here don't allow comparison to prior work on learning pools, such as the paper by Jia and Huang. - The method is not competitive with the state of the art  Suggestions to authors:  In future revisions of this paper, please be more specific about what your source of natural videos was. Just saying vimeo.com is not very specific. vimeo.com has a lot of videos. How many did you use? Do they include the same kinds of objects as you need to classify on CIFAR-10? Comparing to Jia and Huang is very important, since they also study learning pooling structure. Note that there are also new papers at ICLR on learning pooling structure you should consider in the future. I think Y-Lan Boureau also wrote a paper on learning pools that might be relevant. Pros: -The method demonstrates some improvement over baseline pooling systems applied to the same task.  Cons: -Doesn't compare to prior work on learning pools -The method isn't competitive with the state of the art, despite having access to extra training data.The paper presents a method to learn invariant features by using temporal coherence.  A set of linear pooling units are trained on top of a set of (pre-trained) features using what is effectively a linear auto-encoder with a penalty for changes over time (a 'slowness' penalty).  Visualizations show that the learned weights of the pooling units tend to combine features for translated or slightly rotated edges (as expected for complex cells), and benchmark results show some improvement over hand-coded pooling units.  This is a fairly straight-forward idea that gives pleasing results nonetheless.  The main attraction to the method proposed here is its simplicity and modularity:  a linear auto-encoder and slowness penalty is very easy to implement and could be used in almost any pipeline.  This is simultaneously my main concern about the method:  it is significantly subsumed by prior work (though the very simple instance here might differ).  For example, see the work of Zou et al. (NIPS 2012) which uses essentially the same training method with nonlinear pooling units, Mobahi et al. (ICML 2009), and work with 'slowness' criteria more generally.  That said, considering the many algorithms that have been proposed to learn pooling regions and invariant features without video, the fact that an extremely simple instance like the one here can give reasonable results is worth emphasizing.  Pros: (1) A very simple approach that appears to yield plausible invariant features and a modest bump over hand-built pooling in the unsupervised setting.  Cons: (1) Only linear pooling units are considered.  As a result they do not add much power beyond slight regularization of the linear SVM. (2) Only single-layer networks are considered;  results with deep layers might be very interesting. (3) There is quite a lot of prior work with very similar ideas and implementations;  hopefully these can be cited and discussed.Many vision algorithms comprise a pooling step, which combines the outputs of a feature extraction layer to create invariance or reduce dimensionality, often by taking their average. This paper proposes to refine this pooling step by 1) not restricting pooling to merely spatial dimensions (so that several different features can be combined), and 2) learning it instead of deciding the structure of the pools beforehand. This is achieved by replacing the pooling step by a linear transformation of the outputs of the feature extractor (here, an autoencoder), with the constraint that all weights be nonnegative. The main intuition for training is that an invariant representation should not change much between two neighboring frames in a video. Thus, training is conducted by minimizing a cost function that combines a reconstruction error cost and a frame-to-frame dissimilarity cost: the reconstruction error cost ensures that the representation before pooling can be reconstructed from the pooled output without too much discrepancy, and the dissimilarity cost encourages two neighboring frames in a video to have similar pooled representation. Two experiments are provided: the first one shows that training on patches from natural videos yields pool that combine similar features; the second one tests the algorithm on CIFAR10 and shows that the scheme proposed here performs better than spatial pooling.  Learning pooling weights instead of pre-selecting them is appealing, however this work does not demonstrate the value of the advocated approach. First, the context given is insufficient; much previous work has explored how to combine different feature maps across feature types rather than only across space, with good results; some of this work is cited here (e.g., ref. 5, Hyvärinen, Hoyer, Inki 2001, ref. 7 Kavukcuoglu et al. 2009), but only briefly mentioned and dismissed because (1) the clusters are required to be fixed manually, (2) clusters are required to have the same size (I am not sure why this paper mentions that, this is not true -- the clusters do have the same size in these papers but it is not a requirement), and (3) there is no 'guarantee that the optimal feature clustering can be mapped into two-dimensional space'. This is true, but the two-dimensional mapping into a topographical map is a bonus, and the same cost functions could be applied with no overlap between the pools, as in the approach advocated here, and still obtain pools that group similar features. In any case, it is not sufficient to merely state the shortcomings of these previous approaches, without showing that the method here outperforms them and that these supposed shortcomings truly hurt performance. Another line of work that should definitely be introduced (and isn't), is work enforcing similarity of representations for similar images to train coding. There has been much work on this, even also using video, e.g. Mobahi, Collobert, Weston, Deep Learning from Temporal Coherence in Video, ICML 2009 -- or before that with collections of still images with continuously varying parameters, Hadsell, Chopra and LeCun, Dimensionality Reduction by Learning an Invariant Mapping (CVPR 2006), and much other work. Those older works use similarity-based losses to train encoding features rather than pooling, but this is not a real difference, which is my second point:  Second, comparing the pooling step here to a simple spatial pooling step is somewhat misleading; the 'auto-pooling step' in this paper is a full-fledged linear mapping, with the added restriction that the weights have to be nonnegative. Thus the system is more akin to a two-layer encoding network than a single-layer network. The distinction between 'coding' and 'pooling' is an artificial one anyways; given that auto-pooling has as many parameters are a standard coding step, it should not only be compared to the much simpler spatial pooling. In terms of performance, the performance on Cifar 10 is much below what can be obtained with a single layer of features (e.g. compare the 69.7% here to results between 68.6% and  79.6% in Coates et al.'s 'An Analysis of Single-Layer Networks in Unsupervised Feature Learning', and better performance in subsequent papers by Coates et al.), so this is indeed not very convincing.  The ideas combined here (learning a pooling map, using similarity in neighboring frames,  Pros/cons: - pros: ideas for generalizing pooling are intuitive and appealing - cons: many of these ideas have been explored elsewhere before, and this paper does not do a suitable job of delineating what the specific contribution is. In fact, it seems that the proposed approach does not have much novelty and most ideas here are already part of existing algorithms; experimental results fail to demonstrate the superiority of the proposed scheme.",0,5051
"The paper presents an application of clustering-based feature learning ('CL') to image recognition tasks and tracking tasks for robotics.  The basic system uses a clustering algorithm to train filters from small patches and then applies them convolutionally using a sum-abs-difference (instead of inner product) operation.  This is followed with a fixed combination of processing stages (pooling, nonlinearity, normalization) and passed to a supervised learning algorithm.  The approach is compared with 2-layer CNNs on image recognition benchmarks (StreetView house numbers, CIFAR10) and tracking (TLD dataset);  in the last case it is shown that the method outperforms a 2-layer CNN from prior work.   The speed of learning and test-time evaluation are compared as a measure of suitability for realtime use in robotics.  The main novelty here appears to be on a couple of points:  (1) the particular choice of architecture (which is motivated at least in part by the desire to run in programmable hardware such as FPGAs), (2) documenting the speed advantage and positive tracking results in applications, both of which are worthwhile goals.  Evaluation and training speed, as the authors note, are not well-documented in deep learning work and this is a problem for real-time applications like robotics.  Some questions I had about the content:  I did not follow how the 2nd layer of clustered features were trained.  It looks like these were trained on single channels of the pooled feature responses?  Was the sum-abs-diff operation also used for the CNN?  One advantage of the clustering approach is that it is easier to train larger filter banks than with fine-tuned CNNs.  Can the accuracy gap in recognition tasks be reduced by using more features?  And at what cost to evaluation time?  Pros: (1)  Results are presented for a novel network architecture, documenting the speed and simplicity of clustering-based feature learning methods for vision tasks.  It is hard to overstate how useful rapid training is for developing applications, so further results are welcome. (2)  Some discussion is included about important optimizations for hardware, but I would have liked more detail on this topic.  Cons: (1)  The architecture is somewhat unusual and it's not clear what motivates each processing stage. (2)  Though training is much faster, it's not clear to what degree the speed of training is useful for the robotics applications given.  [As opposed to online evaluation time.] (3)  The extent of results is modest, and their bearing on actual work in robotics (or broader work in CV) is unclear.  The single tracking result is interesting, but versus a baseline method.  Overall, I think the 'cons' point to the robotics use-case not being thoroughly justified;  but there are some ideas in here that would be interesting on their own with more depth.I am *very* sympathetic to the aims of the authors: Find simple, effective and fast deep networks to understand sensor data. The authors defer some of the more interesting bits to future work however: they note that sum-abs-diff should be much more efficient in silicon implementation then convolution style results. That would indeed be interesting, and would make this paper all the more exciting.  Methods that make such learning efficient are certainly important.  The paper references [8] to explain seemingly significant details. At least a summary seems in order. Ideally a pseudo-code description. Currently the paper is close to un-reproducible, which is unfortunate as it seems easy to correct that.  Details of the contrast normalization would make the paper more self contained. This is a rather specialized technique (by that name), and should be discussed when addressing the community at large.  I can't parse the second paragraph of Section 2.3.  Many of the details elements seem a bit unmotivated. The authors repeatedly mention in the beginning (which seem like details too early in the paper) things they *don't* do (Whitening, ZCA?, color space changes), but these seem like details, and the motivation to *not* perform them isn't compelling. Why the difference in connectivity between the CNN and that presented here?  The video data-sets are very interesting and compelling. It would be good for a sense of scale to report the results for [22,30] as well.  The fact that second layers that are random still works well is interesting: 'The randomly connected 2nd layer used a fixed CNN layer as described in section 2.2.'  Why was this experiment with a random CNN, and not a random CL (sum-abs-diff) to match the experiments?  What does one conclude from these results? It seems the first layer is, to a close approximate, equivalent to a Gabor filter bank. The second layer, random appears to be perfectly acceptable. (Truly random? How does randomly initialized from the data do?) That seems rather disappointing from a learning point of view.  In general the paper reads a bit like an early draft of a workshop paper. Interesting experiments, but hard to read, and seemingly incomplete.   A few points on style seem in order:  First the authors graciously acknowledge prior work and say 'we could not have done any of this work without standing on the shoulders of giants.' Oddly, one of the giants acknowledged is one of the authors. I assume this is some last minute mistake due to authorship changes, but it reflects the rushed and incomplete nature of the work.  Similarly, the advertisements for the laboratory in the footnotes are odd and out of place in a conference submission.  The title seems slightly misleading: this seems to be a straightforward ML for vision paper with otherwise no connection to robotics.  Pros:  Tacking important issues for the field. Good and interesting experiments. Focus on performance is important.  Cons:  Difficult to understand details of implementation. Many design decisions make it hard to compare/contrast techniques and seem unmotivated. Some of the most interesting work (demonstrating benefits of technique in performance) are deferred to future work. Style and writing make the paper difficult to read.# Summary  This paper compares two types of filtering operator (linear filtering vs.  distance filtering) in convolutional neural networks for image processing. The paper evaluates two fairly arbitrarily-chosen architectures on the CIFAR-10 and SVHN image labeling tasks, and shows that neither of these architectures is very effective, but that the conventional linear operator works better. The paper nevertheless advocates the use of the distance filtering operation on grounds of superior theoretical efficiency on e.g. FPGA hardware, but details of this argument and empirical substantiation are left for future work. The distance-based algorithm was more accurate than the linear-filtering architecture on a tracking task. How good the tracker is relative to other algorithms in the literature on the data set is not clear; I am admittedly not an expert in object tracking, and the authors simply state that it is 'not state-of-the-art.'  The paper's value as a report to roboticists on the merit of either clustering or linear operators is undermined by the lack of discussion or guidance regarding how one might go beyond the precise experiments done in the paper. The paper includes several choices that seem arbitrary: filter sizes, filter counts, numbers of layers, and so on. Moreover these apparently arbitrary choices are made differently for the different data sets. Compared to other papers dealing with these data sets, the authors have made the model much smaller, faster, and less accurate. The authors stress that the interest of their work is in enabling 'real-time' operation on a laptop, but I don't personally see the interest of targeting such CPUs for real-time performance and the paper does not argue the point.  The authors also emphasize the value of fast unsupervised learning based on clustering, but the contribution of this work beyond that of Coates et al. published in 2011 and 2012 is not clear.  # Detailed Comments  The statement 'We used the Torch7 software for all our experiments [18], since this software can reduce training and learning of deep networks by 5-10 times compared to similar Matlab and Python tools.' sounds wrong to me. A citation would help defend the statement, but if you meant simply to cite the benchmarking from [18], then the authors should also cite follow-up work, particularly Bastien et al. 2012  ('Theano: new features and speed improvements').  The use of 'bio-inspired' local contrast normalization instead of whitening should include citation to previous work. (i.e. Why/how is the technique inspired by biology?)  Is the SpatialSAD model the authors' own innovation? If so, more details should be listed. If not, a citation to a publication with more details should be listed. I have supposed that they are simply computing a squared Euclidean distance between filter and image patch as the filter response.  Regarding the two architectures used for performance comparison - I am left wondering why the authors chose not to use spatial contrastive normalization in both architectures.  As tested, performance differences could be attributed to *either* the CL or the spatial contrast normalization.  I am a little confused by the phrase 'correlate filter responses to inputs' - with the sum-of-squared-differences operator at work, my intuition would be that inputs are less-correlated to filter responses than they would be with a convolution operator.  The use of the phrase 'fully connected' in the second-last paragraph on page 3 is confusing - I am assuming the authors mean all *channels* are connected to all *channels* in filters applied by the convolution operator.  Usually in neural networks literature, the phrase 'fully connected' means that all *units* are connected between two layers.  The results included no discussion of measurement error.",0,5052
"In this paper, the authors consider unsupervised metric learning as a density estimation problem with a Parzen windows estimator based on  Euclidean metric. They use maximum likelihood method and EM algorithm for deriving a method that may be considered as an unsupervised counterpart to neighbourhood component analysis. Various versions of the method provide good results in the clustering problems considered.  + Good and interesting conference paper. + Certainly novel enough. - Modifications are needed to combat the problems of overfitting, local minima, and computational load in the basic approach proposed. Some of these improvements are heuristic or seem to require hand-tuning.   Specific comments:  - The authors should refer to the paper S. Kaski and J. Peltonen, 'Informative discriminant analysis', in T. Fawcett and N. Mishna (Eds.), Proc. of the 20th Int. Conf. on Machine Learning (ICML 2003), pp. 329-336, AAAI Press, Menlo Park, CA, 2003. In this paper, essentially the same technique as Neighbourhood Component Analysis is defined under the name Informative discriminant analysis one year prior to the paper by Goldberger et al., your reference [16].  - In the beginning of page 6 the authors state: 'Following [1, 2], the data is progressively corrupted by adding dimensions of white Gaussian noise, then whitened.' In this case, whitening amplifies Gaussian noise, so that it has the same power as the underlying data. Obviously this is the reason why the experimental results approach to a random guess when the dimensions of the white noise increase sufficiently. The authors should mention that in real-world applications, one should not use whitening in this kind of situations,  but rather compress the data using for example principal component analysis (PCA) without whitening for getting rid of the extra dimensions corresponding to white Gaussian noise. Or at least use the data as such without any whitening.Summary of contributions: 1. The paper proposed an unsupervised local component analysis (LCA) framework that estimates the Parzen window covariance via maximizing the leave-one-out density. The basic algorithm is an EM procedure with closed form updates.   2. One further extension of LCA was introduced, which assumes two multiplicative densities, one is Parzen window (non Gaussian) and the other is a global Gaussian distribution.   3. Algorithms was designed to scale up the algorithms to large data sets.   Assessment of novelty and quality: The work looks quite reasonable. But the approach seems to be a bit straightforward.  The work is perhaps not very deep or inspiring.    My major concern is, other than the described problem setting being tackled, mostly toy problems, I don't see the significance of the work for addressing major machine learning challenges. For example, the authors argued the approach might be a good preprocessing step, but in the experiments, there is nothing like improving machine learning (e.g. classification) via such a pre-processing of data.   It's disappointing to see that the authors didn't study the identifiability of the Parzen/Gaussian model. Addressing this issue should have been a  good chance to show some depth of the research.Summary of contributions: The paper presents a robust algorithm for density estimation. The main idea is to model the density into a product of two independent distributions: one from a Parzen windows estimation (for modeling a low dimensional manifold) and the other from a Gaussian distribution (for modeling noise). Specifically, leave-one-out log-likelihood is used as the objective function of Parzen window estimator, and the joint model can be optimized using Expectation Maximization algorithm. In addition, the paper presents an analytical solution for M-step using eigen-decomposition. The authors also propose several heuristics to address local optima problems and to improve computational efficiency. The experimental results on synthetic data show that the proposed algorithm is indeed robust to noise.   Assessment on novelty and quality:  Novelty:  This paper seems to be novel. The main ideas (using leave-one-out log-likelihood and decomposing the density as a product of Parzen windows estimator and a Gaussian distribution) are very interesting.  Quality:  The paper is clearly written. The method is well motivated, and the technical solutions are quite elegant and clearly described. The paper also presents important practical tips on addressing local optima problems and speeding up the algorithm.   In experiments, the proposed algorithm works well when noise dimensions increase in the data. The experiments are reasonably convincing, but they are limited to very low-dimensional, toy data. Evaluation on more real-world datasets would have been much more compelling. Without such evaluation, it’s unclear how the proposed method will perform on real data.  Although interesting, the assumption about modeling the data density as a product of two independent distributions can be too strong and unrealistic. For example, how can this model handle the cases when noise are added to the low-dimensional manifold, not as orthogonal “noise dimension”?   Other comments: - Figure 1 is not very interesting since even NCA will learn near-isotropic covariance, and the baseline method seems to be PCA whitening, not PCA.   Pros and cons: pros: - The paper seems sufficiently novel.  - The main approach and solution are technically interesting. - The experiments show proof-of-concept (albeit limited) demonstration that the proposed method is robust to noise dimensions (or irrelevant features).  cons: - The experiments are limited to very low-dimensional, toy datasets. Evaluation on more real-world datasets would have been much more compelling. Without such evaluation, it’s unclear how the proposed method will perform on real data. - The assumption about modeling the data density as a product of two independent distributions can be too strong and unrealistic (see comments above).",0,5053
"The paper analyses the representational capacity of RBM's, contrasting it with other simple models.  I think the results are new but I'm definitely not an expert on this field. They are likely to be interesting for people working on RBM's, and thus to people at ICLR.This paper compares the representational power of Restricted Boltzmann Machines (RBMs) with that of mixtures of product distributions. The main result is that RBMs can be exponentially more efficient (in terms of the number of parameters required) to represent some classes of probability distributions. This provides theoretical justifications to the intuition behind the motivation for distributed representations, i.e. that the combinations of an RBN's hidden units can give rise to highly varying distributions, with a number of modes exponential in the model's size.  This paper is very dense, and unfortunately I had to fast-forward through it in order to be able to submit my review in time. Although most of the derivations do not appear to be that complex, they build on existing results and concepts that the typical machine learning crowd is typically unfamiliar with. As a result, one can be quickly overwhelmed by the amount of new material to digest, and going through all steps of all proofs can take a long time.  I believe the results are interesting since they provide a theoretical foundation to ideas that have been motivating the use of distributed representations. As a result, I think they are quite relevant to current research on learning representations, even if the practical insights seem limited.  The maths appear to be solid, although I definitely did not check them in depth. I appreciate the many references to previous work.  Overall, I think this paper deserves being published, although I wish it was made more accessible to the general machine learning audience, since in its current state it takes a lot of motivation to go through it. Providing additional discussion throughout the whole paper on the motivations and insights behind these many theoretical results, instead of mostly limiting them to the introduction and discussion, would help the understanding and make the paper more enjoyable to read.  Pros: relevant theoretical results, (apparently) solid maths building on previous work Cons: requires significant effort to read in depth, little practical use  Things I did not understand: - Fig. 1 (as a whole) - Last paragraph of 1.1: why is this interesting? - Fig. 5 (not clear why it is in some kind of pseudo-3D and what is the meaning of all these lines -- also some explanations come after it is referenced, which does not help) - '(...) and therefore it contains distributions with (...)': I may be missing something obvious, but I did not follow the logical link ('therefore') - I am unable to parse Remark 22, not sure if there is a typo (double 'iff') or I am just not getting it.  Typos or minor points: - It seems like Fig. 3 and 4 should be swapped to match the order in which they appear in the text - 'Figure 3 shows an example of the partitions (...) defined by the models M_2,4 and RBM_2,3' -> mention also 'for some specific parameter values' to be really clear - Deptartment (x2) - Lebsegue - I believe the notation H_n is not explicitly defined (although it can be inferred from the definition of G_n) - There is a missing reference with a '?' on p. 9 after 'm <= 9' - It seems to me that section 6 is also related to the title of section 5. Should it be a subsection? - 'The product of mixtures represented by RBMs are (...)': products - 'Mixture model (...) generate': modelsThis paper attempts at comparing mixture of factorial distributions (called product distributions) to RBMs. It does so by analyzing several theoretical properties, such as the smallest models which can represent any distribution with a given number of strong modes (or at least one of these distributions) or the smallest mixture which can represent all the distributions of a given RBM.  The relationship between RBMs and other models using hidden states is not fully understood and any clarification is welcome. Unfortunately, not only I am not sure the MoP is the most interesting class of models to analyze, but the theorems focus on extremely specific properties which severely limits their usefulness: - the definition of strong modes makes the proofs easier but it is hard to understand how they relate to 'interesting' distributions. I understand this is a very vague notion but I would have appreciated hints about how the distributions we care about tend to have a high number of strong modes. - the fact that there are exponentially many inference regions for an RBM whereas there are only a linear number of them for a MoP seems quite obvious, merely by counting the number of hidden states configurations. I understand this is far from a proof but this is to me more representative of the fact that one does not want to use the hidden states as a new representation for a MoP, which we already knew.  Additionnally, the paper is very heavy on definitions and gives very little intuition about the meaning of the results. Theorem 29 is a prime example as it takes a very long time to parse the result and I could really have used some intuition about the meaning of the result. This feeling is reinforced by the length of the paper (18 when the guidelines mentioned 9) and the inclusion of propositions which seem anecdotal (Prop.7, section 2.1, Corollary 18).  In conclusion, this paper tackles a problem which seems to be too contrived to be of general interest. Further, it is written in an unfriendly way which makes it more appropriate to a very technical crowd.  Minor comments: - Definition 2, you have that C is included in {0, 1}^n. That makes C a vector, not a set. - Proposition 8: I think that G_3 should be G_4.",0,5054
"An interesting workshop paper. For such a provocative title, more results are needed to support the conclusions. Part of the resurgent success of neural networks for acoustic modeling is due to making the networks “deeper” with many hidden layers (see F. Seide, G. Li, and D. Yu, 'Conversational Speech Transcription Using Context-Dependent Deep Neural Networks', ICASSP 2011 which shows that shallow networks perform worse than deep for the same # of parameters). This paper provides a different data point where a shallow network is trained using the author’s “MIMIC” technique performs as well as a deep network baseline on the TIMIT phone recognition task. The MIMIC technique involves using unsupervised soft labels from an ensemble of deep nets of unknown size and quality, including a linear layer of unknown size, and training on the un-normalized log prob rather than softmax output. The impact of each of these aspects on their own is not investigated; perhaps a deep neural network would gain from some or all of these MIMIC training steps as well.The authors show that a shallow neural net trained to mimic a deep net (regular or convolutional) can achieve the same performance as the deeper, more complex models on the TIMIT speech recognition task. They conclude that current learning algorithms are a better fit for deeper architectures and that shallow models can benefit from improved optimization techniques. The experimental results also show that shallow models are able to represent the same function as DNNs/CNNs. To my knowledge, training an SNN to mimic a DNN/CNN through model compression has not been explored before and the authors seem to be getting good results at least on the simple TIMIT task. It remains to be seen if their technique scales up to large vocabulary tasks such as Switchboard and Broadcast News transcription. This being said, a few critiques come to mind: - The authors discuss factoring the weight matrix between input and hidden units and present it as being a novel idea. They should be aware of the following papers: T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy and B. Ramabhadran, 'Low-Rank Matrix Factorization for Deep Neural Network Training with High-Dimensional Output Targets,' in Proc. ICASSP, May 2013. Jian Xue, Jinyu Li, Yifan Gong, 'Restructuring of Deep Neural Network Acoustic Models with Singular Value Decomposition', in Proc. Interspeech 2013. - It is unclear whether the SNN-MIMIC models from Table 1 use any factoring of the weight matrix. If yes, what is k? - It is unclear what targets were used to train the SNN-MIMIC models: DNN or CNN? I assume CNN but it would be good to specify. - On page 2 the feature extraction for speech appears to be incomplete. Are the features logmel or MFCCs? In either case, the log operation appears to be missing. - On page 2 you claim that Table 1 shows results for 'ECNN' which is undefined.",0,5055
"The paper is about various ways of training convolutional neural networks (CNNs) using multiple GPUs attached to the same machine.  I think it is sufficiently interesting for the conference track.  The authors may not be aware of all relevant prior work, but they can fix this easily.  I think the paper should definitely be accepted because Facebook is growing in this area right now, and conference-goers will be wanting to talk to the presenters about what's going on there and what opportunities there are.  There are a couple of papers I think the authors should	be aware of; the titles	are  'Asynchronous stochastic gradient descent for DNN training' 'Pipelined Back-Propagation for Context-Dependent Deep Neural Networks'  Also I know that Andrew Ng's group was doing some work on model parallelism for CNNs.  Andrew Maas (Andrew Maas <amaas@cs.stanford.edu>) would be able to tell you who it was and forward any relevant presentations.Problem is clearly important, but paper is light on details, data sets, which gpu's, etc. All such things matter when judging the speed-up. For example, if you used an older gpu, it's easier to get a speed-up because the trade-off between the gain of multiple gpu's vs. the communication overhead is clearly different.  CNN's for audio processing was done in 2012 by Abdel-Hamid. I would recommend to include this reference: Abdel-Hamid, Ossama, et al. 'Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition.' Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012.  Multi-GPU architectures for non-convolutional networks were discussed in: Xie Chen, Adam Eversole, Gang Li, Dong Yu, and Frank Seide, Pipelined Back-Propagation for Context-Dependent Deep Neural Networks, in Interspeech, ISCA, September 2012  I don't really see things that are new. Model and Data parallelization was tried in Chen'2012, and the extension for CNN's are obvious. Also, which layer to parallelize depends really on the network structure. For example, if you have a very large output layer with 128k nodes, you might be better off parallelizing the output layer.",1,5056
"This abstract investigates the idea of learning an object detection model that does not depend on the class, hence being able to generalize to any number of classes, including classes unknown at training time. The idea is compelling but the paper is short on details and results. We don't know how many bounding boxes are used in the softmax, we don't have the details about the Gaussian used to smooth the targets of the softmax (how picky is it?); The paper does not compare to similar approaches (like Szegedy et al 2013). I feel like this is an interesting idea relevant for the workshop track and hope it will be improved later with more details.This is an interesting paper that shows that using a pretrained object classification network's weights to initialize an object localization network leads to improved performance of the latter.   Of course the comparisons are made for a fixed (but unspecified) architecture. It seems that the architecture chosen is that of an object classifier, and it is not clear at all that this is a good architecture for an object localizer. (In particular pooling is sensible for the former but makes less sense for the latter). Thus it's not really clear that the gains are that useful - perhaps it would just be better to design a network for the task in hand. That is not addressed in this paper at all.  The writing itself is clear, but there are a several significant omissions in the paper.    Table 1 caption is unclear, and seems broken.  Section 4 'multiple GPUs' could be elucidated a little more?   You say the output for your bounding box training is discretized, but don't say how. It's not even clear if you output one-hot in the cross-product space or discretize each dimension separately.  While your main result is a relative improvement, this is still a fundamental omission that must be rectified. It's hard to interpret your results without knowing the resolution or the scoring system that you use.   You don't explain the parameters of the Gaussian used for the labels, nor what you mean by 'multiple bounding boxes' - You've not described that possibility. Are 7% of objects labelled with bounding boxes, or only 7% of images?  Where you have multiple objects in an image are you guaranteed to have the bounding boxes for all of them if you have any of them? (I presume not, since 'what is an object' is ambiguous - but you don't discuss this ambiguity).  Figure 1 legend says 'without bounding boxes' but I presume you mean 'excluding the held-out set' - you train with the other 900 classes' bounding boxes?   You don't discuss the details of training at all, (not even the number of nodes in the hidden layers or convolution windows, never mind learning rates etc) nor make it clear just how much is the same as the other references (Coates / Krizhevsky) you cite. ('Similar to' Krizhevsky is all you say) In particular you need to explain how you transition from pretraining to 'training' - initialization of the softmax weights is the same as starting from scratch? What about: Learning rate / momentum & their schedules ?   Bibliography is broken everywhere e.g. with commas separating first and last names as well as authors' names from each other.  First reference has no source.  Junk like this: 'In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. .... 2009'  The first page indicates that this paper is in Proc ICML and in JMLR. I presume this is just a LaTeX oversight?",1,5057
"Summary ------------  The paper explores running A-SGD as an approach for speeding up learning. Overall I think these are very interesting and informative results. Specifically for a workshop paper I believe the paper contains enough novelty and empirical exploration.  Comments: --------------  It would be interesting to try to quantify how much the size of the model influences these results. In particular I'm wondering of how the performance drops with the size of the gradients that need to be send over the network.   Another interesting plot will be to look at the size of the minibatch and how that influence convergence.  I hypothesis that distributed algorithms where the parallelism is made over the data (rather than model), like it is done here at the node level, will benefit a lot more from complicated optimization techniques rather that SGD (even in its asynchronous version). It feels to me that with large models there is a high price to pay for sending the gradients over the network (case an point, n_sync is usually set to something higher than 1). We want to use an algorithm for which each step is itself expensive (and hence we have to send fewer gradients over the network) but that needs much less steps to converge. You can make each step mSGD arbitrarily expensive by increasing the minibatch size, though SGD is fairly inefficient at utilizing these large minibatches. I believe that distributing computation along data for deep models makes a lot more sense with algorithms such as second order methods or variants of natural  gradient.Multi-computer GPU training of large networks is an important current topic for representation learning in industry-scale convnets. This paper describes ongoing efforts to combine model parallelism and data parallelism to reduce training time on the ILSVRC 2012 data set.  Pro:  - they achieve several-fold reductions in runtime over Khrizhevsky's landmark implementation  Con:  - I'm not sure that there is significant novelty in their approach, relative to dist-Belief and existing work on asynchronous-SGD.",1,5058
"This paper introduces an approach to finding near-MAP solutions in binary Markov random fields. The proposed technique is based on an SDP relaxation that is re-parameterized and solved using constrained gradient-based methods. The final step involves projecting the solution using a random unit-length vector and then rounding the resulting entries to the vertices of a hypercube. This stochastic process defines a sampler that empirically produces lower-energy configurations than Gibbs sampling. The method is simple and seems to perform well for approximate MAP estimation, although it is not clear whether this approach will be useful for estimating the partition function.  I liked the result in Figure 1, although the entropy of the rrr-MAP method is much higher in the learned RBM than the one with random weights. Is there any particular reason for this? Also, what would the result be if the Gibbs sampling were performed at different temperatures?  Since the optimization problem itself is non-convex, it would seem that the results would also depend on the quality of the solution to the optimization problem. Is there much variance between optimization runs? What about the effect of the rank of X?  I think that the results should also be compared on a small RBM where the exact MAP solution can be found in a reasonable amount of time by Gurobi.  Perhaps a good test would be on RBMs (or general binary MRFs) with non-negative edge weights. These are submodular and can therefore be globally optimized efficiently. This would serve as a good basis for comparison to other local-search methods.The paper introduces a gradient procedure for map estimation in MRFs and RBMs, which can also be used to draw approximate samples and therefore estimate partition functions.    Pros:  the method is very novel in the context of RBMs, and it seems to work quite well, beating Gibbs-sampling almost every time.  It is also useful for estimating partition functions.  Cons:  the method was able to correctly estimate the partition function of an MNIST RBM well.  But MNIST has few modes.  It would be very interesting to try and estimate the partition function of an RBM with many more modes, and to compare it with other methods (such as AIS).  The approximation of (14) can be quite bad if p_X is very different from the RBM's distribution.   I wonder if this method can be applied to general energy-based models, like the ones used in contrastive backpropagation.",1,5059
"This paper addresses the problem of detecting people in the front passenger seat of a car as a step along the way to help enforce carpooling rules. The paper presents experiments comparing different approaches. In particular the paper explores solving the problem through using: a) Zhu and Ramanan's deformable part model for face detection, and using the detections for the final result, b) the use of Fisher Vectors in the sense of Jaakkola and Haussler, (1998), c) a variant of the widely used bag of visual words (BoW) representation, and d) a technique referred to as Vectors of Locally Aggregated Descriptors (VLAD). Techniques b), c) and d) use a traditional SVM approach for the final classification. The Fisher vector technique appears to have better performance compared to the other methods explored in this paper.  The paper looks at a concrete practical application and explores ways for solving the problem that are fairly in line with current practices in computer vision. The face detection based comparison is certainly important and interesting. However, it is not clear from the current manuscript exactly how the model of Zhu and Ramanan was trained for the experiments here. Was the face detector trained on the training set defined here? Or, was the face detector used with the pre-learned parameters as distributed on their website? The performance could be dramatically different depending on how the method was trained. The size of the test train splits for the other experiments are also not given and the issue of SVM hyper-parameter tuning is not discussed.  The Fisher feature vector technique presented here could be viewed as a form of representation learning, but this is really more of a vision application and method comparison paper as opposed to a paper deeply exploring aspects of representation learning. The paper also has some language problems.This paper is about classifying the presence or absence of a person on the front seat of a car. The main point of the paper is to compare the approach of using face detection and directly classifying the front seat image. They improve accuracy from 92% to 96% with full image classification. It also compares different aggregation methods on top of hand-designed features like SIFT: bag of words, fisher vector or VLAD, and shows better results using Fisher vectors. Other work has also been using more than just face detection but not the entire window itself. The novelty of this work is very limited and is mostly in the trivial idea of using the entire passenger image for classification.  Pros: - improving accuracy to 96% over face approach (92%).  Cons: - no novelty. - no representation learning.",0,5060
"Learning Information Spread  The ms considers the interesting question of diffusion and information spreading in content networks. The modeling is performed by a diffusion kernel with ranking and classification constraints; having both is an innovation.   While the paper is generally clear, I miss details on how this is exactly done and how this is optimized. The optimization problem seems nontrivial, so it would be nice to know the computational effort. While first experiments show encouraging results, it is unclear whether in a simple toy problem the proposed estimation algorithm will find the ground truth consistently.  Furthermore no comparison to other models that consider information spread are given, neither in speed/scaling nor accuracy/severity of errors. Concluding, the paper is interesting but somewhat preliminary; it consists in a small increment over Bourigault et al. 2014 and lacks many  details.This work proposed an extension of the content diffusion kernel model by adding an additional classification constraint. My main concern of the current version of this work is whether the classification constraint is proper for this task (modeling the spread of information in social network):  The information spread could not only depended on the proximity of users but also on the content of information. In other words, one specific user could not be contaminated by the source in one cascade but rather in other cascades. If that's the case, the classification constraints will not be satisfied and the learning cannot converge; This classification constraint could also undermine the performance of ranking. It is indeed shown in Table 1.   Pros -- well-written and organized  Cons --the proposed classification constraint might not be suitable for modeling the spread of information in social networks.",1,5061
"This paper presents an empirical study of the variance in gradient estimates between contrastive divergence (CD) and persistent contrastive divergence (PCD). It is well known that PCD tends to be less stable than CD, requiring a larger learning rate and larger mini-batch sizes. The paper does a fairly good job of empirically verifying this phenomenon on several image datasets, and most of the results are consistent with expectations. The observation that the variance increases toward the end of learning is an interesting and not entirely obvious finding.  One issue though is that the paper seems to miss a crucial part of the story: CD learning enjoys a low variance at the cost of an increase in bias. It is easy to construct a gradient estimate that exhibits zero variance, however practically speaking this would not be very useful. What is more interesting is the trade-off between bias and variance. For example, PCD exhibits significant variance on the silhouettes dataset. Does this mean that it requires an impractically small learning rate?  It has been shown in the past that the technique of iterate averaging can be used to remove much of the variance in PCD learning, but that it does not work nearly as well when applied to CD [1]. The fact that PCD is asymptotically unbiased, but exhibits high variance compared to CD supports these results.   [2] should be cited for PCD as well.  References: [1] Kevin Swersky,  Bo Chen, Benjamin Marlin, and Nando de Freitas, “A Tutorial on Stochastic Approximation Algorithms for Training Restricted Boltzmann Machines and Deep Belief Nets,” Information Theory and Applications Workshop, 2010.  [2] Laurent Younes, “Parametric inference for imperfectly observed Gibbsian ﬁelds,” Probability Theory and Related Fields, vol. 82, no. 4, pp. 625–645, 1989.This paper provides an empirical evaluation of the variance of maximum likelihood gradient estimators for RBMs, comparing Contrastive Divergence to Persistent CD. The results confirm a well known belief that PCD suffers from higher-variance, than the biased CD gradient. While the result may not be surprising, I believe the authors are correct in stating that the issue had not properly been investigated in the literature. It is unfortunate however that the authors avoid the much more important question of trade-off between bias and variance. Before making a final judgement on the paper however, I would ask that the authors clarify the following potential major issue. Other more general feedback for improving the paper follows.   Request for Clarification:  Why the asymmetry between the estimation of CD-k vs PCD-k gradients ? The use of PCD-k is highly unusual. If the goal was to study variance as a function of the ergodicity of a single Markov chain, then PCD-k gradients should have been computed with a single training example (for the positive phase) and computing the negative phase gradient by *averaging* over the k-steps of the negative phase chain.  Could the authors clarify (through pseudocode) how the gradients and their variance are computed for the experiments of Figure 2?  Due to the loss of ergodicity of the Markov chain, the effective number of samples used to estimate the model expectation should indeed be larger at 10 epochs, than at 500 epochs. It is thus predictable that variance of the gradient estimates would increase during training. However, this is for a fixed value of k. I find very strange that the variance would *increase* with k (at a fixed point of training). I am left wondering if this is an artefact of the experimental protocol: the authors seem to be computing the variance of the *sum* of k-gradient estimates. This quantity will indeed grow with k, and will do so linearly if the estimates at each k are assumed to be independent. The linearity of the curves in Fig.2 gives some weight to this hypothesis.   Other general feedback:  * One area of concern is that the paper evaluates PCD in a regime which is not commonly used in practice: i.e.  estimating the negative phase expectation via the correlated samples of a single Markov chain. I worry that some readers may conclude that PCD is not viable, due to its excessively large variance. For this reason, I think the paper would benefit from repeating the experiments but averaging over M independent chains.  * A perhaps more appropriate baseline, would be to run M >> 1 independent Markov chains to convergence and average the resulting gradient estimates. This might not change much, but the above would yield a better estimate of the ML gradient than CD-1000.  * Evaluating the variance of PCD gradients on a model trained via CD may be problematic. The mixing issues of PCD can be exacerbated when run on a CD-trained model, where the energy has only been fit locally around training data (Desjardins, 2010). While I do not expect the conclusions to change, I would be interested in seeing the same results on a PCD-k trained model.  * RE: I-CD experiments. 'This supports the hypothesis that the low variance of CD [stems from] the negative particle [being] sampled from the positive particle, and not from that the negative particle is sampled only a limited number of steps from an arbitrary data point'.  I am not sure that the experiment allows you to draw this conclusion. When computing the 10 gradient estimates (for each training example) did you initialize the Markov chain from a random (but fixed throughout the 10 gradient evaluations) training example ? Otherwise, I believe the conclusion is rather uninteresting and doesn't shed light on the 'importance' of initializing the negative chain from the positive phase training data.  In CD-training, the only variance stems from the trajectory taken by the (short) Markov chain from a fixed starting point. In I-CD, there are two sources of variance: (1) the trajectory of the chain, and (2) the starting point of the chain. If the chain is initialized randomly for the 10 gradient evaluations, then this will undoubtedly increase the variance of the estimator (but with lower bias).   Clarity:  * In I-CD, the 'negative particle is sampled from a random positive particle' ? I would make explicit that you initialize the chain of I-CD from a random training example. In Section 4, 'arbitrary data point' left me wondering if you were instead initializing the chain from an independent pseudo-sample of the model (using i.e. a uniform distribution or a factorial approximation to p(v)).  * 'Conversely, the variance of the mean of subsequent variance estimates using PCD is significantly higher' ? Did the authors mean 'the variance of the mean of subsequent gradient estimates' ? Otherwise, please consider rephrasing.",0,5062
"* A brief summary of the paper's contributions, in the context of prior work. Paper describes experiment of feature extraction with wavelets applied to CALTACH dataset.  * An assessment of novelty and quality.  This is just a single numerical result. It gives some insight, but there is no novelty.  pros : It is good that someone have done experiment on how powerful are wavelets on bigger dataset.   cons : Very little of added value, small amount of content.I am not very familiar with scattering transform work so I cannot judge of the novelty of using 2 layers of wavelet transforms for classification. However the results are impressive in that they do not use any learning and still beat the best ImageNet-pretrained convolutional network on Caltech 101 when using 1 or 2 layers. It does not however on Caltech 256 and some insight into why that might be would have been nice.  pros: - good results with small number of layers  cons: - no experiment with more layers, does it degrade drastically beyond 2 layers?",0,5063
"This paper proposed a new learning algorithm for feedforward neural networks that is motivated by Self Organizing Maps (SOM) and Learning Vector Quantization (LVQ). The paper proposes a way for unsupervised pre-training of network weights followed by a supervised fine-tuning.  The paper lacks discussions of clear motivation behind the work, i.e. shortcomings of existing training methods and how the proposed method overcome them. The description of the proposed method still needs a lot of work. More details are needed to clarify the proposed system. For example, in equation (1), d and sigma are not defined. For x_adv and x_target, which one is the input and which one is the label. What are the motivations for the update rule in equation (1) and equation (2)?.  Pre-training and SOM are described for the first time in the experiment section while they should appear in earlier sections of the paper. The authors only experiment with MNIST and don’t compare their systems to other good baseline systems on MNIST.The paper proposes a modified learning rule for competitive learning in multilayer neural networks. The proposed learning algorithm is not clearly explained, in particular the meaning of the variables x_adv and x_target. The variable x_adv is defined to be the 'advance input', but I do not understand what advance input exactly is. Apart from these clarity issues, the method seems relatively elaborated with a pretraining stage and an architecture consisting of a hierarchy of self-organizing maps.  In the conclusion, the authors claim that the method improves the accuracy of the classification task. However, it is unclear which improvement, as the reported accuracy of approximately 90% on MNIST is lower than previously published results. No further details are given on the exact setting of the experiment.",1,5065
"The paper presents a cascaded architecture that successively transforms an input representation into a sparse code, and then transforms the sparse code into a compact dense representation, making sure adjacent patches get similar dense representations. The resulting representations are passed through a spatial pyramid pooling mechanism and concatenated, and then fed to a linear SVM to learn to classify images. This architecture achieves better or similar results to other sparse coding approaches on 3 small image datasets. The paper reads quite well (the first introductory sections are really nice summary of past work in the sparse coding and dimensionality reduction domains). I like the idea of making sure that two sparse codes encoding overlapping regions should have a similar dense code. That said, I wonder why we need to use intermediate representations as input to the final SVM, and not just the last layer. I found section 3.3 less interesting as it was an obvious result (to me at least), and section 3.4 daunting as it meant two more hyper-parameters to tune in our lives... Overall, I liked the paper and would have liked to see results on one larger image dataset: the caltech-xxx datasets are quite outdated, and I'm worried results would not scale to larger and more recent datasets.Unsup feature learning by deep sparse coding  This paper alternates sparse-to-dense (dimensionality reduction by learning an invariant mapping: DRLIM) and dense-to-sparse (standard sparse coding) modules to produce a multi-layer image representation of images. Compared to earlier deep image recognition architectures using sparse modules and pooling modules, the proposed system is more general and displays better performance on image recognition benchmarks.  The paper itself is clear and well-motivated. While none of the building blocks is new (DRLIM blocks, sparse coding blocks, etc), the combination is novel and works well.  Experiments on Caltech 101, Caltech 256, and Scenes 15 show that the new architecture performs better than earlier versions without this sparse-to-dense mapping. These are bit limited now that better datasets are widely used (e.g., imagenet), but the experiments are interesting and show that the new system allows for deeper training without increasing the dimension of dictionary used for sparse coding. I think other researchers would be interested in these results.  There are a few writing problems (e.g. 'it is important to emphasis' instead of 'emphasize') so please spell check, but this does not hinder understanding. Also, it would be simpler to write 'First' rather than 'first of all' as is done in several places.  Overall this paper presents a more principled variant for a deep image recognition architecture. The datasets used are limited but show that this system has promise.This paper alternates sparse-to-dense (dimensionality reduction by learning an invariant mapping: DRLIM) and dense-to-sparse (standard sparse coding) modules to produce a multi-layer image representation of images. Compared to earlier deep image recognition architectures using sparse modules and pooling modules, the proposed system is more general and displays better performance on image recognition benchmarks.  The paper itself is clear and well-motivated. While none of the building blocks is new (DRLIM blocks, sparse coding blocks, etc), the combination is novel and works well.  Experiments on Caltech 101, Caltech 256, and Scenes 15 show that the new architecture performs better than earlier versions without this sparse-to-dense mapping. These are bit limited now that better datasets are widely used (e.g., imagenet), but the experiments are interesting and show that the new system allows for deeper training without increasing the dimension of dictionary used for sparse coding. I think other researchers would be interested in these results.  There are a few writing problems (e.g. 'it is important to emphasis' instead of 'emphasize') so please spell check, but this does not hinder understanding. Also, it would be simpler to write 'First' rather than 'first of all' as is done in several places.  Overall this paper presents a more principled variant for a deep image recognition architecture. The datasets used are limited but show that this system has promise.",0,5066
"This paper presents methods for visualizing the behaviour of an object recognition convolutional neural network. The first method generates a 'canonical image' for a given class that the network can recognize. The second generates a saliency map for a given input image and specified class, that illustrates the part of the image (pixels) that influence the most the given class's output probability. This can be used to seed a graphcut segmentation and localize objects of that class in the input image. Finally, a connection between the saliency map method and the work of Zeiler and Fergus on using deconvolutions to visualize deep networks is established.  While I'm not impressed by the per-class canonical image generation (which isn't very original anyways, given the work of Erhan et al.), I think the saliency map method is quite interesting. In particular, the fact that it can be leveraged to obtain a decent object localizer, which is only partially supervised, seems impressive. This is probably the most interesting part of the paper. As for the connection with deconvolution, I think it's also a nice observation.  As for the cons of this paper, they are those you expect from a workshop paper, i.e. the experimental work could be stronger. Specifically, I feel like there is a lack of quantitative comparisons. I wonder whether other alternatives to the graphcut initialization could have served as baselines with which to compare quantitatively (but this isn't my expertise, so perhaps there aren't...). The fact that one of their previous systems (which was fully supervised for localization) actually performs worse than this partially supervised system is certainly impressive however!Deep convolutional neural networks (convnets) have achieved tremendous success lately in large-scale visual recognition. Their popularity has exploded after winning recent high-profile competitions. As more research groups begin to experiment with convnets, there is increasing interest into what is happening *inside* the convnet. A few papers have been published within the last year providing ways of visualizing what units inside the convnet represent, and also visualizing the spatial support of a particular class. This paper presents two methods for visualization of convnets: one, based on an approach by Erhan, simply backpropagates the gradient of the class score with respect to the image pixels to generate class appearance models. The other method, class-saliency maps, visualize spatial support for a class and are also used to perform weakly supervised object localization.  The authors are very clear about their contributions, mainly in producing understandable visualizations. In terms of novelty, the method by which one obtains the class appearance models have been used in the unsupervised learning context by Erhan et al., but this paper is the first to apply the technique to convnets. The method by which to obtain the class saliency maps is intuitive and produces reasonable visualizations. I found the weakly supervised object localization application the most impressive part of the paper. Although it does not perform nearly as well as methods that consider localization part of training, it's promising to see how localization can be learned without bounding boxes.  Pros: * Clear, simple * Provides a useful, practical tool for convnet practitioners * The discussion re: Zeiler and Fergus' Deconvolutional net method clears up any misunderstanding among the two methods which do seem pretty similar * Evaluated on large-scale data (ILSVRC-2013)  Cons * Though the similarity to the Deconvolutional net method is acknowledged, the technical contribution of this work is not a massive departure from the other work (though suitable for workshop track)  Overall, I think this is a good workshop paper. The methodology does not depart far from previous work but the weakly supervised localization is interesting and will generate interest at the conference.  Comments ========  Further insight/discussion on the implication of the change in treatment between the Deconvolutional net method and the proposed method is suggested.",0,5067
"This paper investigates the use of so-called distributional models of words to improve the quality of learned word embeddings. These models are essentially high-dimensional vector representations of words, which indicate with what other words in the vocabulary has a word cooccurred (within some context). Since word embeddings can be understood as a linear lower-dimensional projection of the basic one-hot representations of words, these distributional model representations can be exploited by trainable word embeddings algorithms by simply concatenating them to the basic 'one-hot' representation or by replacing the one-hot representation by the distributed one for infrequent words. This paper shows that this approach can improve the quality of the word embeddings, as measured by an average correlation with human judgement of word similarity.  Overall, I think this is a good workshop paper. Learning good words embeddings is an important topic of research, and this paper describes a nice, fairly simple trick to improve word embeddings.   One motivation for using it seems to be that infrequent words will not be able to move far enough away from their random initialization, so I wonder whether initializing all these word embeddings to 0 instead might have been enough to solve this problem. I think this would be a good baseline to compare with.   I would also have liked to see whether any gains are obtained in a real NLP task, i.e. confirm that better correlation with human judgement is actually giving us something in practice.  But these are overall minor problems, for a workshop paper.This paper proposes to derive distributional representation for words that can be used to improve word embeddings. Distributional vectors can present to the neural network that learns embeddings, instead of presenting one-hot vectors. One motivation is that distributional representation could make the learning task easier for rare words. The authors apply this approach only to rare words since word embeddings for frequent words is frequently updated and then can be considered as satisfactory.  The idea is nice. However, my main concern is about the experimental part. I don't understand the results. For the 'WordSim' task, the paper of E. Huang (ACL2012) exhibits spearman correlation above 50. So wether the results are incredibly below the baseline systems used in 2012 (and thus, what can we conclude from this paper since it is straightforward to improve a very poor system), or this need clarification. Anyway, baseline exists and should be mentioned.  A minor comment about the last paragraph of the introduction. The paper (Hai Son Le et al. at EMNLP2010) addressed the issue of the initialization of word embeddings and this seems to perform quite well especially for rare words.",1,5068
"The authors present a complete hybrid system for recognition characters and words from a real world natural scenes. The clue idea is to cede word-to character + character classification and segmentations correction into three convolutional neural networks with maxout non-linearity. Word-to character model makes and additional use of HMM to better deal with sequential aspect of character segmentations across the words.  I think the community can benefit from this work as it shows some interesting experiments, and what is even more important, does it in the context of the complete system. Experimental aspect is sufficient and explore various sub-problems the potential Reader could be interested in, for example, the use and impact of lexicon and different language models on the final system accuracy. It's also important - the presented solution gives a state of the art accuracy  To summarize, I think the authors put a lot of effort into this work and present some nice experimental results.  Suggestions for improvements:  - I would appreciate an implicit distinction between likelihoods and probabilities, for example, by p and P respectively. That applies to all paper content starting from the equation 1 and including occasional in-text references to certain probabilistic quantities.  - (thing to consider) Section 3.4 argmax_Qp(Q|O) - isn't HMM producing argmax_Q p(O|Q) - i.e. the likelihood of state sequence Q producing the observation sequence O? If you agree with this comment, you need also to fix it in the last paragraph of sec 5.1.This paper presents a system for text recognition from natural images that leverages recent advances in deep learning. Contrarily to previous methods that often focus on a single aspect, this work addresses all simpler sub-problems and incorporates classifiers into different sub-modules of the whole system. The resulting method achieves impressive results and seems computationally efficient. The paper is very well written, comprehensive and does a good job at condensing a lot of information into 9 pages.  A minor weakness concerns the novelty aspect: the paper mostly reuses existing algorithms, such as convolutional maxout networks, hybrid HMMs, beam search, MSER. What the authors call 'Cascade Beam Search' is in fact ordinary beam search. However, the pipeline is novel and produces good results and insightful discussion, especially about the trade-offs involved.   - Section 5.3 (especially Algorithm 1): I found the notation confusing. Can you define Q_i in words? are those implemented with priority queues? What is the difference between intervals s_i and v_i?  - Section 5.4: The 55.6% figure is obtained for which dataset? Why not use both a lexicon and a language model? What happens when the minimal edit distance is reached for more than one word?  - The authors' main movitation for a language model is to achieve 'constant time in lexicon size per query', while the edit distance technique is slower but apparently has a better accuracy. Would it be possible to use hash tables to improve accuracy whenever the minimal edit distance is zero?",0,5069
"This paper extends the mixture-of-experts (MoE) model by stacking several blocks of the MoEs to form a deep MoE. In this model, each mixture weight is implemented with a gating network. The mixtures at each block is different. The whole deep MoE is trained jointly using the stochastic gradient descent algorithm. The motivation of the work is to reduce the decoding time by exploiting the structure imposed in the MoE model. The model was evaluated on the MNIST and speech monophone classification tasks.  The idea of deep MoE is interesting and, although not difficult to come out, is novel. I found the fact that the first and second blocks focus on distinguishing different patterns is particularly interesting.   However, I feel that the effectiveness and the benefit of the model is not supported by the evidence presented in the paper.   1.	It’s not clear how or whether the proposed deep MoE can beat the fully connected normal DNNs if the same number of the model parameters are used (or even when deep MoEs use more parameters). A comparison against the fully connected DNN on the two tasks is needed. In many cases we don’t want to sacrifice accuracy for small speed improvement.  2.	It’s not clear whether the claimed computation reduction is true. It would be desirable if a comparison on the computation cost between the deep MoE and the fully connected conventional DNN is provided when both the number of classes is small (say 10) and large (say 1K-10K). The comparison should also consider the fact that the sparseness pattern in the deep MoE is random and unknown beforehand may not save computation at all when SIMD instructions are used. 3.	It is also unclear whether deep MoE performs better than the single-block MoE. It appears to me, according to the results presented, the deep MoE actually performs worse. The concatenation trick improved the result on the MNIST. However, from my experience, the gain is more likely from the concatenation of the hidden features instead of the deep architecture used.  There is also a minor presentation issue. The models on row 2 and 3 are identical in Table 2 but the results are different. What is the difference between these two models?The paper extends the concept of mixtures of experts to multiple  layers of experts. Well, at least in theory - in practise authors stopped their experiments at only two such layers - which somehow invalidates the use of buzzy 'deep' word in the title - does more (than two) layers of mixtures still help?  The clue idea is to collaboratively optimise different sub-networks representing either experts or gating networks. Authors propose also 'the trick' to effectively learn mixing networks by preserving too rapid selection of dominant experts at the beginning of training stage.  It's hard to deduce whether presented idea gives a real advantage over, for example, usual -- one or two hidden layers feed-forward networks with the same total number of parameters. Perhaps, I am also missing something important here -- but was there any good reason for using the Jittered MNIST instead of the MNIST itself? In the end both are just toy benchmarks while the latter gives you the ability to cite and compare your work to other many other reported results. If you did that, not doing some basic baselines by yourself would be OK.  I've got similar comments to the monophone voice classification. On the top I've already written for MNIST I do not see the need to use simplified  proprietary database. It would be better to do the experiments in TIMIT benchmark and then cite other works that reports frame accuracy (where a single frame is a monophone) so the reader could get a bit wider picture of how your work fits into broader perspective.  Anyway, idea is sufficiently novel and interesting and I am in favour of  accept. Perhaps the authors could at least improve MNIST experimental aspect.The paper introduce a deep mixture of experts model which contains multiple layers each of them contains multiple experts and a gating network. The idea is nice and the presentation is clear but the experiments lack proper, needed, comparisons with baseline systems for the Jittered MNIST and the monophone speech datasets.  As the authors mentioned in conclusion, the experiments use all experts for all data points which doesn’t achieve the main purpose of the papers, i.e. faster training and testing. It is important to show how does this system perform against a deep NN baseline with the same number of parameters in terms of accuracy and training time per epoch. Regarding the speech task. What is the error you are presenting in Table 2, is it the Phone or Frame error rate?",1,5070
"The authors propose a model that takes a set of events (written as English text) as input, and outputs the temporal ordering of those events. As opposed to a previous DAG based method (also used as a baseline here), in this work words are represented as vectors (initialized with Collobert's SENNA embeddings) and are input into a two layer neural net whose output is also a vector embedding.  The output is then taken as input to an online ranking model (PRank) and the whole thing (including the word vectors) are trained using backprop.  A dataset containing short sequences of events (e.g. the process of making coffee) gathered for previous work using MTurk is used for train and test.  The proposed embedding method shows a substantial improvement over the DAG baseline.  This is interesting work, I thought the execution was good, and the results are impressive. I only have a few suggestions/questions: first, why, in the abstract and elsewhere, do you claim to be using unlabeled data?  The data is labeled by order of events (by MTurkers), is it not?  I suspect that you mean that no further labeling was done, but this is confusing.  Second, your model (Fig. 1) shows one predicate (i.e. verb) and two arguments (i.e. nouns), but some of the examples from the ESD data are more complex (e.g. 'fill water in coffee maker').  How are these more complex phrases mapped to your model? Finally, you use a lot of space on previous work; I think that the paper would be improved by adding more details on your method, and shortening the previous work sections (1 and 2.1) by better focusing them.  A minor issue: at the end of Section 1, some unnecessary extra space has been inserted.This paper investigates a model which aims at predicting the order of events; each event is an english sentence. While previous methods relied on a graph representation to infer the right order, the proposed model is made of two stages. The first stage use a continuous representation of a verb frame, where the predicate and its arguments are represented by their word embeddings. A neural network is used to derive this continuous representation in order to capture the compositionality within the verb frame. The second stage uses a large margin extension of PRank. The learning scheme is very interesting: the error made by the ranker is used to update the ranker parameters, but is also back-propagated to update the NN parameters.   This paper is well written and describes a nice idea to solve a difficult problem. The experimental setup is convincing (including the description of the task and how the learning resources were built).  I only have a few suggestions/questions.  For a conference that is focused on representation learning, it could be interesting to discuss whether the word embeddings provided by SENNA need to be updated. For instance, the authors could compare their performances to a system where the initial word embeddings are fixed. Moreover, the evaluation metric is F1, but how the objective function is related to this metric. Maybe a footnote could say a few words about that and I'm curious to see how the objective function evolves during training. The ranking error function is quite similar to metrics used in MT for reordering evaluation (see for instance the work of Alexandra Birch in 2009).",0,5071
"The paper presents an approach for learning the filters of a convolutional NN, for an image classification task, without making use of target labels. The algorithm proceeds in two steps: learning a transformation of the original image and then learning a classifier using this new representation. For the first step, patches are sampled from an image collection, each patch will then correspond to a surrogate class and a classifier will be trained to associate transformed versions of the patches to the corresponding class labels using a convolutional net. In a second step, this net is replicated on whole images leading to a transformed representation of the original image. A linear classifier is then trained using this representation as input and the target labels relative to the image collection. Experiments are performed on different image collections and a comparison with several baselines is provided. This paper introduces a simple idea for feature learning which seems to work relatively well. The paper could be easily improved or extended in several ways.  A natural extension would be to tune the learned filters using the target labels, which would allow a comparison with state of the art supervised techniques. This method might be less expensive for training than some of the alternatives, but the complexity issues are not discussed at all. The choices made for the convolutional net produce very dense codes. This could be discussed and a comparison with alternatives, e.g. larger filter size could be provided. Also there could be more practical details like what are the combinations of transformations used for the patches, what is the increase provided by the dropout etc.This paper proposes to reduce the unsupervised feature learning problem to a classification problem by: a) sampling patches at random from (unlabeled) images (in the order of several thousands) and b) creating surrogate classification tasks by considering each patch as a class and by generating several other samples by applying transformations (e.g., translation, rotation, scaling, etc.). The features trained in this manner are used as patch descriptors for to classify images in Caltech 101, CIFAR and STL-10 datasets. The method compares well with other feature learning methods.  The paper reads well and has a clear narrative. The reduction from unsupervised to supervised learning is presented in an intriguing way. On the other hand, this method seems closely related to work in metric learning and it would be nice to have an explicit discussion about this.  Pros + clearly written + simple idea + empirical analysis demonstrates good results  Cons - some baseline experiments are missing, namely   - compare to random filters (i.e., what’s the role played by the architecture used)   - it would be nice to see a comparison of the accuracy after fine-tuning the whole system - prior reference to work in metric learning (neighborhood component analysis, DrLIM style) is not mentioned. One can cast a similar learning problem: making the features of patches belonging to the same “class” be similar, and making the features of patches belonging to different “classes” be as far as possible. I believe that by using a ranking loss on such triplets would yield similar results. Under this view, the paper would become very much similar to: Raia Hadsell, Sumit Chopra and Yann LeCun: Dimensionality Reduction by Learning an Invariant Mapping, Proc. Computer Vision and Pattern Recognition Conference (CVPR'06), IEEE Press, 2006 except that the generation of similar and different patches is produced by transformation known in advance. One advantage of these metric learning approaches is that they naturally scale to an “infinite” number of “classes”.  Minor details: - the schedule on the number of classes seems rather hacky - the overfitting hypothesis in sec. 3.2 could be easily be tested.",0,5072
"This paper explores simple ways to embed linguistic units composed of discontiguous words such as 'HELP TO' in the sentence 'Paul HELPS me TO write my paper'.   The frequency of occurrence of such discontiguous units is very language dependent (high in German, lower in English). The authors propose a method that essentially amounts to rewriting the sentence in a manner that considers such units as a single word and using Mikolov's vec2word code. Experiments show that such embeddings perform better on a simple task, namely classifying entities are animated or non-animated.  In my opinion this is a very preliminary work at this stage.  Neither the claim not the results are very surprising.Summary This paper proposes learning representations for discontinuous pairs of words in a sentence. Representations for such linguistic units such as “helped*to” are potentially more useful than bigrams or other units for particular NLP tasks. Rather than introducing a new algorithm to induce such representations, they alter a text corpus and use a skip-gram training algorithm. Representations are compared against previous word representation approaches on a task of classifying markables.  Review Generally the idea of inducing representations for disjoint linguistic units is novel, and seems to hold good potential. It seems strange to use word2vec which is a skip-gram algorithm to induce such representations. The process of creating fake ‘sentences’ with disjoint units to induce skip grams seems hacky. I would prefer to see a more straightforward approach, such as one based on token-document matrix factorization, to induce representations for the disjoint tokens. The evaluation task is obscure, and why this task is chosen is unclear. The authors should include experimental evaluation, visualization, or controlled experiments on at least one more standard task. Generally, there is a kernel of an interesting idea in this paper but the work needs a more thorough investigation into the representation learning algorithm used and evaluation.  Key points + Interesting linguistic idea + Use of pre-existing word vector learning package makes experiments seemingly easy to reproduce - Using a skip-gram learning algorithm doesn’t make sense. A matrix factorization or other similar approach seems more natural - Non-standard and somewhat difficult to understand evaluation - No visualization or control experiments to understand the learned representations - Paper is short to the point of lacking sufficient descriptions of the evaluation task and conclusions",0,5073
"The paper considers the task of categorizing queries into classes revealing user intent (e.g., weather vs. flight booking). In doing this, the authors exploit query log data (specifically, queries paired with URLs clicked after sending the query to a search engine), however they use no or little supervised data. Instead,  they learn query representations predictive of URLs, and use proximity between a query representation and a category name representation to make the classification decision (word(s) in the category name are also treated as a query).  Additionally they consider a regularizer which favors low entropy predictions on unlabelled queries (with a distance-based probability model). Finally, they also use their representation in supervised learning.   I find the ideas and their implementation interesting and the experiments seem fairly convincing to me as well (however, see (2)).  Comments: 1) The type of regularisation used is similar to entropy minimization  proposed in Grandvalet and Bengio (2004), and can also be regarded as a form of posterior regularization (Ganchev et al., 2010) or generalized expectation criteria (Druck et al., 2008). I think the authors should briefly discuss the relation. 2) The dataset used for testing query classification does not seem to be very standard and actually consists of utterances by users of a spoken dialog system rather than search queries. I am wondering why a more standard dataset is not selected (e.g., KDDCUP). 3) Not being an expert in query classification, I would appreciate some discussion of related approaches, as I assume this is not the first method which considers a semi-supervised approach to this task.  4) Section 5: 'the best class name would have a meaning P(H | C_r) that is the mean of the meaning of all its utterances  E_{X_r | C_r}{ P(H | X_r)} '. I am not sure in what sense it would be the best. If in terms of classification accuracy then this is not entirely correct -- e.g., a single datapoint can move the mean to an arbitrary position and dramatically affect the error rate. I understand what the authors are trying to say here but I think it is a bit vague. 5) The authors limit themselves to using only 1,000 most frequent  URLs when learning the representations, perhaps because they use the sofmax error function. They might consider using 'standard' techniques which avoid summation over the categories (i.e. URLs)  in training: some form of binarization (e.g., Huffman trees) or a ranking error function with sub-sampled negative examples.   The paper would also benefit from some polishing, a couple of points: -- Section 6 (page 5, 1st par): 'However, it is not clear how of a proxy that is for a given task.' ??  -- caption Fig 3: sentence 'ZSL achieves …' does not seem grammatical  Overall, I have a feeling that the authors tend to over-generalize -- it is a nice paper about query classification, and this over-generalization makes it less readable (e.g, see section 6 where 'proxy functions' are discussed).   Pro: -- a clever application of distributed representation learning for an important task -- the approach may have applications in other domains (e.g., in opinion summarization -- learning to categorize online review sentences according to product features) Con: -- the dataset may not be entirely appropriate  -- discussion of related work does not seem quite adequate  -- writing (minor)This paper offers 2 contributions: one confirms that zero-shot learning has some practical use for semantic classification. The second one, about zero-shot clustering is much more original, but unfortunately less mature. When using deep learning for sentence or document-level classification, it has been observed that discriminantly tuning the word embedding significantly improved performance. This paper does such discriminant tuning *without* labeled data, by assuming that the classifier has been obtained through zero-shot learning. They call the method 'zero-shot clustering', and I find it very neat and original.  Unfortunately, this paper seems to have been hastily written . Explanations  are thought-provoking but very idiosyncratic, thus very hard to follow. Experiments  are limited and poorly explained, especially about zero-shot clustering.  Section 5:  Zero-shot learning is introduced in section 5, but I had to get back to the ' Zero-Shot Learning with Semantic Output Codes' paper to understand the idea in its full generality. In particular, it would be useful to formalize the 'intuition' behind equation (2), which corresponds to the 'knowledge base'.   Section 6: It  is the most interesting: it proposes an excellent, and to my knowledge, novel idea, that I understood as soon as I saw equation (3). However, the 3 paragraphs of explanation that precede are so confusing that I nearly suspected deliberate obfuscation of a good idea. The discussion starts by assuming we want to build density  model of the data P(X) like in auto-encoder, and then show this is a bad idea: why bother? What is proposed here has nothing to do with density estimation. This proxy framework is completely cryptic to me: proxy of what? P(C|X)?  What has this to do with the choice of the entropy (excellent by the way)? The link seems to be in the sentence: “The better the proxy function hat{f} the better this measure (H(f(X)) - H( hat{f}(X))^2<= K*(f(X) - hat{f}(X))^2 by Lipschitz continuity).” What this sentence tells us is that we should get P(C|X) as close as possible to the true posterior to get its entropy close to the true entropy? But we are doing the opposite here: minimizing the estimated entropy, which does not even have to be close to the true entropy.  Section 7: experiments The part about how zero shot clustering improves SVM classification is very frustrating to read: results are so promising, but very few details are shared (table 3). -	What are the raw features? N-grams? -	Why only SVMs are tried on the DNN and ZSC embeddings? It would make sense to try DNNs or DCNs. -	An interesting further experiment would be if discriminant fine tuning of the embedding further improves performance over ZSC. In this case, ZSC training would be comparable to semi-supervised training, with a mixture of labelled and unlabeled examples.This paper introduces a method to classify search queries into a set of classes in an utterance frame classification task.  The main proposed method is to learn from query click logs a representation of words into an embedding space.  The paper has one huge problem which is its lack of comparison to the most obvious baselines which are other word embedding models.  For example, the table 2 of the embedding nearest neighbors looks very similar to what neural network language models would learn. In fact, the paper mentions that the proposed model is also very similar to such neural network language models (NNLM).  Given that the model is very similar to NNLMs like those of Bengio, Collobert and Weston, Huang et al., Mikolov et al., etc it would seem crucial to compare to these existing methods to know whether this model improves over existing literature.  Unlike NNLMs which are truly unsupervised and can be trained on easily accessible abundant large text corpora like wikipedia, the paper instead proposes to use a proprietary dataset that is not and will not be available to anybody to learn essentially the same type of embeddings.  It also seems like, if the click log data includes words like restaurant that are similar to the SUC class names, the task becomes essentially supervised. Even if not, the tasks are related and I would call this approach more of a distant supervision type approach and not zero shot learning since users clicking on semantically important URLs is some type of supervision (albeit one impossible to obtain for anybody except the big search engines).  Frome et al. and Socher et al. both had a NIPS paper last year that also used deep methods for zero shot learning, using embeddings that were learned in an unsupervised way, mention or comparison to these projects would be reasonable.  The terms/ideas of zero-shot clustering seem confusing. Clustering is, by its usual definition, always unsupervised and hence 'zero-shot'. In fact, the point of zero shot learning is that one can do a usually supervised task but without supervision of the classes that are to be predicted. When I assign an element to a cluster in k-means, am I doing zero-shot clustering in the author's view?  The improvement to a simple existing kernel based method in table 3 is tiny (0.2%) improvement.  Minor comments: - 'models [15] who learn' -> models which learn - Table 1 is in an odd place that is way too early since it's referenced only a few pages later. - it would be better if citations mentioned the names of the authors so the reader familiar with the field doesnt have to go back and forth between the text and references to know what paper is being cited. - citation 25 just mentions ICML and has no title or authors - fig.3 is not readable in a black and white printout - 'to the the 1000 most popular'    Conclusion: The merit of the paper is highly questionable without a comparison to similar models that learned word embeddings with just raw text. Several word embeddings are available for download and I encourage the authors to pick one and update their paper on arxiv with an added comparison.",1,5074
"The paper proposes a new method for constructing features by hand. Features are constructed as follows: First, one averages the input dimensions that are strongly correlated (for images these will be nearby pixels). Then one attaches to the result the differences between subsets of pairs of these features which are still highly correlated.  The beginning of the introduction is just a copy of the abstract.   I find the section Constructing the representation hard to follow. It would help to use a much more detailed description. Also, under 1. the term 'features' is used to refer to the learned representations as well as to pixels (second page, after 1.), under 2. the same indexes are used on the two sides of the equal sign, and under 3. defining the symbol 'equal with triangle on top' would help.  The construction of the features reminded me of locally binary patterns LBP (unless I am missing something), so it feels like it would be good to compare to these.  While 94 errors on MNIST seems decent, the method contains a huge number of hyperparameters (first level correlation threshold, second level correlation threshold, several boosting parameters, size and choice of random matrix to estimate correlations etc. etc. etc.).  The result of CIFAR-10 seems encouraging. Is this without image priors (that is, permutation invariant)?  As is, I find the paper quite hard to read, and it will be important to improve the clarity of the presentation.The paper proposes a method for feature construction/augmentation based on grouping features that correlate, in an iterative/recursive fashion. These feature sets are validated using AdaBoost.MH and a few interesting results on MNIST and CIFAR are presented, as well as a few interesting negative results. The authors claim a relatively state of the art results on MNIST (without using image priors). While the CIFAR results are not as competitive with state of the art, they do improve on the boosting state of the art.  The method is relatively simple: group features that correlate, then connect neighborhood features that correlate and construct edge features by subtracting the correlated neighborhood features. The authors suggest this was inspired by biology/Haar/Gabor filters, but I am not sure this connection is particularly informative.  A hybrid AdaBoost.MH with decision stumps (for picking the features to augment) with AdaBoost.MH + Hamming trees was run.  On MNIST this gives a 0.94% test error, which sounds like state-of-the-art(-ish). Interestingly, on CIFAR-10 this kind of approach beats boosting approaches on top of Harr filter features (though is not very close to state of the art!). For smaller and lower-dimensional datasets, the gains are relatively small -- the authors hypothesize that this is because the dimensionality is too low basically.  Why not run something like a linear SVM on top of the constructed features? I think it would be an interesting result too. Or even just a second-degree polynomial expansion of the feature-set found by the decision stump phase? Would make the results in this paper stronger, in my opinion: right now, it’s unclear if the extra gain is because of the clever selection of which features to combine or simply because features were combined at all.In its present form, the paper proposes a very neat task-agnostic feature transformation trick that brings Adaboost nearly up to the start of the art on image classification tasks, which is quite a feat. The trick seems so far to bring top performance on tasks where there are known feature correlations (like pixels) but not on others.  While it would be a nice workshop contribution  in its present state, I think it requires some work to be a significant contribution to representation learning: some important aspects of the work are not properly explained, and experiments only use one algorithm (Adaboost.MH with hamming trees) and would need to be try  other algorithms (SVMs, first layer of deep learning).  The authors take the traditional local smoothing and edge detection used for image processing, but remove all knowledge that pixels are adjacent. Thus the smoothing can be done on any set of pixels provided they are similar (measured through correlation). The correlation is measured again between smoothed pixels to determine edges. I assume this is original, as in its exhaustive form, one would probably discard the method as computationally not practical (I assume it scales as O(N_example*N_features^2)). To my opinion, an important contribution is the use of boosting in an auto-associative setting to select a subset of pixels that is used to select the neighborhoods, however, this appears in the paper only as a speedup trick. From an image processing viewpoint, what are these most predictive pixels picked up by Adaboost? This actually a very neat representation learning trick I have not seen elsewhere.",0,5075
"This paper proposes to regularize auto-encoders by minimizing the mutual information between input and output. The minimization of mutual information is based on an alternative definition of entropy (Sanchez et al. 2013). Although auto-encoders have been around for more than 20 years, the introduction of deep learning (Hinton et al. 2006,Bengio et al. 2006) has renewed the interest in these models and their regularization schemes (Vincent et al. 2008), as they can be stacked to achieve state-of-the-art performance.  A first contribution establishes a link between the proposed model (rate-distortion auto-encoders) and PCA in the case of a Gaussian input variable but does not discuss the already established proof that traditional linear auto-encoders are equivalent to PCA (Baldi and Hornik;1989).  The authors then derive a gradient training procedure for the rate-distortion auto-encoder and report the result of two experiments in dimension 2, namely they compare the input to the reconstruction : 1 - In the case where the input is a specific Gaussian variable. 2 - In the case where the input is a specific mixture of 3 Gaussian variables.  finally the authors  conclude that the reconstruction approximately fits the input distribution.  Although the use of rate distortion theory to regularize an auto-encoder is new, the paper suffers from several issues. First, the authors do not define the problem which they are trying to address. They present a cost function and their goal is to minimize it. To what end ? Accordingly, it is unclear what the authors are trying to prove in their experiments: a regularization property ? Since the end goad is not clearly defined, why is regularization important at all ? Additionally, the experiments are very insufficient as they only consider two very simple artificial datasets: the first with a single predefined Gaussian, and the second with three Gaussians. In these experiments, the proposed method is  not compared to any other model or baseline.  pros: cons:  - Goal is not defined.  - experiments are in a very low dimensional space (dim=2) which is not very relevant for auto-encoders.  - experiments do not compare the proposed model to a baseline (traditional auto-encoder) or to other forms of regularized auto-encoders (e.g. denoising auto-encoders or contracting auto-encoders).  - experiments do not report any quantitative measure of performance (e.g. log-likelihood or classification accuracy).This paper proposes a new criterion to train auto-encoders: minimizing the mutual information (MI) between the input and output distributions, under the constraint that the output is close enough to the output. This constraint can be seen as the 'risk' we want to keep small, while the minimization of the MI adds regularization to prevent overfitting (i.e. learning an identity mapping).  This seems to be an interesting direction to investigate, however I find the submitted paper to fall short in two important areas:  1. The motivation and intuition behind this algorithm are not very clear. At the end of section 2, we do see that minimizing the MI 'can have the effect of lowering the entropy of the output variable and thus, we can think of the mapping f as a contraction' but that does not really explain what kind of properties we can expect when minimizing eq. 2. For instance, if we wanted to lower this entropy, why not just do that directly? Another point that confuses me is that the problem is initially stated as a constrained optimization one, but if I understand correctly, the actual algorithm is performing gradient descent on the Lagrangian (eq. 8): this Lagrangian is the sum of two terms, one being the risk to minimize, and one being the regularization, and the parameter mu gives the trade-off between the two. Now my question is: why not start directly from this criterion (which I personally find more intuitive) instead of the constrained optimization formulation (whose added value is not obvious to me)?  2. The experiments are extremely limited, being run only on two 2D toy datasets  and without any comparison to other popular auto-encoder algorithms. I also feel they are not enough to give additional intuition on the algorithm's behavior. Here are some examples of topics which could have been investigated: - How does the output change with mu? - How does the output change with different kernels used in the entropy    estimations? - How does the algorithm behave on real data? - How does the algorithm behave as dimension increases? (when the data do not    lie on a low-dimensional manifold, local kernel methods tend to fail) - How does the algorithm behave compared to the typical auto-encoders mentioned    in the introduction? (the goal would not necessarily be to show it works   better, but to gain understanding of the differences between algorithms)  Overall, an interesting idea, but one that would deserve a more in-depth  treatment (note that novelty seems limited, since the starting point is a criterion already proposed for manifold learning, and the kernel-based entropy estimation comes from a previous paper by the same authors).  A few more small points: - There is a non negligible amount of typos, it could use a proofread pass. - Notation inconsistencies (or not well explained): D instead of d in intro   (not defined by the way), using both hat{x} and 	ilde{x} to denote the   reconstructed data (and hat{x} is also used in the intro to denote the noisy   input in the denoising auto-encoder), multiple P's in eq. 1, not clear if we   are working in a continuous (eq.1)  or discrete (eq. 3) space, h not defined in   1st paragraph of 2.1. - The manifold learning algorithm described in Section 1 is not very clear. Eq.1   seems to be only part of the cost, and it is not clear if it is minimized   or maximized. Unfortunately I did not have time to read the corresponding   reference, but I feel like it may deserver a more thorough description, given   that it seems to be key to the algorithm presented here. - Greek letters cannot be seen on some PDF readers (like an iPad), although it   works under Windows.* Brief summary of paper: The paper proposes a novel kind of regularization for learning autoencoders that is rooted in rate-distortion theory. The regularization term is a kernel-based estimator of entropy (of the reconstructed data) proposed by the authors at last year's ICLR. Experiments on 2D toy data show that it works as expected.  * Assessment:  The rate-distortion approach to autoencoders is interesting, and I beleive novel. I would however refrain from stating, as written in the paper, that it allows learning autoencoders without an explicit regularization terms: the conditional entropy clearly plays the role of a data-dependent regularization term (similar to most alternative approaches for learning overcomplete autoencders: they also don't define an explicit penalty on the parameters).   The kernel-based entropy estimator seems however computationally very expensive, since it requires full eigendecomposition of Gram matrices.   The main weakness of the paper is the very limited experimental evaluation of the method (2D toys), and the lack of comparison with any other regularized autoencoder approach, not even a discussion of what this new mathematically sophisticated and computationally heavy approach might offer as benefits. Similarly, the authors only use their own gram-matrix-based entropy estimator: a brief discussion of properly referenced alternative, more classical, nonparametric entropy estimators would have been in order (and ideally with experimental comparison).   The paper is mostly well written. There is however a confusing proabably unintended notational shift happening after Eq. 14. In eq 14 and in ghe previous sub-section you use S_alpha, and in what follows you use H(K) which has nowhere been formally defined or related to S_alpha. Also where does the N arise from in eq 16. Please clarify.  Lastly I have a question/remark: conditional entropy H(X|X^) could also be rewritten as  H(X|X^) = H(X) + H(X^|X) - H(X^) Now since you are concerned with a deterministic autoencoder, X^ is a deterministic function of X, so it would seem that H(X^ | X) is a constant. So it seems you might as well penalize only H(X^) rathe than trying to estimate and penalize the actual conditional entropy. Does this reasoning seem valid?    * Pros and cons:  Pros: + original approach to autoencoder regularization  Cons: - computationally very heavy approach - very limited experimental evaluation (toy 2d data) - lack of comparison (neither in discussion nor experimental) with anything related",1,5076
"My review refers to the most recent arXiv revision (#3) at the time I downloaded papers for review.  Summary  This paper applies convolutional neural networks to the task of predicting upper-body keypoints (face, shoulder, elbow, wrist) in static RGB images. The approach trains one ConvNet per keypoint (all with the same architecture) for the task of deciding if the center pixel of an image window is the location of the target keypoint. A spatial model (a simple chain connecting face-shoulder-elbow-wrist) is estimated to provide a prior between locations of adjacent keypoints. At test-time, the ConvNets are run in a multi-scale, sliding-window fashion over the test image. The “unaries” from the ConvNet keypoint detectors are then filtered using the prior.  Much recent work on pose estimation in static RGB images has focused on combining HOG-based part detectors via a spatial model. These models are often enriched with local mixture models. Yang & Ramanan and Sapp & Taskar are popular recent examples. This is one of the first papers that uses ConvNets within this “parts and springs” paradigm. A paper similar in spirit was posted to arXiv slightly before this paper was submitted (“DeepPose” by Toshev and Szegedy http://arxiv.org/pdf/1312.4659v1.pdf). While too new to require a comparison, I list it here for completeness.  Novelty and Quality  While ConvNets have been used for pose estimation in previous work (as properly referenced in this paper), the current generation of ConvNets (following from Krizhevsky et al.’s work) have not been tried on the current generation of human pose datasets (e.g., FLIC). While the technique is not very novel, the proposal and investigation are good to see. The paper is well written. However, the experimental evaluation is confusing (unclear baseline methods, unclear if the subsets of images used are the same across methods) and computation employed for spatial modeling seems odd (more specific comments follow).  Pros  + It’s good to see an investigation of ConvNets into pose estimation on modern datasets like FLIC. + The paper is well written and easy to follow. + The proposed architecture is similar to existing ones based on HOG, but with HOG filters replaced with ConvNets, making for an interesting comparison.  Cons  - I found details of the experimental comparison on FLIC lacking (I’ll be specific below). - The abstract and intro lead one to believe that the results on FLIC are going to much, much better than prior work, yet they only look marginally better. - The DPM baseline on FLIC doesn’t make sense (details below). - The choices made in the spatial model needs to be explained more.  Details questions and comments  Sec. 2: Shakhnarovich et al. [37] do not use HOG [12] features. Note that [37] predates HOG [12] by a few years.  Sec. 3.1: Please be more specific about the form of LCN used.  Footnote 1: “the the” typo  Unnumbered equation bottom of page 5: I might be confused by the notation and terse explanation, but I think this should be (p_u|i=0 * p_u). More generally, the computation needs to be explained/justified more. Given a chain like this, one would typically compute the marginal likelihood of a keypoint at each location using dynamic programming (same as sum-product on a tree/chain). Here, it seems that when computing the “marginal” for the shoulder, the wrist is completely ignored. This seems very strange and ad hoc--given all of the literature on pictorial structure models, why implement this odd variant?  Experimental setup / DPM comparison:  Sec. 4: “Following the methodology of Felzenszwalb et al. [16]...” Felzenszwalb et al. does not deal with pose estimation or propose a methodology for this dataset. There is some confusion here.   If only 351 images were used (instead of 1016) how did you compare with MODEC? Eyeballing the plots, they appear to be the same as in the MODEC CVPR 2013 paper, which from what I can tell used all 1016 test images. Is this an apples-to-apples comparison?  In Figure 6, how is the DPM baseline implemented? DPM [16] was not designed to do pose estimation, so how did you modify it to estimate pose in this work? What data was it trained on?This paper examines a way to use convolutional neural networks to estimate human pose features. As many people in the community know, convolutional neural networks have had a major impact in the ImageNet object recognition evaluations. This paper looks at using them for a restricted setting of the challenging problem of human pose estimation. This is clearly an interesting direction to explore, and the details of how one does so are important - as noted by the authors.   At a high level it seems the main idea and contribution here involves the use of a simple chain structured model to capture spatial relationships between some key parts, namely: faces, shoulders, elbows and wrists. I think the presentation of the spatial model in Figure 3 could be a little cleaner and clearer. Basically it seems like the paper is coming up against the classic problem of how to combine local activity maps with some form of spatial prior or spatial model for how parts fit together. This type of issue has come up a lot in vision in many contexts and a number of approaches have been proposed on how to address the issue within a common theoretical framework, ex CRFs. Here it seems the approach has been to treat the output of the binary predictions for part locations from the CNN as a form of likelihood term that interacts with a prior that has been encoded through the discrete distributions of what seems to be essentially a linear chain Bayesian Network structure. The part that seems like it doesn’t quite match up is the fact that the prior is encoded within the conditionals of the discrete distributions of the Bayesian network while the likelihood is actually the result of the CNNs prediction for a set of binary decisions arising from the sliding window setup.  In general the paper presents some promising results, I think the theoretical framework could be cleaned up a little, but the ideas and results are going in a good direction.The paper proposes an architecture that takes as input an image and outputs the locations of human body parts (face, shoulder, elbow, wrist). The architecture consists of two parts, of which the second part, however, does not contribute much to classification accuracy (on the data on which the model was tested).   The first part is a sliding window detector using a binary-output convolutional network. The networks uses smaller pooling regions than what is conventional, in order to retain a high degree of spatial precision, and is otherwise not different from commonly used networks. I find it surprising, that this makes much of a difference, because I would have thought that the peak response of the sliding window conv net be pretty much at the same location, with or without a lot of spatial pooling (especially since you use non-maximum suppression).   The second part of the architecture is a graphical model (Markov chain) that represents a prior over relative spatial location of the different body parts. It is used to clean up the conv net detections. Unfortunately (but this is also an interesting finding) it does not help much.   I find the first sentence in section 3.2 strange. Why do you care about false-positives? Or put another way, why don't you increase the detection threshold? It seems like you should really only care about the complete ROC curve. But then, as you show later, the prior you propose here doesn't help much to fix it.   Using sliding windows with a conv net sounds like it will be slow. Could you say something about the efficiency as compared to the other models? Sapp et al., for example, seem to show that MODEC is not only fairly accurate but also fast (well, as compared to DPM). Apologies, in case this is discussed somewhere and I overlooked it.   In Section 4.1 the references to the Figures are wrong.   This is yet another paper showing that conv nets work well in tasks previously dominated by more complicated vision architectures. The paper has some minor issues as pointed out, but overall I enjoyed reading it.",1,5077
"* A brief summary of the paper's contributions, in the context of prior work. Paper experimentally verifies how relevant are anecdotal descriptions of dropout.   * An assessment of novelty and quality.  It is novel, and has a good quality. However, it doesn’t bring any new ideas. It rather presents experiments which were missed during initial development of dropout.  * A list of pros and cons (reasons to accept/reject). pros: - Verifies some of previously postulated intuition. - Gives some indications what are the major building blocks of a good regularizer for neural networks.  cons: - Experiment should be run on a larger datasets where not all possible masks are utilized but a large amount of them (e.g. not all 2^N, but e.g. 10^6 random one). - All the comparisons should be with respect to networks without any dropout or model averaging. Maybe on the datasets which you considered regardless of dropout you would get the same results. Such setting should be compared on all the plots. It was unclear for me if such experiments were executed. - Authors should write point-by-point what are all possible interpretations of dropout, and which one they are going to validate. It would be also good to have some suggestions how other anecdotal interpretations could be validated (e.g. co-adaptation). - The same authors were working on max-out networks, which seems to play well with dropout. It should be explained here (or just experimentally compared) why maxout is a good architecture for dropout.  - Figures 1, and 2 are hard to interpret. How should I know if relative difference of 0.1 is big or small.The authors attempt a further understanding of dropout through a set of empirical analyses that test a number of questions: 1, how close is the weight-scaling approximation to the geometric mean; 2., how good is the geometric mean compared to the arithmetic mean for classification; 3., Is the role of weight-tying in dropout important; and 4., is the ensemble aspect of dropout important compared to the benefit of using masking noise.  The conclusions of the authors are convincing, and the paper is a illuminating companion to some of the more theoretical analyses of dropout that have been presented recently. The dropout bagging vs dropout boosting is especially interesting. Standard dropout, which most resembles ensemble bagging, is compared to a version of weight-tied boosting. This comparison is constructed to try to determine whether there is a benefit to the bag ensemble, where each sub-model is independently tested and trained, compared the weight-tied boosting, where each sub-model is trained based on the performance of the entire ensemble. In weight-tied boosting, the benefit of weight-tying and masking noise are preserved, but not the independent sub-model training. The results seem to show that the independent ensemble/bagging aspect of dropout is important, rather than just the noise or the weight-tying.  The submission is relevant to the ICLR community. The experiments are carefully chosen and the conclusions are not overstated. The only significant barrier to publication is the lack of analysis of the empirical data and the figures, which are poorly chosen and not well explained. Figures 1 and 2 are difficult to read/interpret. A single summary/analysis for each of the 2 sets of data points would be very helpful. Figure 3 only goes to 120 ensemble members, but the text describes results at 360. It would be valuable to plot the full results, even if it is flat after 120.This paper provided a very interesting analysis of dropout, which has recently shown great success in a variety of DNN applications. The paper is well written and the experimental analysis is strong. My comments are mainly to help improve the paper and clarify ambiguity in some places  •	Section 1, page1: when you give the equation f(x) = max(0,x) you should state that this is known as a recitified linear unit (ReLU) and provide the appropriate reference, which to my knowledge was first done here: Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. What is the best multi-stage architecture for ob- ject recognition? In Proc. International Conference on Computer Vision (ICCV’09). IEEE, 2009. •	Section 2.2, page 3: why does ReLU work better than sigmoid when using dropout? You say this as well but providing some intuition would be good. •	Section 3, page 3: The reason you have chosen small networks for your initial analysis is so that you can do an exhaustive enumeration. You state this in Section 4 but not 3. You should state this upfront in Section 3, otherwise the reader might think your analysis might not be generalizable to larger data sets. •	Section 3, Page 4: Your training criterion (early stopping, etc) has been done in previous papers. Pls cite one of these so readers know that this is a commonly used approach •	Section 4, Page 5: Pls switch the order of sentences “The overall result… and In order to make differences..” to make the flow easier to read.  •	Section 4, Page 5:  Pls give references for Wilcoxon signed rank test and Bonferroni correction, as not all readers will be familiar with this. •	Section 4, Page 5: There seem to be some outliers in Figure 1, but your significance test shows that this doesn’t matter. You might want to add a sentence stating that the outliers in Figure 1 are not really significant as shown by the test. •	Section 5, Page 5: Can you comment on how expensive the arithmetic mean is? •	Section 6, page 6: Pls provide reference for “utilized norm constraint regularization” → this is known as max-norm and can be found in Nitish Shrivastava’s thesis •	Section 6. Page 6: You need a period after the “2” footnote. •	Section 6, page 7: You should also state that the ensemble method required training 360 different networks which is computationally expensive compared to training just 1 network with dropout.  •	Section 7, page 8: It is not clear to me how the dropout boosting injects the same amount of noise as dropout, and should be clarified.",0,5078
"The paper describes an Adaboost algorithm, with tree based learners, adapted to the multi-class setting. The claim is that direct multi-class algorithms cannot be used to grow tree base learners in Adaboost. The author proposes a factorization of the classifier into a product of 3 terms, one performing a binary separation of the input space, a second one projecting this binary value onto the  classes and the third being a confidence term. With this formulation, base tree learners can be grown using the binary separator corresponding tot the first term. Experiments are described on a series of UCI data sets and reach good results compared to a series of baselines. The idea of factorizing the multiclass classifier is interesting and seems to lead to good results – even if the experiments have been performed on rather small size problems. The paper is relatively clear, but could be improved. In many places, there are shortcuts which will probably be hard to follow for non-specialists of multi-class Adaboost. Also there should be a description of the alternative multi-class Adaboost methods used in the experimental comparison, in order to appreciate the originality of this new proposition. Overall, I think that this is a good paper. On the other hand, I am not sure that it is adapted to ICLR. It is not concerned with representation learning and would probably be better suited to a more general ML conference.In the last 10 years, boosted trees have been considered as the best performing algorithm for large scale data with numerical features (though this may change with deep learning). However, why they perform so well has never been fully understood, especially as the boosting and the tree parts of the algorithm tend to be optimized separately. This is all the more true in the multi-class setting, where boosted tree implementations are full of ad-hoc patches.  This paper offers a model where the construction of the tree is part of the boosting algorithm in a full multi-class setting, and I assume this is novel. It matches the performance of SVMs on small tasks where they were traditionally superior to boosting, and the performance of the best previous multi-class booting implementation. This is a well written paper of high technical quality with extensive and rigorous experiments, and effective descriptions of the algorithm implementations.  The main issue I found is how this algorithm scales: boosted trees are used for large scale problems, and addressing this issue is critical for significance. The authors only report large scale experiments in very vague terms, without any comparisons or training times (the authors do not report any numerical results and do not give reference. I do not know what to make for instance of the claim that it won the recent Interspeech challenge). The cost per iteration appears linear in the number of features, examples and dimensions, which is standard, but what about the number of iterations? My experience is that discrete Adaboost is quite slow on larger datasets compared to real-value prediction or gradient-based boosting.  This paper packs a lot of technical contributions in 9 pages, to the point that the authors had to take a few short cuts that impact clarity, but I would only recommend to cut comments that are not understandable by non-expert in boosting, or expand them (for instance all the references to the weak learning condition, which is never explained).  I noticed there was not a single boosting paper at ICLR’13, while this is probably still the most widely used class of machine learning algorithms. Through the greedy selection of weak classifier, boosting offers a way to learn representation that is different from deep learning, but highly practical for many problems. The same author has submitted another paper that targets specifically the choice of the features for boosting (correlation-base construction of neighborhood and edge features), however, I find this paper the most worthy to be accepted.  Detailed comments: -	Authors should better explain the weak learning condition. -	In section 3, there is a single alpha, so the subscript j in “alpha_j” must be a typo. -	Section 3: choice of font for h_j should be consistent. -	I do not see how one can get O(nKd log(N)) if the tree is balanced: I assume one still has to run TREEBASE, which calls BASE N times, and BASE is O(nKd)??? -	Section 2.2: the case against Adaboost.M1 used in a multiclass setting that would lead to an error rate of (K-1)/K sounds strange. I thought one would use K separate Adaboost.M1 in a 1-vs-other setting?The author presents a way to train vector-valued decision trees (Hamming trees) in the context of multi-class AdaBoost.MH, as one way of not doing K binary one-vs-all classifications. The idea is a clever factorization that allows for efficient modeling and predictions.  The paper seems to be performing very thorough validation, comparisons and hyper-parameter selection, which is obviously a good thing.  Unfortunately, I cannot speak with any authority as to whether the algorithmic contributions of this paper are substantial or not. While the results do seem to support the idea that the proposed method, AdaBoost.MH + Hamming trees, works as well or better than SVMs/other AdaBoost methods, I cannot evaluate in earnestness whether this is a very novel contribution or incremental, since recent developments in boosting are not exactly my area of expertise.",0,5079
"The paper applies the recursive ICA algorithm to visual data and describes the connections between the layers in the model to neural representations.  The RICA algorithm is modified from its original form to utilize the author's previous innovation of 'sparse PCA.'  The results are conveyed through a few depictions of the receptive fields and some qualitative links to neurophysiology.  Contributions: > The authors have updated the RICA algorithm to utilize SPCA instead of PCA. > The authors claim that sPCA on the outputs of the first-layer ICA, produces V1-complex cells. > The authors suggest that some unoriented neurons in a V2 experiment correspond to a subset of their second ICA layer.  Overall, this is an interesting direction and there are signs of promising results.  However, the poor exposition, preliminary nature of the result, and lack of quantitative matching to neurophysiology (or other strong metrics for differentiation/novelty), indicate that this work is not yet ready for publication/acceptance.  The update of RICA to use SPCA is a minor innovation, and the claims about V1 complex cells and V2 are not well supported by experiments (matching to neural findings).  In the following I provide some feedback that I hope will assist the authors in this work and a future publication of this work.  In the background, more attention should be paid to similar techniques to RICA.  For example: Chen and Gopinath 2001 Gaussianization. NIPS. Lyu and Simoncelli 2009, Nonlinear extraction of 'Independent Components' of natural images using radial Gaussianization. Neural Comp.  and there are a number of works that produce findings similar to your second ICA layer: Y. Karklin and M. S. Lewicki, A hierarchical Bayesian model for learning non-linear statistical regularities in non-stationary natural signals, Neural Computation, 2005. A. Hyvärinen, M. Gutmann and P.O. Hoyer. Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in V2. BMC Neuroscience, 6:12, 2005.  Honglak Lee, Chaitu Ekanadham, and Andrew Y. Ng. Sparse deep belief net model for visual area V2. NIPS 20, 2008. CF Cadieu, BA Olshausen, Learning intermediate-level representations of form and motion from natural movies. Neural computation 24 (4), 827-866 These should be discussed.  Some points on exposition: > be careful when referring between human visual system and then relying on non-human primate data.  You seem to use these different species interchangeably > your statement that 'a previous neurophysiological study improperly discarded some of their recorded neurons' could be rephrased.  I think you can be more fair in discussing the actions of our colleagues. > You repeated state 1st layer ICA algorithms (including yours) learn edge/bar shaped receptive fields.  This statement is false.  The generative fields (and receptive fields) resemble Gabor functions.  Gabor functions are not edge or bar shaped.  They are localized in space, orientation and position.  While edges/bars are broadband.  Don't fall into this sloppy language.  Note also that results in neurophys. indicate that V1 are not edge/bar selective, but selective for stimuli localized in space, orientation and position (just like Gabors, and unlike edges/bars). > you use both 'sPCA' and 'SPCA'. > you are missing citations that you refer to in the text. > the term 'autoencoder' refers to a specific class of models and I do not believe your optimization falls into this class.  My major suggestion is to spend more time on the link between the properties of the intermediate layers of your model and the neurophysiology literature.  There is a tremendous amount of data on V1-complex cells and you can produce non-trivial (meaning you need a control) links between your model and these quantitative findings.  While there are fewer results on V2, there are quantitative experiments to run on your model.  Your current results are too qualitative and require the reader to make leaps from the receptive field depictions you use and an experimental result using certain stimuli, plotted in a completely different methodology (Figure 5 and 6).  The experiment in Figure 4 is a step in the right direction, but showing one cell isn't sufficient, there is no control, and their are quantitative distribution metrics that are relevant in the Chen, Han, Poo and Dan PNAS 2007 paper (e.g. Figure 5 in that paper).Review Efficient visual coding: from retina to V2:  This paper tweaks the recursive ICA model (RICA). RICA made ICA stackable by combining PCA and ICA, and applying a component-wise nonlinearity to the outputs of each layer. This version replaces PCA by sparse PCA.  When trained on natural images, the updated pipeline produces standard images of local center-surround receptive fields, and oriented edge detectors, and also complex-cell-like units on the second layer.   sPCA is not new, nor is RICA, so there isn't much of a technical contribution. The results look good and the overall system is simple and stackable. There isn't much new but other researchers might want to see what representation this system learns.  However, I would like to see:   - a discussion of how the results after training on natural images differ from RICA: is this only that the lower layer now has local-center-surround receptive fields? - many more references to work by other people that also used deep architectures to create V2-like cells; the 6 references given here are woefully insufficient and this work cannot be presented in a vacuum and cite only the authors' own RICA. e.g.:  Karklin and Lewicki, Learning higher-order structures in natural images. Network: Computation in Neural Systems, 14:483–499, 2003  Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato and Yann LeCun: What is the Best Multi-Stage Architecture for Object Recognition?, Proc. International Conference on Computer Vision (ICCV'09), IEEE, 2009  H Lee, C Ekanadham, A Ng Sparse deep belief net model for visual area V2 Advances in neural information processing systems 20, 873-880This paper expands upon the authors previous work on recursive ICA.  Here they explore the application if sparse PCA (where the weights have a sparsity constraint), followed by ICA.  The results seem to similar to what they have shown before, and also to what was learned by Karklin & Lewicki's model - i.e., grouping of oriented units by orientation and position.  I find the previous work on RICA very interesting, as is this paper, but it is not entirely clear what is learned here beyond the previous work.",1,5080
"This paper presents a simple method for denoising noisy mnist digits with a deep belief network. The method looks at the relative activities of the hidden units when the input is a normal vs a noisy image. After obtaining these statistics, at test time, noisy nodes or nodes which are affected by noise are removed, leading to better reconstructions.  The authors are recommended to reference the paper: Deep Networks for robust visual recognition, Tang&Eliasmith icml 2010, where the tasks are similar but with a more elaborate algorithm and experimental results.  While the task is interesting and potentially very important, the method proposed in this paper is extremely simple and are not shown to work for difficult noise/occlusion cases. For example, simple reconstruction with DBN is already very good for the mnist digits.  Possible improvements include trying more difficult noise and occlusions; look at how denoising can help reduce recognition error; and coming up with a more principled way of determining which nodes are affected. Since a DBN is a distributed network, it is likely that all hidden nodes would be affected somewhat, and each image would lead to a different activation for a particular hidden node, simply by looking at relative activations seems very ad hoc. There are several papers related to denoising using DBNs/DBMs that can be found with simple google search. The authors should compare/contrast, cite and test on similar experiments with those other papers.This paper presents an approach for image denoising, based on deep belief networks (DBN). The idea is to train a DBN on a training set consisting of both noising and non-noisy images. Then, the activity of top-hidden-layer units is compared between the noisy and non-noisy images, in order to identify units which are mostly involved in the modelling of noisy images. Denoising is then performed by inputing a noisy image, inferring the value of the top hidden units, fixing the 'noise' hidden units to its neutral value (i.e. its average value on the clean images) and then regenerating the input image. Experiments show that this approach has some success in performing denoising.  The main weakness of this paper is that no comparisons are made with other good denoising baselines. I would have at least expected a comparison with the denoising autoencoder work cited in this paper [6]. Also, denoising experiments on MNIST are not particularly compelling and too simplistic.   Pros:  - The presented idea is simple.  - Results seem OK.  Cons:  - The results are too preliminary, as no comparisons are made with a good denoising baseline (including the deep learning work on denoising, cited in this paper). - Writing could be improved.  Other comment - How is denoising performed in the 'reconstruction without eliminating any node'? Specifically, is this network trained on both noisy and non-noisy images, or only on clean images?This work presents a method for denoising images using a DBN by identifying feature nodes that are associated with noise.  This is done by measuring the mean differences in activations when presented with noisy versus corresponding ground-truth clean images in the training set.  Test images are denoised by performing inference, reseting the 'noise' feature nodes to their average values across the clean images, and reconstructing from the resulting representation.  The method is tested on MNIST with additive Gaussian noise.  The method is simple and appealing; however, it is evaluated only on one very limited test case, and is under-analyzed.  How does this method perform for other types of input or noise?  Also, although the authors review some prior work on the subject, they do not explicitly compare their method against any other algorithm.  A larger question I have is whether it is necessary to require clean/noisy inputs be associated in pairs, or if this association could be weakened or removed.  This would be a major advantage of this method if it were the case.  That is, is it enough to have a pool of known clean images and a pool of known noisy images, with no elementwise correspondence between the two?  If the clean data underlying the noisy data generation is the same between these two populations, then the difference in mean activations should be unchanged.  But it seems to me that these means might also not change much if the two sets of underlying clean images are distinct but from the same general population, e.g. if half of training images are clean, and the other half is used to generate noisy ones.  The paper is pretty clearly written, though I think the overview of RBMs and DBNs takes up too much space (2 pages); this seems it could be condensed, and replaced with more experiments and details on the method presented.  I also would have liked to see more illustrating the method's internals.  Why was the threshold of 0.9 chosen for identifying a node as a 'noise' feature, for example?  Some plots/histograms of the activations and 'relative activity' measurement could have been useful here.   Pros:  - Natural and simple method with limited demonstrated effectiveness.  Cons:  - Applied to only one limited setting - No comparisons to other methods - Could have more measurements to illustrate the method's internals",0,5081
"Two methods are proposed for making a distinction b/t extracted features based on relevancy. The first method is looking at the variance of hidden nodes when inputs vary on some aspects but stay the same in others.  The second method is looking at the relative activity of the hidden nodes when additional input features are subsequently added. The hope is to identify some “Grandmother” like cells in a DBN which would be selective of faces and digits.  Overall I think the paper is somewhat interesting in it’s motivation of probing the DBN to see the meaning of each hidden layer units. However, DBNs are notoriously hard to interpret due to the fact that it is a distributed representation. While i think the paper is in the right direction, the method presented are very simplistic and it is not clear what can be concluded. In particular, it is somewhat expected that if we remove the nodes which are modified by when a face image changes a face+digit, then what remains can better reconstruct the face. Is training performed sequentially on the three datasets 1,2, and 3?  Another issues is that the digit is just so much brighter than the face, making this problem easier, what happens if you use the digit with the similar intensity as the face? my suspicion is that the DBN would have a lot of trouble with that.  Possible way to make this a good paper would be to use the hidden node discovered as a form of detector. It would perform worse than a discriminatively trained detector but it would be interesting to see a comparison.This work proposed two methods to make a distinction between features learned by DBN. The authors seemed to imply that DBN could be capable of learning compositional features from the training data. However, I do not think DBN can learn those features easily.  The method of variances is kind of counter-intuitive. While a average face feature could be activated by most of face images, the variance of its activation could be very small. According this method, the average face feature would be seen as a non-face node.The submission investigates how to distinguish between relevant and irrelevant features for a given task, of a given trained model. The context seems to be that of face recognition. The proposed methods of analysis are not explained particularly well -- the “method of variances” and “method of relative activities” sections would benefit from some equations that explain the actual details on a more-than-intuitive level.  The experimental setup is rather unique (and strange) -- it looks like the authors are training on a dataset of faces corrupted by digit images, and then presenting either images or digits to measure the relative importance of the features using the two methods exposed in Section 3. The thresholds in 4.1 and 4.2 seem rather arbitrary -- any particular insights into how the results change as they change? Figures 2-a and 2-b, referenced in the same sections, seem non-existent, too, unless the authors refer to Figure 3 instead?  Figure 3 is confusing and not sure it adds much value to the paper. While Figure 4 has some interesting results, in and of itself it does not make a paper. With some more analysis as to why and how their methods work, this body of work could be interesting to the community, as a way to analyze a trained model. But as it stands, it’s unclear whether their results are simply an artefact of the actual training data being bimodal and thus the hidden units modeling that efficiently...",0,5082
"This paper is trying to employ DBNs at the traditional task of associative memory. This is an interesting problem as the human brain is thought to contain an “association cortex” dedicated to combining sensory modalities. However, while the paper is written reasonably well, it did not introduce any significantly new learning method. The experimental section is weak and leaves this reviewer unconvinced of their conclusions.  The framework of having dual modalities has been proposed by the original DBN paper [8] (the reference is missing a third author). Contrastive wake-sleep algorithm is proposed for unsupervised learning in [8], which seems more principled than what this paper proposes in the paragraph starting with “To fine-tune channel 1…”.  In the experimental section, the natural comparison is to an autoencoder (e.g. net in figure 6), which is also trained in an unsupervised manner. However, it is hard to believe that a network with 500-1000-500 hidden nodes can’t reconstruct better than what is shown in Fig. 7. last row. What kind of learning algorithm was used? CG or SGD, did the optimization converge?  The authors also ref “Hinton’s software” and it’s 1.15% error on MNIST. However, that is a totally different net with 10 1-of-k label units. It is unclear what the authors mean by using hinton’s software: was there no additional learning for your particular image pair association task been performed? If that is the case, then the results are believable but not a good baseline.  Several suggestions to improve the paper: compare with the contrastive wake-sleep algorithm and autoencoder trained with CG/SGD. Investigate more in depth on your proposed algorithm, is it approximating some objective? You should truly use multimodal data like images and speech, since it is so often mentioned in the introduction/background sections. It is a lot of work to combine audio and natural images are much higher dimensionality, but this would make the paper stronger.  The claim in section 2.2 that “The mammalian brain is..” is a huge claim which is not proven. It is the current mainstream theory, but you should not treat it as a fact. Note that models like [14] do not even do learning except the last SVM layer.  The claim that RBM can’t recall patterns when half of is corrupted is not convincing and maybe should be qualified to a particular task/learning algorithm. There are papers on rbm and noise+occlusion which suggest otherwise. It is true that how RBM performs is dependent on how it is trained (e.g. using fast pcd is critical).eq i), which probably is computational intractable and without any analytical form?   - Sec. 2.2, the end of the third paragraph: I believe many people consider convent as one of deep learning methods as well.  - Sec. 3.1, 2nd paragraph: 'RBM is unable to recall patterns when only half of the visible neurons are given correct pattern values' I completely disagree with this sentence. It may highly depend on data as well as the model size. On MNIST (which the authors used for their experiments), I believe reconstructing the missing half is not too difficult and can be done pretty well with a reasonably large and well-trained RBM.   - Sec. 3: What is noticeably missing in Sec. 3 is how the authors actually reconstruct the missing modality given the other modality. Is it simply a single forward pass from one modality to the other using the recognition, then generation weights? Does it involve sampling at each layer in between? If so, it's likely that p(x2 | x1) has multiple modes (x2 missing modality, m1 observed modality). How do you resolve among multiple possible reconstructions? If there's no sampling involved, why does the proposed model work better than a conventional autoencoder (two-way) trained with SGD and backprop? Is it possible that the problem of the conventional NN is due to learning difficulty only?  - Sec. 3.1, the last paragraph: I believe S&S (2012) did not do supervised finetuning for all experiments. For instance, for image retrieval, multimodal DBM does not require any discriminative finetuning.  - Sec. 4: In general, I'm not sure why the authors had to use only that small dataset. And, due to this small size dataset and the pretraining strategy used during the experiments, each parameter of the BP-ANN gets unfairly smaller number updates.  - Sec. 4.1, the last paragraph: why do you suspect that? I think it's simply that BP-ANN wasn't trained enough.  - Sec. 4.2: you should state the type of noise you used.  - Sec. 4.2, the last paragraph: 'DLAs attempt to probabilistically differentiate features from noises' I think I understand what the authors are trying to say, but I'm not entirely sure if that's correct or I'm not misunderstanding. Anyway, this sentence sounds somewhat weird (if not wrong).   - Sec. 4.2, the last paragraph: 'BP ANNs .. features ... are for the purpose of mapping and not reconstruction' I'm lost here. How does 'mapping from one modality to the other' differ from 'reconstructing the other modality from one modality'?   - Sec. 4.3, Results and Discussion: 'using non-paired examples to better develop .. associate learning system' Either this sentence has been mistyped, or I'm misunderstanding the Fig. 11 completely. It seems to me that adding non-paired examples does not help at all. Also, the remaining of the paragraph after this sentence is extremely difficult to understand, and I'd suggest to rephrase it.Summary:  This paper presents a neural network that predicts a structured output. The authors proposed a new algorithm for training this structure. The new algorithm firstly pretrains the neural network as a stack of RBMs, and subsequently finetunes the model by untying the recognition and generation weights (which is as authors mention reminiscent of wake-sleep algorithm.)  Novelty:  Unfortunately, I don't see any novelty in the proposed approach. The exactly same neural network was proposed earlier by Ngiam et al. (2011). Also, it is very close to the multi-modal DBM proposed by Srivastava & Salakhutdinov (2012). The authors may argue that Ngiam et al. (2011) did not attempt to actually generate a missing modality, but precisely that was done by S & S (2012). Furthermore, I don't think the proposed wake-sleep-type algorithm should be better than finetuning the whole model (or each path) as an (denoising) autoencoder.   Pros:  (1) Representation learning from multiple modalities is important.  Cons:  (1) Experiments are weak. (2) Explanation of the methods could be done better. (3) The relationship to the previous research should be made more explicit and clear.  Detailed Comments:  - Sec 2. 1st paragraph: 'back-prop ANNs are ... not as good for reconstructing, or recalling a pattern' => I don't really understand this sentence. Does it mean that the backpropagation (or SGD with backprop) is not able to find a solution of a neural net even if the input and target were constructed explicitly to make the neural net reconstruct corruption or noisy input?   - Sec 2.1, the end of the 1st paragraph: I don't think the last sentence is correct. First of all, what is the energy state? Isn't it a simple scalar corresponding to the log of unnormalized probability? Then, if two orthogonal patterns were in data, shouldn't their probabilities (or energy states) be close to each other regardless of their orthogonality?  - 'Weight is updated until the global energy E reduces below a threshold' What is the global energy? Do you mean the average of all training samples' energies?  - Sec. 2.1., the end of the 2nd paragraph: 'w_ij is equal to the probability of feature h_j given input v_i'. Why is it so? Shouldn't the probability of h_j given v_i require marginalizing out all v_j (j The authors propose a an algorithm, in the setting of multimodal data, for learning to generate one modality given the other. The algorithm contains stack of RBM's for each modality, a join RBM on the top and then a predictor of joint probabilities from one modality. The experiments are too weak to demonstrate strength of the proposed approach.  Novelty: Small. A standard multimodal deep belief net with an addition of a relatively obvious idea, which is anyway quite similar to previous ideas. Quality: Experiments are too weak.   Details: First, the main complaint - the weakness of the experiments. Whether feedforward network Figure 6 can learn the task depends on how you optimise it. I think if you do a good optimisation then it would do it. In particular you can take your pre trained deep network Figure 4 and, go up the left and then down the right as your feedforward net - which is essentially the idea in 'Multimodal deep learning' (http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf).  The task is also too simple - may be it would be at least good to pair a random digit from one class with a random digit from another. But even better (really necessary) it would be to do some different sets of data as in the above mentioned paper.   Other details:  Page 1, paragraph 2 - I wouldn't say deep learning typically uses RBM's. (e.g. feedforward networks, auto encoders…) Page 2, Background - I don't think we have an ability to recover *complete* information. Also 'many do not work in the same fashion as the human visual system' - and which do?… Page 3, 4th paragraph - you should write the cost function between the two probabilities Page 7 - What does it mean 'Features … for the purpose of mapping and not reconstruction' Page 8 - Results and Discussion - First you say that the results are not significant (within error bars) and then you draw a conclusion as if they were.",1,5083
"The paper considers learning cross-lingual representations of words using parallel data aligned at the level of sentences. A representation of a sentence is just a sum of embeddings of words in the sentence. These representations for pairs of sentences are learned to be similar. Specifically,  a ranking objective is used: for every sentence x in L1, the representation of the aligned sentence in L2 should be closer to the representation of x  than to representations of (a random sample of) other sentences in L2.  The resulting representations are used in transferring  a document classifier across languages (without retraining - i.e.  so called 'direct transfer').  Interestingly, the results are (mostly) better than these with cross-lingual word embeddings of Klementiev et al. (2012) learned using automatically word aligned sentences (as well as significantly better than a machine translation baseline -- which applies the classifier to automatically translated documents).   I find the paper quite interesting and well written. The results are fairly impressive as well.   However, I am not entirely convinced that calling this 'multilingual compositional semantics' is very appropriate. Though I agree that a more complex compositional model is probably not necessary for the document classification task (after all, a unigram model achieves competitive results on classifying RCV documents), it seems a bit misleading to call a bag-of-words approach compositional. After all, Klementiev et al. also sum word representation to yield a representation of a document. (However, such added compositional model, of course, have been considered in the past and called compositional -- e.g., Mitchell and Lapata (2008)).  From my perspective, the interesting aspect here is learning word representation without using word alignment information. In this way, the work is similar to the paper of Lauly et al. presented at the NIPS Deep Learning workshop (http://arxiv.org/abs/1401.1803). However, their results are not directly comparable as they used a different test set.  My concern is that a different learning objective would be needed if more expressive compositional models are used (perhaps combining both similarity across languages and the reconstruction error as in Socher (EMNLP 2011)).   Minor: -- it would be interesting to see results on other language pairs (e.g., French was already used in training, but not in testing, even though RCV contains articles in French) -- also I am wondering how performance varies depending on the size of parallel data (as this might be a concern for low resource languages where direct transfer approaches are especially attractive) -- It was not entirely clear how negative examples are sampled (formula 6):  are they chosen at every epoch  (as, e.g., in Rendle et al. (UAI 2009)), or chosen once for every sentence pair and then kept fixed during training? -- section 2.1, par 1:  'multi-agent learning' -> 'multi-task learning' ?This paper proposes a simple model to learn word embeddings in a bilingual setting. The model learns embeddings at the sentence-pair level, where aligned sentences are similarly represented. This simple model does not rely on word alignments or MT system.  The paper is well written and presents convincing results. My only concern is why the authors use the term 'Models of Compositional Distributed Semantics' for a model that is more related to annotation transfer. Moreover, the continuous representation of a sentence is the sum of word embeddings. This more related to a bag of word model than a model that can handle compositionality. A minor remark about the results: the authors observe that BICVM+ outperforms the BICVM model when training on English data, but performs worse in the opposite direction. Could you comment on that.  The authors could read the paper of Lauly et al. at the last NIPS workshop on Deep Learning. This is clearly related to this work. Finally, I wonder what could be the performances with more complex model. For instance, the same authors proposed a translation model based on recurrent Net that could be used for this task.This paper introduces an interesting model to learn single word vector embeddings for 2 languages simultaneously. It is applied to a classification task.  The paper is very clear and well written.  It does not seem to actually learn compositional semantics in the usual sense:  http://en.wikipedia.org/wiki/Principle_of_compositionality Principle of Compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them.  Certainly, averaging all words in a bag of words is not a compositional rule that would allow people to retrieve the meaning. From wikipedia:  'The principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence 'Socrates was a man'. Once the meaningful lexical items are taken away—'Socrates' and 'man'—what is left is the pseudo-sentence, 'S was a M'. The task becomes a matter of describing what the connection is between S and M.' The connection between S and M would not be retrievable from a bag of words representation.  On a related note, the model could not be used for (presumable the final goal of) machine translation in its current form.  It would be great to see a comparison with the work from the same lab of Kalchbrenner and Blunsom.  Despite its problems, it seems an interesting paper.",1,5084
"In this paper, the authors try to analyze theoretically the dynamics of learning in deep neural networks. They consider the quite restricted case of deep linear neural networks. Such linear deep networks are not of practical interest, because the mapping y = Wx between the input vector x and output vector y in them can always be realized using a single weight matrix W which is the product of weight matrices of different layers, and therefore adding hidden layers does not improve their performance.  As the performance criterion the authors use the standard mean-square error between the output vectors of the networks and their target values (desired responses), which is in practice approximated by the respective squared error over all the training pairs. The gradient descent method applied to this criterion provides the batch learning rule (1) used in the paper. It should be noted that the authors' learning rule is not the batch version of the standard backpropagation algorithm. This is because already in the case of a single hidden layer it depends on the two weight matrices between the input and hidden layer, and between the hidden and output layer.  Despite the linearity of the studied deep networks, this learning rule has nonlinear dynamics on weights that change with the addition of each new hidden layer. The authors show by simplifying further their analyses that such deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initialization.  It is good that the authors know and refer to the paper by Baldi and Hornik from the year 1989, reference [19]. In this paper Baldi and Hornik showed that in a linear feedforward (multilayer perceptron) network with one hidden layer the optimal solution minimizing the mean-square error is given by PCA, more accurately by the subspace spanned by the principal eigenvectors of the correlation matrix of the input data, coincides with the data covariance matrix for zero mean data. However, Baldi and Hornik did not analyze the learning dynamics.  The paper suffers from lack of space. It extends to 10th page and with supplementary material to 15th page. Obviously because of space limitations figures are too small and there is no Section 1. Introduction, only plain text after the abstract.  Pros and cons: --------------  + The analysis on learning dynamics is novel and interesting. I am not aware of this kind of analyses on deep networks before this paper. + With their mathematical analyses, the authors are able to explain some phenomena observed experimentally in learning deep networks. - The deep network analyzed is linear, with no practical interest. - The learning algorithm(s) that the authors study are not used in practice.This paper analyzes gradient descent learning in deep networks of linear units.  While such networks are trivially equivalent to 1 layer linear transforms, the authors argue that their learning dynamics should act as a tractable analogy for learning in non-linear networks.  Under certain simplifying assumptions they are able to produce some analytic expressions for how gradient descent learning acts in the idealized continuous time case.    There is also an attempt to justify pre-training in terms of this analysis that I find not particularly convincing, as here the 'pre-training' amounts to solving the problem almost in the linear case but certainly not in the linear case.    A new addition to this paper is a discussion non-linear networks and the role of initialization, which includes some interesting numerical simulations demonstrating that certain choices of scaling constant under a particular 'orthogonal' initialization scheme.  I like this part of the paper the best.  However, it is not the focus of the paper and should be developed further (perhaps in another paper?).  I should say that I reviewed this paper before.   It is in many ways improved from the original version.  A few problems still remain, and there are also many new ones (page 8 in particular). However, I'm mostly satisfied with it and would recommend acceptance.  I'm hoping the authors can answer my various question, as some parts of the paper either confused me and possibly contain mistakes.   In terms of the main content, the central issue I have with this paper is that the special initial conditions in which in analysis is done are essentially a partial solution to an SVD problem, which if it were solved fully, would give the optimal weights of the linear network in one step.  In particular, the assumptions used require that the weight matrices share singular vectors in a way that basically turns the optimization across all layers into a decoupled set of scalar optimizations.  In non-linear networks, where the optimal solution isn't given by an SVD, it is harder to believe that this kind of partial SVD would help as much as it does in the linear case (or perhaps at all).  I appreciate the attempt to make the analysis more rigorous in terms of 'learning speed'.  Although perhaps a better way to work out a realistic lambda would be to bound the error one gets by taking the continuous time approximation of the original difference equations and choose lambda based on when this error can be argued to have a negligible effect.  A lambda which has been chosen via some kind of curvature criterion will generally give a more stable iteration, but there is no guarantee that this will imply a reasonable correspondence between the discrete and continuous time versions, where mere stability isn't enough.      Abs/Intro:  'edge of chaos'?  This is only explained late into the paper, and sounds bizarre without any context.  Page 3:  'the input-output correlation matrix contains all of the information about the dataset used in learning'.  You mean just for learning linear networks/functions right?  Surely this isn't true in general...  Page 3: 'column of W^{21}'  Should this have a bar over it?  Page 4:  When you say that the fixed point structure of gradient descent learning was worked out in [19], you don't mention on what kinds of networks this analysis was done.  Page 5:  There are formatting issues at the top of the page.  Some text is cut off.  Page 6:  The word 'caveat' (used twice) doesn't seem appropriate here.  I usually don't think of special cases of definitions, or simplifying assumptions, as 'caveats'.  Page 7:  The bottom of the page contains formatting issues with the figure.  Page 7:  You should mention this is a classification task and also that the details are in appendix C.  The way it is written now it sounds like only the details of the choice of learning rate are in the appendix.  Page 8:  My experience is that optimization performance is only improved in the short term and maybe medium terms by pre-training vs standard carefully scaled inits.  In the longer term (which is what really matters in the end, as it dominates most of the run-time) the difference is less significant, if it is present at all. Improvement in generalization performance is less controversial, and in my opinion remains the best reason to try unsupervised pre-training.  I think it is quite reasonable to suspect that, insofar as the networks are approximately linear, perhaps unsupervised pre-training is having the boosting effect that your paper predicts that it might have.  But it seems reasonable to suspect that in later stages of optimization that the network may behave in a much less linear fashion as the units move out of their linear regimes, and so the optimization advantages of pre-training will diminish or even disappear.  This is consistent with the evidence from the literature.    In Appendix D you cite some evidence from other papers.  But the results with Hessian-free optimization and other methods are consistent with what is written above (believe me, I know this work very well).  Look for example at the results for SGD in Figure 3 of the Chapelle & Erhan paper that you cite.  SGD reaches nearly identical KLs on MNIST after 700 dataset passes when you compare random initializations vs RBM pre-trained initializations.  The Curves results seem to favor pre-training a small bit with SGD, but the difference is small (maybe 1.5x faster), and would likely become insignificant in the longer term as random-init SGD appears to be catching up before the graph cuts off.    My experience, along with that of many other people who have studies these methods is that if you run these kinds of experiments longer, the approximate 1.5-2x speed increase with pre-training wanes significantly over time, eventually reaching near parity with well chosen random initialization methods, so that overall not much time is saved, if any.  Page 8:  The discussion about what pre-training should do didn't make sense to me in multiple places.  For example, previously you said that you were assuming Sigma^11 = I, but I guess this is no longer the case.  You then say that the product of the weights converges to Sigma^31(Sigma^31)^-1.   But how can this be true unless N2 >= N1,N3?  When N2 < N1,N3, the product of the weight matrices has rank at most N2, while Sigma^31(Sigma^31)^-1 will be full-rank in general.  You defined a^alpha to be a vector before, but now it is a 'strength'?  When you consider taking W21 = R2Q^T where R2 is 'an arbitrary orthogonal matrix', what does this actually mean?   Orthogonal matrices are square, and so is Q, so does that make W21 square (i.e. N1 = N2)?  And if R2 is truly arbitrary, then it is meaningless, as any orthogonal matrix M can be written as R2 Q^T for some R2 (take R2 = MQ).  When you say 'Now consider fine-tuning on...' are you now taking the input and output not to be equal.  This is confusing especially since this sentence appears in a paragraph which starts by saying that we are now talking about auto-encoding with input = output.  In what sense is 'W^21 = R2D1V11' a 'task'?   I suspect you meant something else here but this sentence is very awkwardly phrased.  I gave up trying to understand what this part was actually trying to say.   Page 9:  What is the error being plotted in the figure?  Page 9:  That initializing a linear network with part of the optimal SVD solution (which if you take the full solution, will instantly give the optimal error) isn't too surprising.  It would be much more interesting to see if these initialization schemes work well in the nonlinear case.  Page 9:  The Glorot initialization scheme uses uniform random numbers, and I think there is a factor of 6 somewhere in there.  Gaussians with 1/sqrt(N) is definitely not what they end up recommending.  Also, they did their calculations for tanh units, although it should probably work for linear ones.   In my experience, getting the scale constants and other details right matters a lot in practice!  Page 9:  How can you have the weight matrices be random orthogonal matrices?  Orthogonal matrices are square and the weight matrices don't have to be.  Page 11:  Merely scaling g very large won't be any good.  The 'activity' will propagate, yes, but the units will completely saturate.  Page 12:  'Variance' isn't really what eqn 21 is measuring.  That implies deviation from the mean, which doesn't have to be 0 here.  The units could all be completely saturating (this is bad), but eqn 21 would still have a non-zero finite value.  Page 12:  The analysis done in section seems interesting.  It does sound similar to work done on Echo State Networks (ESNs) and initializations.  Have you compared to that?  Sutskever et al. (2013) also report finding empirically that there was a critical range of scale parameters around which learning seemed to work best.  These scale parameters were also applied to weight matrices which had their spectral radius initially normalized to 1, which is similar again to the ESN papers.   What is missing for me from this section is an application of these ideas to real networks.  Do these ideas improve on optimization performance?  A good comparison would be against a *properly implemented* Glorot-style initialization (see my above comments re. this), and/or something like the sparse initialization used in the HF papers.* Brief summary of the paper:  This work is mostly a mathematical analysis of the dynamics of gradient descent learning in *linear* multi-layer networks, which surprisingly displays a complex non-linear dynamic. It delivers analytic results and novel insights regarding learning speed, how it is affected by depth, and regarding optimal initialization regimes, drawing connections to the benefits of pre-training strategies. In particular it shows that orthogonal initialization is a better strategy than the appropriately scaled random Gaussian initialization proposed in Glorot & Bengio AISTATS 2010. Mathematical results are empirically validated with linear networks trained on MNIST. Some empirical results using non-linear tanh networks are also provided, convincingly showing that some of the insights gained from the analysis of linear networks can apply to the non-linear case.   * Quality assessment:  This is a very well written paper. I believe it offers a novel, thorough, and enlightening analysis of the dynamics of learning in linear networks. This is a worthy enterprise, and succeeds in uncovering novel insights that will likely also have an impact on practical approaches to training non-linear deep networks. It also paves the way to a more formal analysis of the dynamics of learning in non-linear deep nets. I found the analysis of initialization strategies in sections 3 and 4 particularly interesting due to their practical relevance.   My only concern, since this paper literally offers a lot, has to do with the ICLR policy on page limits (and text squeezing), that I am unsure about. To be checked.   * Pros and Cons:  + Thorough mathematical analysis, conveying valuable novel insights on the dynamics of learning in deep linear networks + Evidence that these insights can have practical relevance for improving approaches to training deep non-linear networks - TODO: Check ICLR policy on page limits??   * Detailed suggestions for improvements:  - Top of Figure 3 hides part of the above paragraph (a part that seemed important for understanding!) - Bottom of caption of figure 3 too close to main text (almost overlaps)  - I find the term 'learning time' ambiguous or ill-chosen, as you do not take into account the fact that deeper networks usually involve more computation per epoch. I suggest you clearlry define it as meaning number of training iterations (epochs) upon first introducing it.  - Top of page 5: there is an ill-placed newline between 'logarithmically spaced' and the rest of the sentence.  - p 9:  'mode strength was chosen to be small (u=0.001)'  ... because ... ?   'We show (Fig 6A, left, red curve)' -> 'We empirically show (...'  - Figure 6: 'A Histograms of the singular values' should be 'B Histograms of the singular values'  - p 11:  'netoworks' -> 'networks'  - Section 4: 'and phi(x) is any nonlinearity that saturates as ...' Any nonlinearity, really? This doesn't sound right, since any nonlinearity could incorporate some scaling that could override the role of gain factor g. You should more specifically characterize the properties of the nonlinearity, and state here which one(s) you will actually consider.  - Discussion: 'propery' -> 'property'  Appendix D.2: 'An analysis of this effect in deep linear networks' I believe you meant to write 'in deep nonlinear networks' since linear networks would all converge to equivalent solutions (at different paces) and thus could not exhibit any difference in generalization.",0,5085
"This paper investigates the construction of convolutional[-like] neural networks (CNNs) on data with a graph structure that is not a simple grid like 2D images. Two types of constructions are proposed to generalize CNNs: one in the spectral domain (of the graph Laplacian) and one in the spatial domain (based on a multi-scale clustering of the graph). Experiments on variants of the MNIST dataset show that such constructions can lead to networks much smaller than fully connected networks, with similar or better generalization abilities.  Of the 4 papers I reviewed, this is definitely the one I spent the most time on, but also the one I understand the least. It is a very dense paper, with lots of interesting ideas and observations, but without detailed enough explanations in my opinion, thus making it pretty difficult to follow. I need to mention, though, that my knowledge of CNNs is limited to seeing a few times figures of LeNetX architectures, and hopefully people more familiar with CNNs will be able to better grasp the ideas presented here.  My first suggestion would be to start with the spatial construction (2.2) rather than the spectral one, as it is probably easier to visualize. And speaking of visualization, a picture showing the neighborhoods and how the various scales are used would be very helpful (I believe I understand what is being done here, but to be honest I think I need a picture to be sure).  On the spectral construction, if there is a way to put eq. 2.2/2.3 in pictures, it would be great as well. Something unclear about these equations is that we seem to keep only a given number of components at each step, but the definition of y_k does not show that. The main point I failed to understand here is what it means for the group structure to 'interact correctly with the Laplacian'. Unfortunately the example in 2.1.1 is not clear enough for me: instead of saying it recovers a 'standard convolutional net', could you describe the exact net structure, in particular in terms of weight sharing, pooling and subsampling layers? For 2.1.2 I do not really have any specific comment/question -- I was quite lost at this point -- except you could say what is a cubic spline kernel and why it makes sense to use it.  For experiments, please first say in intro of section 4 that the full description of the projected dataset will be given in 4.2. I read the intro of section 4 several times, trying to understand what it meant (it did not help that I understood 'the 2d unit sphere' as 'the unit sphere in 2d', ie a circle)... before finally giving up (I think I got the idea now after reading 4.2 and looking at the pictures, but the description is still confusing: what are e1, e2 and e3 and what is the motivation in the choice of their norms?).  Some other comments on experiments: - I find the color maps hard to read. Would they look better in grey scale? (Fig. 4 (a)(b): how are we supposed to see it is the same feature?) - Codenames for models in the results tables do not seem to be documented. - In Fig. 2 is (a) really the finest? It looks like the coarsest.  Overall, I do believe it is a paper worth publishing. Taking advantage of the inner (unknown) structure of input variables is definitely a direction that could bring substantial improvements, the early experiments presented here are encouraging, and it brings some new ideas to the able. I hope, however, that the authors can increase the readability of the paper by adding more figures and explanations for those less familiar with CNNs.  A few more small remarks: - The O(1) used in the intro could be a bit misleading, I think it is more O(kd) with k the local neighborhood size and d the number of layers? (or O(d log d) if k decreases exponentially with d) - 'just an in the case of the grid': typo - In 2.1.1 I am wondering how important is the assumption of equal variance, and if you coul use the correlation instead - 'Suppose have a real valued nonlinearity': typo - 'by a dropping a set number of coefficients': typo - 'The upshot is that the the construction': typo - 'navie choice': typo - w_k-1 under eq. 2.4 should be W_k-1? - 'gauranteed': typo - 'the property that the subsampling the Fourier functions on the grid to a coarser grid': typo? - Figure references in section 4 are messed up (all are Fig. 4.1) - You could mention 'Learning the 2D topology of images' in the related work section> - A brief summary of the paper's contributions, in the context of prior work.  Exploiting the grid structure of different types of data (e.g., images) with convolutional neural networks has been essential to the recent breakthrough results in various pattern recognition tasks. This paper explores generalizing convolutional neural networks from grids to weighted graphs.  The reviewer finds weighted graph inputs best motivated by two datasets constructed at the end of the paper in order to test the proposed techniques. Both datasets are MNIST derivatives. The first subsamples the MNIST pixels in a disorganized manner, destroying the grid structure. The second projects MNIST onto a sphere, giving the input a more complicated manifold structure. Both of these are interpreted as weighted graphs in the natural manner. (A couple real-world examples of such structures occur to the reviewer: geo-spatial data and surfaces in 3D graphics.)  If we are persuaded that weighted graphs are an interesting type of input for a neural net, how can we generalize convolutional neural networks to them? The paper introduces two broad approaches. The first approach is to use a metric on the graph to define neighborhoods and build a locally-connected network.  The second, 'spectral,' approach is a bit more complicated. This can be understood as similar to how one can look at convolutional neural networks in terms of the Fourier Transform. A regular convolution can be thought of as pointwise multiplication in the Fourier domain. Drawing on the harmonic analysis of graphs, the paper uses the eigenvectors of the Laplacian, which have similar properties. Functions on the graph can be decomposed into coefficients of these eigenvectors and pointwise multiplied to achieve a convolution-like effect.  As mentioned previously, the authors test their techniques on two constructed datasets. For the subsampled MNIST, they are able to beat a fully-connected network with a locally-connected one, but only tie it with the spectral approach (though the spectral approach uses almost two orders of magnitude fewer parameters). For MNIST on a sphere, both approaches achieve slightly worse results than the fully-connected network (but, again, use fewer parameters).  > - An assessment of novelty and quality.  The reviewer is not familiar with this area but believes this work to be novel.  The ideas in this paper seem quite deep, and the experiments performed are interesting. In fact, the constructed datasets alone are interesting.  The exposition of the paper could be a bit stronger. This seems somewhat more sensitive because most people in the neural networks community will not have the mathematical background the paper presently requires.  A little more motivation and hand-holding could make the paper more accessible. That said, this doesn't seem like something that should be a barrier to publication.  > - A list of pros and cons (reasons to accept/reject)  Pros: * Generalizing convolutional neural networks to graphs seems like a valuable enterprise. * Explores some very intriguing ideas. In particular, the spectral generalization of convolutional networks feels quite deep. * Constructs cute datasets to test the ideas on.  Cons: * Paper could be more accessible (see above).Spectral networks  This paper aims at applying convolutional neural networks to data which do not fit into the standard convolutional framework. They do so by considering that the coordinates lie on a graph and using the Laplacian of that graph.  The topic is of utmost interest as CNNs consistently achieve very high performance while keeping the number of parameters in the network. I am glad to see advances in that direction. This excitement is moderated by the extreme difficulty with which I read the paper. In fact, most of the paper assumes advanced notions of harmonic analysis which I do not possess. I fully understand that such notions are necessary to fully apprehend this work but I would have appreciated if the authors had provided pointers or tried to give intuition on the concepts. As it is, only people familiar with the field will capture the full gist of the method.  Additionally, I find the experimental section a bit weak, in great part because of the sole use of the ubiquitous MNIST dataset (albeit distorted versions of it).  That being said, I want this work to be disseminated so that CNN can be used in wider contexts.  Pros: - Great extension of CNNs - Results are based on profound understanding of harmonic analysis and not just trial and error  Cons: - Extremely difficult to read for an audience not familiar with harmonic analysis - Experimental section a bit weak.",0,5086
"The manuscript articulates a problem with earlier solutions to the Collective Matrix Factorization (CMF) problem in multi-view learning formulations and proposes a novel solution to address the issue. The concern is that current CMF schemes ignore the potential for view-specific structure or noise by implicitly assuming that structure must be shared among all views. The authors solve this problem by putting group-sparse priors on the columns of the matrices. This allows private factors that can be specific to one or even a subset of the matrices. Also note that the use of variational Bayesian learning by the authors provides a large reduction in computational complexity relative to the MAP estimates used in some of the prior literature.  I agree with the importance of the problem being addressed since, clearly, the need to accommodate view- or subset-specific structure is going to be important in many real-world problems. Also noted is the elimination of the need for tunable regularization parameters.  There are a couple of typos in the first paragraph of section 4.2 (top right of page 4). The manuscript is heavily dependent on several of its sources for implementation details of the complete algorithm. This isn't a criticism, since the authors should not repeat details available elsewhere, but I think it is important to understand that this is necessitated by the complexity of the method, and this complexity is a small drawback and potentially an area for future improvement.  Another issue is that it would be valuable to see the proposed scheme compared to a wider variety of alternatives in the experimental section (mostly for context that elucidates the importance of CMF itself and therefore their improvement of it for certain applications). However, given the scope of the paper in general, this is a minor point.Collective matrix factorization (CMF) is a method for learning entity embeddings by jointly factorizing a collection of matrices, each of which describes a relationship between entities of two types. The set of rows/columns of a matrix corresponds to an entity type, while each row/column of the matrix corresponds to a different entity of the type. This approach assumes that all dimensions of the embeddings of entities of a particular type are relevant for modelling all the relationships they are involved in. This paper extends CMF by relaxing this assumption and allowing embeddings to have dimensions used for modelling only some of the relationships the entities are involved in. This is achieved by extending the model to include a separate precision for each embedding dimension of each type to encourage group-sparse solutions. The authors propose training the resulting models using a variational Bayes algorithm and show how to handle both Gaussian and non-Gaussian observations.  The paper is nicely written and makes a small but novel extension to CMF. The resulting approach is simple, and seems scalable and widely applicable. The experiments are fairly small-scale but are sufficient for illustrating the advantages of the method.  Corrections: In the section dealing non-Gaussian observations, the pseudo-data is referred to as Y instead of Z_m.  The description of variational Bayes as 'minimizing the KL divergence between the observation probability and ...' is not quite right, as it seems to describe KL(P||Q) instead of KL(Q||P).  Section 8: 'unability' -> 'inability'",0,5087
"This paper presents an approach that considers a sequence of local representations of an image, in order to classify it into one of many labels. The approach decomposes an image into multiple non-overlapping sub-windows, and tries to find a sequence of subsets of these sub-windows that can efficiently lead to classify the image. The idea is interesting as it could potentially classify faster by concentrating only on the relevant part of the image; on the other hand, the training complexity is significantly increased (and I suspect for the approach to work we should include many more sub-windows at various scales and potentially with overlaps). The proposed approach is not compared to any other approaches, for instance the work of Larochelle and Hinton, 2010. The results are encouraging but not groundbreaking: it seems one needs to see a significant portion of the image in order to get similar or better performance than the baseline, so it's not clear the proposal works that well. I wonder if the policy used to guide the search space among sub-windows could be analyzed better.This paper describes a method to select the most relevant grid regions from  an input image to be used for classification.  This is accomplished by training a chain of region selection predictors, each one of which outputs the k'th region to take, given the image features from k-1 already-selected regions.  Each selector is trained to choose a region that leads to an eventual correct classification, given already-trained downstream selectors.  Since downstream predictions are required for training, the chain is trained last-to-first, and random region selection is used to generate training inputs at each stage.  Both the conditional region selection chain and its training method are interesting contributions.  The method is evaluated on two tasks, playing vs. holding a musical instrument (PPMI) and outdoor scene classification (15-Scenes).  Here, I wish the authors were more detailed in their descriptions of these tasks.  In particular, I'm a bit unsure whether the PPMI task is a 12-way classification on musical instruments, or an average of 12 binary classification tasks (playing vs holding), one for each instrument.  I think it's the latter -- if so, I'm also unclear on whether a different selection/classification chain was trained and tested for each of the 12 subsets, or if a single classifier was trained over the entire dataset.  Still, the proposed method beats a random selection baseline for both tasks (though not by much for 15-Scenes), and for PPMI it also beats a baseline of including all regions.  The latter is a particularly nice result, since intuitively region selection should stand to help performance, yet such gains can be hard to find.  Evaluating the 12- or 24-way instrument classification task for PPMI would have been good here as well, though, as there is clearly a compatibility between region selection and this data and/or task, and this may help provide insight into why that is.  Pros:  - Interesting and new method of region selection trained for classification - Shows a nice gain in one task and reasonable results in another - Interesting discussion sheds light on how the method operates (figs 7, 8)  Cons:  - Tasks and datasets could be better explained   Questions:  - Why does selecting 8 regions beat using all 16 for PPMI but is only about the same for 15-Scenes?  Some discussion on the difference between the two datasets and their fit with region selection would be nice here.  - Fig. 8:  I might have expected the B=8 (right) histogram to have its highest values mostly where the B=4 (left) histogram does, since one would think the best regions would be required in both cases.  However, region 3 (x=1,y=3) seems to be used with more frequency for B=4 than B=8, for example.  Why does this occur?  - Is it possible to continue retraining the classifier and selectors?  Currently the chain is trained once, starting with the classifier and proceeding upstream.  Yet by doing this, each stage must be trained on a random sample of input regions, which can include many more configurations than would be seen at test time.  Could each stage (particularly the final classifier f) be iteratively retrained given *all* selectors?  Would this help by adjusting the training distribution closer to the test distribution and allowing better use of resources, or might it lead to worse generalization by narrowing the training set too much?  - Alg. 4 adds a sample to the training set only if it leads to a correct prediction.  But what if no region has this property -- is it better to ignore these cases, or should some be included (perhaps by trying to predict the choice closest to correct)?  Surely such cases will arise at test time, and the consequences of ignoring them isn't entirely clear.  I suppose there's an argument to be made that the classifier will eventually fail anyway, so it's better to bail on these cases and concentrate the predictor's resources only on those where it stands a chance.  But in cases where the classifier is wrong, might this also lead to more arbitrary region selection and more drastic types of mistakes (e.g. mistaking a clarinet for a harp vs a recorder)?This paper tackles the problem of deciding where to look on an image. The proposed solution is to start from a center region and use the information extracted to decide what region to examine next. This is trained through reinforcement learning.  The paper is well organized and clear. Experiments on 2 benchmarks (15 scenes and people playing musical instruments) show that selecting a smaller number of subregions of the image does not result in a big loss of accuracy, or even improves accuracy by eliminating noise from information-poor regions.  Figures 5 and 6 have unreadable annotations, this should be fixed and/or the caption under the figure fleshed out to better describe the results.  The datasets are limited and this would need to be extended to more realistic datasets, but the problem tackled is important and the proposed solution is a welcome step in this direction.",1,5088
"The paper describes a second order stochastic optimization method where the gradient is computed on mini-batches of increasing size and the curvature is estimated using a bound computed on a possibly separate mini-batch of possibly constant size. This is clearly a state-of-the-art method. The authors derive a rather complete ensemble of theoretical guarantees, including the guarantee of converging with a nice linear rate.  This is a strong paper about stochastic optimization.  In the specific context of ICLR, I regret that the authors did not explain why this technique is useful to learn representations. The basic setup is that of maximum likelihood training of an exponential family model.  In practice, there are many reasons to believe that such a technique would work on mixture models or models that induce representations (although the theory might not be as simple.)  I believe that this paper should be accepted provided that the author pay at least some lip service to 'representation learning'.The paper introduces a certain second-order method that is based on quadratic upper-bounds to convex functions and on slowly increasing the size of the batch.  A few results show that the method is well-behaved and has reasonable convergence rates on logistic regression.   This work is very similar to Hessian-free optimization, because it also uses CG to invert low-rank approximations  to the curvature matrix, and it has comparable cost but greater memory complexity due to its need to store many parameter vectors.   Likewise, it builds up on previous work that finds quadratic upper bounds to convex functions, but a quadratic upper bound seems restrictive, and perhaps a quadratic approximation would be more appropriate.   Pros:  Method is somewhat novel.   Cons:    - Experiments very small and unrealistic (sometimes tens of dimensions), and there is no comparison with Hessian-free optimization  - Theorem 1 part 1 is wrong:  the method is biased, because while E[Sigma_S] = Sigma,  E[Sigma_S^{-1}] != Sigma^{-1}.  In general,  second order methods that use modest numbers of samples for the curvature matrix are necessarily biased.  - The paper has two ideas:  a certain second order method, and a simple scheme for growing the minibatch.  But which of these is essential?  Would we get similar results if we didn't grow the minibtach?  How large would the minibatch end up being?   What if the other methods grow their minibatch as well, do  they become competitive?This paper looks at performing a stochastic truncated Newton method to general linear models (GLMs), utilizing a bound to the partition function from Jebra & Choromanska (2012).  Some basic theory is given, along with some experiments with logistic regression.  Stochastic or semi-stochastic truncated Newton methods such as Hessian-free optimization, and the work of Byrd et al. have already been applied to learning neural networks whose objective functions correspond to the negative LL of neural networks prediction under cross entropy error, which is like taking a GLM replacing theta in the definition expression with g(theta), where g is the neural network function.  In the special case that g=I this correspond exactly to logistic regression.   One thing I'm very confused about is what the bounding scheme of Jebra & Choromanska (2012) that is applied in this paper actually does.  Since it involves summing over all possible states, it can't be more efficient than just computing the partition function, and its various derivatives and second-directives directly.  Why use it then?  The Hessian of the negative LL of a general linear model will already be PSD, so that can't be the reason.    Detailed comments:  Abs:  What do you mean by 'maximum likelihood inference'?  Do you mean estimation?  Learning?  Page 1: There are versions of those batch methods that can use minibatches and they seem to work pretty well, despite lack of strong theoretical results.  I guess though that this would fall into the category of what you are calling 'semi-stochastic'?  Actually, having read further it appears that you would only call these stochastic if the size of the minibatch grows during optimization.  Page 1: When you say that stochastic methods converge in less iterations that batch ones, this makes no sense.  Perhaps you meant to say passes over the training set, not iterations.  Page 2: The abstract made prominent mention of partition functions.  Yet, the introduction doesn't make any mention of them, and seems to be describing a new optimization method for standard tractable objective functions.  Your intro should mention that you will be focusing on generalized linear models (what you are calling generalized linear model) and extending the work of Jebra & Choromanska (2012) to the stochastic case.  This becomes clear only once the author has read well passed the intro.    Page 3: I think you should have some discussion of this Bound Computation subroutine.  It looks quite mysterious.  Does the order at which the loop goes over the different y's matter?  I can't see why it wouldn't, and that seems problematic.    Page 3:  You don't define n.  Is this the size of Omega?  Page 3:  'dataset Omega'?   I thought Omega was the set of values that y can take.  Page 4:  The statement and proof of Theorem 1 seems similar to one of the theorems from the Byrd et al (2011) paper you cite.  And like that result, it is extremely weak.  Basically all it says is that as long as the curvature matrices are not so badly behaved that the size of their inverses grow arbitrarily, multiplying the gradient by their inverses won't prevent gradient descent from converging, provided the learning rate becomes small enough to combat the finite amount of blowing up that there is.  It says nothing about why you might actually *want* to multiply by the inverse of this matrix.  But I guess in this general setting there is nothing stronger that can be shown, since in general, multiplying the the inverse curvature matrix computing on only a subset of the data may sometimes do a lot more harm than good.  Page 6:  I don't see why the first part of Cor 1 should be true.  In particular, it isn't enough to for the matrix to be positive definite everywhere.  It has to be positive definite with a lower bound on the smallest eigenvalue that works for all x (i.e. a uniform bound).  Page 6:  Theorem 2 should start 'there exists mu, L>0, and rho>0'.  You are missing the rho.  Page 6:  This theorem doesn't seem to show that multiplying by the inverse curvature matrix is actually helping.  One could just as easily prove a similar bound for standard SGD.  Like this bound, it certainly wouldn't give linear convergence for SGD (an absurdity!), due to presence of the Ck term in the bound, which will eventually dominate in stochastic optimization.   Page 6:  Could you elaborate more on the point 'further regularizes the sub-problems'?   There is a detailed discussion of this kind effect in 'Training Deep and Recurrent Neural Networks with Hessian-Free Optimization', section 8.7.  What in particular does [38] say about this?  Page 7:  In section 12.1 from the above mentioned article, a similar result to Lemma 4 is proved.  It is shown that the quadratic associated with the CG optimization is bounded, which implies the range result (since if the vector is not in the range, the optimization must be unbounded).  They look at the Gauss-Newton matrix, but for general linear models, the Hessian has the same structure, or the matrix from the bound from Jebra & Choromanska (2012) has the same basic structure as this matrix.",0,5089
"This paper presents a feature ranking method for RBMs and a method to transfer RBM representations from a source domain to a target domain in an adaptive manner. While the method achieves good performance when transfering between very similar domains, the paper suffers from the following problems:  - The proposed ranking function is equivalent to the L1 norm of the weight vector associated with each hidden unit, which makes the method qualitatively similar to PCA in that it prioritizes the components that explain the most variance in the data (Bengio, JMLR 7 (2011) 1--20). However, it is not obvious from the experiments that the proposed ranking has an advantage for transfer learning. A connection with the existing literature also seems lacking.  - To the best of the reviewer's knowledge, the adaptive learning method for RBMs (with theta=1) is novel, but many very similar approaches to transfer learning using RBMs already exist and, once again, a review and comparison with competing approaches would be crucial.  - Despite the simplicity of the concepts presented, I have found the paper confusing at times. For example, what exact setup does each row correspond to in Tables 3 and 4? What is the difference between the last and second to last rows?  - Finally, the experimental section considers only the task of character recognition.  Other questions:  - In Figure 2(b), would we get a different interpretation if the fitler bases were normalized? Using the L1 norm of the bases for scoring, it is hardly surprising that the low-score bases have a lower magnitude when plotted on the same scale. - In Figure 3 (sparse RBM), are the bases normalized by the L2 norm?The submission proposes a scoring method for RBMs to rank hidden units according to their information content, and to use that ranking to prune networks and to do transfer learning. The transfer learning mechanism uses the ranking in the original domain to choose which hidden units to use when training in the target domain, and then uses the output of those transferred units to affect the training of the added units.     The submission is clearly written, has relevance and interest to the ICLR community, and a number of experiments are described that validate the method. The results are promising, however there are a number of missing experiments that would have helped to validate the method. An important baseline experiment would be to compare the pruning of low-scoring units with the pruning of randomly selected, or even high-scoring units. Similarly, with the transfer learning experiments, it would be important to verify that the author's metric for replacement of sub-networks is optimal, or at least better than random choice. The submission also suffers from not making comparisons to other approaches to transfer learning in neural networks.    pros: well-written, relevant and interesting cons: simple approach, not adequately validated, not adequately compared against other transfer learning approaches.",1,5090
"Summary This paper proposed a probabilistic framework for learning generative models where the 2D Similarity transformation is a part of the top-down process and a ConvNet is used to initialize the posterior inference.  Though this paper is clearly interesting and important for both deep learning and computer vision community, it could potentially be improved. The main problem is that the current  submission does not include all of necessary details (please see the below), especially for the experimental section.   Pros -- well-written and organized -- an unified probabilistic graphical model framework for generation and detection  -- interesting experimental results    Cons The current submission lacks of some important details:      The details of the ConvNet and its training process;      How many steps of Gibbs samples executed in step 3 in fig.3? For a Gaussian RBM trained on face image the mixing of its markov chain could be extremely slow;       What do you mean 'training DBN by FPCD' in page 7? Do you mean wake sleep algorithm?      How do you determine the threshold for logP(x|u,v)? I believe this threshold could be crucial for learning without gaze labels. There are not any baseline for comparison in the section of experiments. Which part of the framework is more important? The DBN or the convNet? At least you could try to change DBN to a Gaussian mixture model (and/or change the ConvNet to a randomly guess).  Is the Monte Carlo EM algorithm described in section 5 valid? Is there any guarantee? It can be seen in section 6.2 that the E-step could fail to localize faces. How could you prevent the DBN from learning those false faces. To be honest, I am not convinced that a RBM (or DBN) trained on face images can be such a good generative model that always give higher free energy for non-face images.   Minor comments Last paragraph , Page 2: Combining two energy functions of RBMs forms a DBM. Fig. 3: What do you mean about the arrows above the ConvNet in step 2 and step 4? Section 6.2: please explain how you do inference in a DBN.The authors present an attentional generative model, inspired by the routing circuits model of Olshausen et al. (ref. [14]). Similar to some earlier work, attentional aspects result from the model essentially being misspecified to only represent a single object, treating everything else in the image as noise implicitly (besides the cited [18], Chikkerur et al, 2009, is another relevant example). The model consists of a Deep Belief Net (DBN) that models individual objects (here: face patches) in a canonical reference frame, and a transformation operation that scales/rotates/translates the object to be positioned in a larger image. Inference iteratively updates both the internal representations and hypothesized positioning in the image, and is based on Hamiltonian Monte Carlo (HMC) as well as a convolutional net (ConvNet), which makes initial guesses.  Pros: I find the topic interesting, and the approach original and creative.  Cons: The paper suffers from quality and clarity issues. Moreover, this is not primarily a biological model, so the question is whether the proposed approach is promising for the machine learning application. As with other attention models, I find the evidence lacking that attention is really needed here to solve the task or that the approach works better than other, perhaps simpler alternatives.   Details:  First of all, the paper has a number of typos and grammar issues that should be fixed (three examples right in the abstract: 'enormous sensory stimulus available in the neocortex' is an awkward formulation; 'we describe for [a?] generative learning framework', 'signals from [a] region of interest').  I understand that biological attention this is not the primary focus of the paper, but I would have enjoyed a bit more discussion of the notion that attention corresponds to transforming input into an object centered frame, other than citing ref [14]. As far as I know, this is a speculative proposal and it would be nice to discuss whether there is recent evidence or theories supporting it. Similarly, the authors could have explained better how their framework is to be interpreted in terms of biological attention, if at all. For example, they appear to contrast their model to the 'covert' one of [18], and similarly the term 'gaze variables' suggests an interpretation in terms of overt eye movements. But note that the Olshausen model was supposed to model covert processes, i.e. internal routing of information with fixed retinal input. I also take it that we're not supposed to take the 'gaze' interpretation too literally, seeing as arbitrary scaling and rotations are involved...  The description of the model could be made a bit clearer. For example, the full graphical model could be displayed in Fig 1, rather than 'hiding' parts in the black box. {p} appears to be introduced twice (Section 3, paragraphs 3 and 5), and later p is used as momentum variable in HMC. I also get the impression that some parts of the model are not explained perhaps because they further complicate the model. In particular, the figure and detailed description only cover inference in a RBM. However, a DBN is what is actually used. Presumably the the hybrid directed/undirected DBN further complicates inference?   Similarly, the architecture and training parameters of the ConvNet are not described at all, even though the latter seems to be what does the main work when it comes to localizing the faces. Speaking of which, I found iteratively predicting the transformations with the ConvNet interesting--it would be good if the authors could comment on whether this is a novel contribution or whether there are related approaches. I'm less convinced by the performance of the HMC, which mostly seems to fine-tune the window position. Note that the authors motivate the HMC in Section 3 by writing that it helps 'with jumping out of local maximum' [sic], but then justify the ConvNet in Section 4 by writing that HMC tends to get stuck in local optima...  Also Section 6.1, is the full model run (including the DBN) here? If so, what are the model parameters? The detection performance is only compared to two, presumably baseline methods. What is state-of-the-art on this dataset? Also, footnote 1 says that u was initialized to be centered. Depending on the dataset, isn't that potentially cheating?  Section 6.2: the authors state that the novelty of their approach is demonstrated here. The authors essentially first train their model with labels, and then use this first model as a face detector on a second, unlabeled dataset to localize the faces and train a second DBN. Here it was not clear to me whether they train the second DBN from scratch or merely fine-tune the first DBN pretrained with labels (they say they train the second DBN with FPCD. FPCD is for RBMs, so are they referring to layer-wise pretraining?). Either way, the main issue here is that the authors don't actually quantify the performance of the second model, so it is not clear if anything is gained with this approach, over just taking the first model that was only trained on the labeled data.  Lastly, I found Section 6.2, 'Inference with ambiguity', the most interesting bit, as it actually demonstrates the interaction between the canonical face model and the localization. Unfortunately, this part is very short. Perhaps the authors can expand this in future work.  Leftovers:  * Section 2: 'If a second level RBM is used to model the activities of the hidden units of the first layer GRBM, we can combine their energy functions to form a Deep Belief Net (DBN) [2]'. Doesn't simply combining the energies give a deep Boltzmann machine rather than a DBN? * Section 4.1: 'the spatial frequency of the natural image signals can form many local minima': this should be expressed better. * Section 4.1: '(e.g. 72×72) and v (e.g. 24×24)' here and elsewhere: why the 'e.g.'? Just say that this is what you are using. * Figure 3: the caption should clarify that this is not the full inference process, only the initial part. * Figure 4: there seem to be artifacts in the images? * Figure 5: the fonts are too small to be readable.The proposed solution is a conditional model (conditioned on a large input image I), which augments the typical RBM with two additional sets of random variables: transformations parameters $u$ and a 'steerable' visible layer $x$ (a patch of I, whose location is defined by $u$). An additional term in the energy function ensures that low-energy configurations are given to settings of $u$, for which $x$ is close to $v$ in an L2 sense. Inferring the transformation parameters in this model is thus analogous to the task of objection detection.  To shortcut this difficult optimization process the authors propose to use Hybrid Monte Carlo (HMC), but where the states $u$ are initialized via a convolutional neural network (CNN).  I find the general idea behind the paper compelling: the problems of attention and scaling up of generative models are important ones which warrant further attention. While a solid step in this direction, the experimental section does not convincingly show evidence in favor of the model, in particular, it lacks proper baselines.  Section 6.1: How do state of the art object detection / tracking algorithms do on this task ? Normalized cross-correlation and L2 template matching do not seem like appropriate baselines.  Section 6.2 aims to show that the 'novelty of our model is that it can learn on new large images of faces without label information'. Unfortunately, the authors provide little evidence in favor of this. After training a DBN and the approximate inference module on Caltech, the authors report a successfull detection rate of 951/1418, unfortunately without any context or comparisons.  How does this compare against a standard Viola-Jones detector ? How would the model perform by simply clamping 'v' to the average face image ?  I would not expect the proposed generative model to outperform a dedicated face detection module, but the baseline seems necessary to provide context.  Furthermore, how are the samples in Fig. 6b evidence that the DBN learnt the Multie-PIE dataset ?  Are the samples 'more' qualitatively similar to Multi-PIE than Caltech ?  Without being familiar with Multi-Pie, this is not at all obvious. Samples in 6b appear similar (but worse) to 6a. A quantitative analysis seems necessary.  The experiments of Section 6.3 is interesting at a high-level, but its execution seems flawed. It appears that the authors clamped 'v' to the very patch (i.e.  cropped face image from the test set) which they intend to detect (?). This seems like an all too trivial task. To highlight the benefits of having a proper generative model, v could have been clamped to e.g. the face of a different individual but of the same sex as the 'target'. A more convincing application might be with a classification RBM whereby clamping label units results in 'attending' to the corresponding areas of the image.  That being said, I do look forward to the next revision of the paper.   Other points:  * A threshold parameter is used for detection in Section 6.2. How was this parameter chosen ? Precision/Recall seems like the only relevant measure here.  * I found Section 3 to be particularly confusing, in large part due to the notation which obfuscates the relationship between $x$ and $u$. From the energy function, one could not be blamed for thinking that the model is a simple 3-layer DBM (with constant term f(u), to be rolled into the partition function).  * I also found the description of the transformation parameters and warp w a bit confusing. For instance, 'used to rotate, scale [...] the canonical v' is a bit misleading, as v never actually undergoes any transformation. At a high-level, it seems simpler to think of u as selecting a patch x of I (where the probability of u is proportional to the L2 distance between x and v) ? One possible suggestion to help with clarity would be to delay the description of the warp transform to Section 4, and describe the model in terms of a generic transform T(I, u) ?  * Overloaded notation for p (both pixel coordinate and momentum variable)  * Contradictory statements about HMC and local minima.  * lots of typos   PROS: * novelty of model and potential for applications * approximate inference scheme (which continues the trend of using function approximation for approximate inference) CONS: * weak experimental evidence (lack of proper baselines) * clarity of presentation",1,5091
"The paper presents some empirical analysis of various types of neural network with regard to finding examples close to the training examples which are misclassified.  It also proposes a technique to train on these perturbed examples, and they mention some improved results on MNIST (it would be nice to see more focus on this method, rather than so much on the analysis).  Novelty: it's very novel, I'd say. Quality: so-so.  The experimental justification of the training method is quite    limited, and there are some flaws in the analysis in my opinion.  Pro: The paper has an original idea. Con: There are various weaknesses in the analysis and the presentation.   The paper gives the impression of being a little half-baked.  But I think it's   interesting enough to publish; the issues could probably be fixed without too   much trouble.    Detailed comments are below.  ------ typo in abstract: contains of->contains extend->extent  When you say 'Our experiments show that ... properties.'...  I don't think what you are talking about is really a scientific experiment.  It's something anecdotal and informal.  A proper experiment would have human test subjects judging the extent of semantic similarities of sets of images, where the tests had been chosen using different methods and the test subjects didn't know which method the set of images had been chosen from.  Until you do this and show statistical analysis of some kind, you can't really say anything for sure. Of course, you can still report the anecdotal observations, just don't say they are experiments.  When you say 'this can never occur with smooth classifiers by their definition'...  I think your definition must be problematic.  There will always be a classification boundary, and it will always be possible to find images that are close to that boundary.  The only question is how easy is it to find such images.  Unless you quantify that somehow, and compare across different modeling strategies (e.g. the average distance you have to go to find the negative example), I don't think you can say very much.  Fig. 5: I think you can safely assume the reader knows what an ostrich is.  You might want to check that you don't have your description of hard negatives backwards (I'm not a computer vision expert, but something seems wrong).  disclassified->misclassified  The sentence 'The instability is expressed mathematically as...' is to me quite problematic.  I think you are trying to formalize something that is really a little vague, and not doing it quite right. The rest of the spectral analysis is interesting though, but with the stuff regarding A(omega), I think it would be more useful if you tried to explain in words what is happening rather than force the reader to wade through notation.  I don't think the comparison to the set of rational numbers is useful, given that we know the last hidden layer is a continuous function of the input (and thus the sets we're dealing with are much more well-behaved than the set of rational numbers).  Overall, some of the analysis and explanation in the paper is a little problematic, and sometimes over-stated, but the basic idea is I think quite interesting, and I think you should emphasize more your training method, which in my mind is probably more interesting than the analysis itself.This paper studies the interesting case of showing where deep nets fail. It proposes a simple mechanism to demonstrate that even small (visually almost imperceptible distortion) to the training samples can cause drastic changes in recognition (ie, decision boundaries).   This is a fresh perspective and I applaud the authors for providing interesting diagnostic means to help to understand deep nets.  A few comments:  (1) the paper actually does not state how to address the issue of deep nets learning very rugged decision boundaries:  a few ideas come into mind, though. Using drop-out in training process might eliminate this problem? If # of classes is small, would this problem be less severe? (For example, the required distortion is large so one can detect ).     (2) I could not follow the analysis in section 4.3 -- not clear where it leads to  (3) Looking at Table 1/2: I notice that the required distortion is larger for more complex models. So perhaps this model needs to be better regularized?  Have you tried contractive autoencoders?The paper highlights two counterintuitive properties of neural networks: (1) a tendency to encode factors of variation in multiple units rather than single units, and (2) an absence of local generalization. The first hypothesis is tested by finding a set of samples correlating most with certain directions in the feature space and visually inspecting their similarity. The second hypothesis is tested by generating 'adversarial samples' that are close neighbors to the data points but predicted of different class.  I think that the first property makes a lot of sense in the context of sigmoidal networks, as the squashing effect of the sigmoid clearly encourages the representation to be redundantly encoded in a large number of neurons in order to preserve the principal components of the data. However, I am wondering whether these findings apply broadly or whether some modeling decision (e.g. sparsity, type of nonlinearity, competition between units, etc) do favor the alignment of independent components to the canonical coordinates.  The second property is very interesting and shows that imperceptible deviations from a data point in the input space can cause the neural network to change its prediction. This finding is likely to generate further research on the nature of this effect. In particular, I would find it interesting to study whether these adversarial samples are a degenerate effect caused by the limited capacity of the network or a more fundamental issue with the learning algorithm that would occur for networks of any size.  Authors provide several examples of adversarial samples for both the MNIST and the image datasets. However, I could not find whether these samples are typical (with distortions corresponding to the average minimum distortion) or whether they have been preselected to have low distortion.",0,5092
"This paper addresses the problem of zero shot learning for image classification. Like in their recent NIPS2013 work, the authors relies on an embedding representation of the classes inferred from a language model. Their prediction scheme first predicts an embedding vector which linearly combines the vectors representing the n-best prediction among the training classes and  then looks for the nearest neighbors of the predicted vector among the test class embedding.  The paper reads well and appropriate reference to related work is given. The proposed approach is very simple and yet improve over the DEVISE classifier. I have however a concern regarding the results which either indicates (i) the implementation differs from the paper description, (ii) I missed something. It seems to me that hit@1 for ConSE(1) predicts the embedding of the best prediction of the 'Softmax baseline' over the 1,000 training classes, i.e. f(x) = s(y_0(x,t)) and outputs its nearest neighbor in the search space. When the search space include the training classes, it should output y_0(x,t). This implies that in table 4, the first column should contain hit@1 for ConSE(1) = 55.6. Similarly, the result of hit@1 for ConSE(1) in the (+1K) results should be 0. This is not the case, could you explain/correct?  Apart from this technicality, this is a good paper. The approach is simple and improves the state of the art. It might be further improved by a deeper analysis of the errors, possibly grouping them according by type of classes, looking at the accuracy of the convnet on the classes related to the labels or the quality of the corresponding text embeddings.This paper proposes a method for performing zero-shot learning of an image labeling system. The proposed method is very simple and yet general and efficient: it consistently outperforms the DeVISE system presented recently on the ImageNet benchmark.  The paper is nicely written and ConSE is actually so simple that the paper is not too complicated too understand anyway. But I have no problem accepting a paper even if the method is simple, if it proves to be efficient. ConSE appears to be but some questions/comments remain.  The method is simple so I would have expected more studies/experiments/discussions to explain its good performance. A simple intuition is given is Section 4.1 with the (funny) 'liger' example. But this could perhaps detailed more. For instance:  - it is claimed several times that ConSE can be used with any semantic embeddings. Is this really true? According to the 'liger' example, ConSE works because s(liger) ~ 0.5*s(tiger) + 0.5*s(lion). It is true for the skip-grams embeddings, since it has been shown that such linear relationships (and translations) were existing among those embeddings. I'm not sure that one can claim that all word embeddings work the same way and have such linear relationships. Without such property within the embedding space, would ConSE still perform well?  - how important is it to normalize the T top probabilities of the combination? Doing so, they are implicitly calibrated on the train labels, whereas one would like better to calibrate them on train + test labels. In the conclusion, there is an interesting comment regarding the norm of the convex embedding combination, especially when probabilities are not normalized, indicating that it gives a measure of the confidence of the prediction. I feel like the main point of the paper might be there but the paper does not exploit it well. Basically, one of the most difficult problem in zero-shot learning is to detect whether to choose a label among training labels or test labels (before even trying to choose the right one). That's why for me the most interesting (and realistic) experiments of the paper are when train labels are also added to the candidate label sets (+1K setting). These show that the bias towards training labels is big, especially at top-1 (this is not surprising). The intuition about the norm of the convex combination and its connection to confidence seems to be promising to soften this bias, but this is just sketched unfortunately.  I wonder why the performance of ConSE(1) in hits@1 is not 0 in the +1K setting. If I understand correctly, the output of ConsE(1) is simply the embedding of the top-predicted train label and hence, the closest according to the cosine distance should be this very train label. Since, no test example is labeled with a train label, it should always be a mistake.  On a more general point, it could also be discussed how significant are the results. I mean, perhaps ConSE outperforms DeVISE, but performance are very low (5% of hits@10 in the most general setting). Can we consider that such poor performance is still actually meaningful and useful in some way?  Minor: - Tables 1 & 4 are in %, whereas tables 2 & 3. It should be consistent.This paper presents a simple but really neat idea of combining semantic word vectors trained on text with a softmax classifier's output. Instead of taking the softmax output as is, it uses its probabilities to weigh the semantic vectors of all the classes which allows the model to assign labels that were not present in the training data.  The results are not always better than previous work from the group but in many settings they are.  Simple but overall very cool idea.",0,5093
"This paper presents an extensive empirical evaluation of various methods and frameworks for domain adaptation, that is, when training (i.e. source) and test (i.e. target) data are expected to be sampled from different - but related - distributions. The study is carried out for the image labeling problem, with the CNN of Krizhevsky et al. as base classifier for the source domain. The main goal is to explore various strategies for applying this network trained for 1,000 categories on ImageNet to other images of the same categories but taken in different conditions (basically taken from a webcam here). The main ideas is to use features from the CNN architecture (different layers are considered) and feed them to various domain adaptation techniques, supervised or not. For the supervised case, the setting is drastic and allows only a single labeled example per category from the target domain.  ----  The paper tackles an important issue and is sound; the numerous experiments appear to be reliable. It is clearly written despite some typos. But it also raises some questions.  In the beginning of Section 3, it is written 'We propose a generic framework to selectively adapt the parameters of a deep network', which is rather ambitious. The experimental study is extensive and covers many methods but I'm unsure that this actually provides a generic framework for deep learning because (1) this only covers CNNs and (2) it is more the application of existing methods than the definition of a new scheme. The main conclusion of the paper, which is that transferring from ImageNet is more efficient than from Amazon, is fine but does not lead to a framework definition.   A very interesting point of the paper is to provide some elements about which features from a CNN should be fed to adaptation methods (which layer basically). I think this is a key problem and I regret that the paper does not elaborate too much on this point. There is some discussion but it seems that the quality of adaption given features from a certain layer can also indicate the abstraction level of the features learned by the CNN. Perhaps, one could elaborate on this.  When labeled data is unavailable, which is the case here for the target domain, training is complicated (and that's what's addressed in the paper with at most 1 example per category) but model selection is also tricky. For instance, setting the C for the SVMs of the Daume III method or the alpha of the linear interpolation can be complex. This crucial problem is not really tackled in the paper since hyperparameters values are either set through unjustified heuristics or left somewhat undecided. The alpha of the linear interpolation seems to be chosen while looking at the curves of Figure 1 (a), that is, by looking at the evaluation set.. Nothing is said on how the C used for the SVMs of Daume III is chosen and its value is not given. This is problematic since these are the 2 best performing methods.  It is said a couple times that adaption methods based on SVMs (such as Daume III) might suffer from long training duration because of the large number of training examples. I disagree. I suppose that the SVM is using a linear kernel (it should be stated). If this is the case, then it has been shown many times that linear SVMs can be very efficient in training time and memory usage. I suspect the SVM training time to be somewhat negligible compared to that of the CNN.  The test set is rather small, only 160 images, which are split into 20 random splits of 8 images. I wonder if averaging test results obtained on such small sets, which hence never contains all 16 categories, makes sense. I would rather like some statistical significance paired tests for instance.   Minor remarks: - Section 2: typo: reported in 4 -> reported in Section 4 - Section 4.2: how many unlabeled examples from the target are used for the unsupervised adaptation approaches, is it 10 per categories as in the test set or 20 as for the source domain train set? - Section 4.3: typo: make use a subspace -> make use of a subspace . - Section 4.6: the discussion on the size of the subspace is useless. It is pretty obvious that a dimension lower than the number of categories is detrimental.The submission tackles the problem of one-shot classifier adaptation between biased datasets that contain overlapping object categories. The authors design a number of experiments to evaluate known transfer/adaptation approaches on deep convnet features taken from the last 3 layers of the Krizhevsky network. The basic approach is that the convnet is trained on LSVRC 1000-category data, then 16 categories are chosen that overlap with categories in 2 other datasets (amazon and webcam). Feature representations are taken from one of the layers of the network using amazon or imagenet data and webcam data, and adaptive classifiers are tested using the amazon or imagenet source and a single webcam image as target.  There is little that is innovative in the submission, since it uses only published or trivial approaches for the convnet and the domain adaptation and the empirical results are not broad enough. Moreover, the work does not contribute to our understanding of learned representations, so I see little relevance for ICLR.   The paper is well-written and offers a number of intuitive explanations for the results, although some of the conclusions don't seem well-justified given the limited evidence from only one target domain. The authors identify an interesting next step of doing learning in the convnet layers using feedback through the adaptation classifier, which could be worthwhile.This paper studies dataset bias problem. It investigates whether using large source datasets can eliminate dataset bias. The paper shows that such datasets reduce but still do not remove completely dataset bias.  It also shows that deep learning features are useful in helping domain adaptation.   This is a largely empirical study of the important issues.  The observation made the paper is important and thought-inspiring and worth reporting.  It might be interesting to report all layers (instead of just DeCAF6 and DeCAF7)'s performance on adaptation --- is it always the case that high-level layers are better at adaptation?   GFK was used in the paper as an unsupervised domain adaptation method. However, it can be used easily as a semi-supervised or supervised method. For example, once GFK is learnt on unlabeled data, one can learn a classifier by revealing the labels of the target data. The benefit is a better metric is used to measure distances.",1,5094
"The paper proposes a new pre-training scheme for neural networks which are to be fine-tuned later with back propagation.  This pre-training scheme is done in two steps  1 - sampling the parameters of the first layer using Importance sampling or an accept-reject MCMC method (both methods  are apparently confused by the authors) in a data dependent way.  2 - train the parameters of the output layer using linear regression.  The experiments compare the test RMSE/error-rate obtained using traditional back propagation and that obtained using the proposed method, on three datasets: a 1D function, a Boolean function and the Mnist dataset.  The proposed pre-training scheme is new but the scientific quality of the paper is questionable. First the proposed method is given a misleading name since it proposes to do the initialization in a data dependent way (with a linear regression step). This may be understood as a 'pre-training scheme', not as an 'initialization'. Second, the paper is very misleading in its report of previous work, for instance stating that (Efficient Backprop, Le Cun 1998) proposes to initialize neural networks by sampling from a uniform distribution [-1/sqrt(fan-in);1/sqrt(fan-in)] when it suggests in fact to sample from a normal distribution of mean zero and standard deviation sigma=1/sqrt(fan-in).  Additionally, the experiments are again very misleading. First, the main claim of the paper is that using the proposed pre-training scheme, BP will converge faster. However, the time to convergence is reported in terms of the number of BP iterations and does not take the pre-training time into account. This is especially worrisome since the pre-training scheme relies on MCMC sampling which is usually very computationally expensive compared to back propagation. Finally, the results reported on the Mnist dataset are inconsistent with previous work when then give a test error rate for back-propagation and 300 hidden units around 90% when it should be around 1.6% (cf. Mnist dataset website).   pros: cons:  - Misleading summary of previous work.  - Misleading reference to an initialization strategy which is in fact a data dependent pre-training step.  - Experiments do not report the pre-training time and are therefore strongly biased in favor of the proposed method.  - Results on Mnist are inconsistent with previous work.This paper presents a new method for initializing the parameters of a feedforward neural network with a single hidden layer. The idea is to sample the parameters from a data-dependent distribution computed as an approximation of a kernel transformation of the target distribution.   * The parameter initialization problem is important and the main idea of the paper is interesting.   Now, computing the transformation of the target distribution is the same as solving an equation for the parameters of the network, analytically, assuming an unlimited number of hidden units. As this is a difficult problem, the method relies on an approximation of the parameter density and sampling therefrom N times when the actual network is assumed to have N hidden units.   * It would be really helpful to have a notion of how expensive it is compute the approximation of the parameter density and to sample from it. Judging from the formulas this does not seem cheap.   * The paper studies networks with sigmoid pairs. What can the authors say about sigmoid units?   In Figure 1 left, the figure does not show that the support is non-convex, as claimed in the caption.   The axes labels in Figure 1 are too small.This paper introduces a new method for initializing the weights of a neural network. The technique is based on integral transforms. The function to learn f is represented as an infinite combination of basis functions weighted by some distribution. Conversely, this distribution can be obtained by projecting the function f onto another (related) set of basis functions evaluated at every point x in the input space.  The powerful analytic framework yields a probability distribution from which initial parameters of the neural network can be sampled. This is done using an acceptance-rejection sampling method. In order to overcome the computational inefficiency of the basic procedure, the authors propose a coordinate transform method that reduces the rejection rate. It would be useful to have more information on the order of magnitude by which the method is slower/faster compared to training a classically initialized neural network, and how does the method scale with the number of data points and the dimensions of the input space.  The experimental section consists of three experiments measuring the convergence of learning for various datasets (two low-dimensional toy examples and MNIST). On the low-dimensional toy examples, the proposed initialization is shown to be superior to uniform. However, these two datasets are to a certain extent already well modeled by local methods, for which good initialization heuristics are readily available (e.g. RBF networks + k-means). I am concerned about the validity of the MNIST experiment where a baseline error of >0.8 (80%?) is obtained with 1000 samples while other papers typically report 10% error for similar amount of data.",0,5095
"The paper presents a method for object recognition, localization and detection that uses a single convolutional neural network to locate and classify objects in images.  The basic architecture is similar to Krizhevsky’s ImageNet 2012 system, but with modifications to apply it efficiently in a “sliding window” fashion and to produce accurate bounding boxes by training a regressor to predict the precise position of the bounding box relative to the sliding window detector.  Numerous tweaks are documented for making this system work well including:  multi-scale evaluation over widely-spaced scales, custom shifting/pooling at the top layers to help compensate for spatial subsampling of the network, per-class regressors to localize objects relative to the input window, and a simple merging procedure to combine the regressed boxes into final detections.  This method is shown to achieve state-of-the-art results on ImageNet localization and detection tasks while also being relatively fast.  Overall, this paper presents a very thorough accounting of a fully functioning detection pipeline based on convnets that is the top performer on one of the toughest vision tasks around.  One of the challenges with reporting results like this is to make them reproducible, and I think this paper includes all of the details that a researcher would need to do so, which is really excellent.    There is currently a lot of work on detection architectures (e.g., from Erhan et al.) but this one is fairly complete and high-performing.  So, while there aren’t huge new ideas here, considering the depth of experiments and the cornucopia of tricks for maximizing performance the work looks very worthwhile.  Pros:  End-to-end training of the entire detection and localization pipeline.  The decomposition into 3 clean stepping stones (classifier, localizer, detector) is a nice strategy.  State-of-the-art detection performance on Image-Net.  Cons:  Somewhat “specialized” convnet architecture to deal with subsampling issues and multi-scale (e.g., it is mentioned that the detector of Fig 11 also uses multiple scales for context)  Other: The text is very detailed in order to make the system reproducible.  This is great, but perhaps some of the tables and minor notes [parameter settings, etc.] could be moved to an appendix to tighten up the text.The authors present a system that is capable of classification, localization and detection, to the advantage of all three. The starting point of Krizhevsky’s 2012 work is adapted to produce an architecture capable of simultaneously solving all tasks, with the feature extraction component being shared across tasks. They propose to scan the entire convolutional net across the entire image at several scales, reusing the relevant computations from overlapping regions already processed, making both classification and (per-class) bounding box predictions at each and a clever scheme for aggregating evidence for the bounding box across predictions, and lots of other tricks. They evaluate on ILSVRC 2012 and 2013 tasks with excellent results.  Novelty: low-medium Quality: high  Pros - This is an excellent piece of applied vision research. The results on ILSVRC speak for themselves, really. - It brings to light some potentially non-obvious (to a convolutional networks neophyte) advantages of convolutional nets for tasks like this, namely that dense application - Details are copiously documented: this is an excellent example of authors paying serious mind to the reproducibility of their work, a tendency that is sorely lacking in computer vision and machine learning in general. Please keep up the good work in future publications.  Cons - There isn’t a lot of methodological novelty, although there is some in the tricks employed to deal with subsampling, etc. (to my knowledge, these are novel). That said, it is a tour-de-force application paper, so I don’t see this as a serious drawback. - The only serious barrier to publication I see is clarity of exposition in certain parts. - Some discussion of not just which hyperparameters were chosen but how and why, including some rationale for departures from Krizhevsky’s architecture, would be nice (e.g. the learning rate schedule, your choice to drop contrast normalization, non-overlapping pooling regions, etc.). Providing the details as statement of fact is good, but insights into how you made some of these decisions would make for more compelling reading, especially to those familiar with the Krizhevsky work. - The organization of the paper could also use work: essential ideas should be distilled into an 8ish page manuscript and the details (which, as I said, are an extremely positive feature of this paper) relegated to an appendix.  Detailed comments: - If you have some rough idea of the relative importance of horizontal flipping and other tricks described in 3.3, it would be useful to know. I don’t expect an exhaustive ablative analysis, but even an informal statement as to which elements seem to be the most critical would be interesting. - The exposition on “dense application” and why this is a computational savings is less clear than it could be (section 3.5). Basically what you are trying to get across, I think, is that applying a convolutional net to every PxP window of an MxN image, where P << M and P << N, can be performed efficiently by convolving each layer of filters and doing the pooling and so on with the entire image at once, reusing computation for overlapping window regions, and thus it is much more efficient than if you had some arbitrary black box that you had to apply at every window location and reuse no computation whatsoever. However the text was very unclear on this point, and a reader with less background may not understand what you mean (which would be a shame, as this is a very important point, practically speaking). I’m sure I’ve heard this idea spoken of before -- is this the first time it’s appeared in print? If not, I’d make sure to include a citation.This paper demonstrates a convolutional architecture for simultaneously detecting and localizing ImageNet objects. It is the winner of localization task of ILSVRCS2013 competition and that by itself makes it interesting.  They implement a combined architecture where first 5 layers share features compute features that are used for both classification and localization tasks.  A lot of detail goes into constructing an architecture in a way as to make network windows aligned with the objeect. They use 6 scales increased in steps between 1.08 and 1.27 (why this arbitrary choice of steps?) along with 1 pixel offsets and a skip feature connections at the top to prevent the stride from being too large.  This is a solid paper which summarizes significant body of work in neural network design, which makes its relevance to ICLR high.  Some suggestions: -- I wish the authors gave more detail on computational efficiency/accuracy compromise since that needs to be considered when running in an industrial setting.  For instance, coarse vs fine stride seems to provide 1% absolute improvement, while requiring 9x more computation at the lowest level. How much does that affect total computation? This could be done by adding an extra column 'FLOPS' to Table 5.",1,5096
"Very interesting paper on different kinds of feature representations for agglomeration of 3D neuron fragments. The paper demonstrates how the performance using a set of hand designed features can be further improved with representations derived using both end-to-end and unsupervised training techniques on this task. Several interesting machine learning techniques for feature discovery and training, adopted in other fields have been successfully deployed for this task.  - Well written paper with solid results evaluating different novel features and their combinations, for agglomeration of 3D neuron fragments.  - Several powerful techniques - end-to-end learning/unsupervised training/dynamic pooling, have been successfully used for this task.  The paper is quite domain specific although novel in its domain. It does not seem to prescribe any new take-home techniques for other tasks/domains.  The features and machine learning techniques introduced in the paper produce remarkably high performance results. It would be useful to have more insights on the nature of the task and how challenging it is in general.  The proposed 'boundary map features' are capable of achieving 90% AUC for the task (Table 1) - Why is that? - Is this something specific to the task? It would be useful  to understand how other features (object/image based features) perform by themselves (Table 1) - the results presented for these features are after combination only. Do they perform only in combination or do they also have a base performance of 90% by themselves?  The paper shows very clear benefits of using features from unsupervised learning techniques. What kind of attributes are these features learning? Are they similar to the boundary map features and hence the high performance as well? More insights would be useful.This paper compares the use of hand-designed and learned features for the analysis of 3d objects. The authors use an extensive set of 25 hand-designed features to obtain 92.33% accuracy for the task of agglomerating neuron fragments. They then explored fully supervised end to end learning of features from raw inputs, but only obtain 85.54% accuracy. However, because the data is small compared to its dimensionality, unsupervised learning provides some improvement. Finally, a dynamic pooling method allows them to match the hand-designed features score. Data augmentation however brings fully supervised and unsupervised approaches on par.  Novelty and quality: I am not very familiar with the literature in 3d analysis but the introduction suggests that feature learning (fully supervised and unsupervised) has not been used before or is not common. If so, the methods themselves are not novel but their application to this particular field is. It would be good if the authors could state clearly if this has ever been tried before or not. The quality of the work is good and thorough.  Pros: - directly comparing hand-designed and learned features is good. - learned features are shown to be slightly superior in accuracy.This works presents experiments on different features for the task of classifying agglomeration of 3d neuron fragments. Results of experiments using a wide range hand-crafted features and their combinations, as well as learned features are presented. The results shows that learned features can obtain performance similar to hand-crafted features, and that their combination can yield even better performance.  Novelty: I am not very familiar with the field of 3d imaging, so it is difficult for me to asses the novelty of this work. But, I trust the authors when they claim that showing that learn features can perform as well as hand-crafted feature for analysis or classification of 3d shapes is a new result. Other contributions include evaluating performance of a large set of hand-crafted features, proposing an end-to-end approach to derive features, and augmenting the 3d images dataset with transformations.  Quality: I found that the paper is well written and well structured. The experimental method is sound and the conclusions are in line with the results.  I recommend to accept this paper.  Pros:  - Experiments are well constructed.  - Results are well presented are are relevant to the field of representation learning.  Cons:  - The figures are not grayscale friendly, which made it harder to understand some important points. Once I saw the color figures, it made things much clearer.  Small nit: In section 3.2, the authors use the term 'significantly' when there are no uncertainties on the performance measure. Perhaps using another term would be more appropriate.",0,5097
"Summary This paper explores the behavior of a neural network when the cost function changes from an initial task to a second subsequent task. The authors construct pairs of tasks from standard benchmarks in textual sentiment classification and MNIST digit classification. For constructed pairs of tasks the authors analyze performance of networks trained with four different standard hidden unit activation functions, and dropout regularization. The neural networks used are standard architectures, and there is not anything new from a modeling perspective. The authors conclude that dropout regularization is beneficial on the datasets they evaluate, but there is not a consistent best choice of activation function.  Detailed Review The problem of catastrophic forgetting seems irrelevant to most of the current research in deep learning, and seems to be a somewhat constructed thought experiment. The paper does not mention a practical task that requires training the same neural network on two problems in sequence. Indeed, I can’t think of such a task, and the authors should significantly rewrite the introduction to make clear why this problem is important.   The ability of a network to “remember” a previous task is purely an artifact of poor optimization. If the neural network cost function could be better optimized, neural networks, like SVMs, would completely forget any previous task. This is actually a good thing! Neural networks are function approximators and any “remembering” of a previous task exhibits poor performance on fitting the function of interest. This observation again points to doubts as to why catastrophic forgetting is a problem worth studying.   In terms of experiments, the authors do a relatively good job of exploring possible network architecture choices. My concern with the experimental setup is the contrived nature of the dataset pairs. The pairs of tasks used don’t seem representative of realistic task pairs, but again I can’t think of a task pairs which actually require sequential training. The paper seems incomplete without comparing sequential task training to jointly training on the two tasks. I suspect joint training should perform at least as well.  When discussion dropout, the authors should be careful to not describe it as an alternative to SGD. Dropout is a regularization technique and can be written into the cost function of network training. SGD is an optimization technique used to optimize that cost function, so for example SGD can be replaced with conjugate gradient to train a model with dropout regularization.   In dropout training some papers have found significant performance impact from the setting of dropout probability. The authors argue that setting the dropout probability to 0.5 is usually best, but it would be nice to have an experimental validation of this claim.  Review Summary - Forgetting seems like a contrived problem not relevant to current deep learning research - Evaluation is on toy datasets which do not require sequential task training and could easily be trained jointly - No comparison is given to jointly training  + Reasonable evaluation of various hidden unit activation function choicesThe paper discusses the problem of catastrophic forgetting, in the context of deep neural networks. While this is the purported goal of it, an equally interesting outcome of this analysis is the in-depth comparison between a variety of common activation functions used with or without dropout. This analysis reveals that training with dropout is always beneficial in terms of adapting to the new task, but that the various activation functions studied can perform quite differently depending on the task in question. The authors’ analysis does suggests the maxout activation function is consistently a good choice.  I am mystified about the comment on dropout having a “known” range of values that works well (0.5 or 0.2, depending on whether it’s a hidden or visible unit). This may very well be true in the context of obtaining a good performance on a given set of tasks that the authors consider. But since dropout can be seen as having a regularization effect: this effect can manifest itself by less (or more) catastrophic forgetting. I encourage the authors to come up with a single number than can represent well the entirety of the curves shown in Figs. 1,3,5 and then plot out the influence of dropout vs. catastrophic forgetting, one curve per activation function (and per dataset, of course). One good single number may be the area under the curve, perhaps discounting for the left-most and right-most parts? Such an analysis might elucidate the aspects related to the increased number of parameters that dropout seems to favor in the “similar” tasks scenario.  Other than that, it would be nice if the authors proposed hypotheses for why maxout seems to perform better -- it is not obvious from their analysis that this should be the case.  A general comment: it seems rather odd that catastrophic forgetting seems to be somehow tied to the choice of an activation function in this paper -- this (plus dropout) cannot possibly be the only aspect that influences the magnitude of the catastrophic forgetting problem. Arguably, things like the optimization and, generally speaking, the regularization strategies employed, are equally as important. This paper touches very little upon those issues, glossing over anything other than SGD + dropout.  Small comment: there’s a dangling “McClelland, James L” reference.The paper 'An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks' investigates the robustness of 'modern' feed-forward networks trained on dropout and a variety of activation functions to task-switching during training. Biological neural networks have the property that after being trained for a task A, and then trained for a different task B, they can be re-trained for A more quickly than networks that have never mastered A before.  This paper investigates the capacity of various sorts of feed-forward neural networks to exhibit this property.                                                                       Novelty:                                                                                                                               The authors state that the issue was 'well-studied in the past' but provide no references or summary of that past work. I am not familiar with that literature.  Quality:                                                                                                                              The paper was a joy to read. As a reviewer, I want to thank the authors for taking the time to write so clearly.                      The large parenthetical remark before section 4.1 should probably be the caption on the page with Figures 1 and 3.   Pro:  The issue of how deep networks might transfer patterns learned from one data set to another is important and topical.  The experiments are presented clearly and reveal an interesting new property of dropout training.   Con:  The suggestion that the first layer of a neural network is not rewired in response to a remapping of MNIST pixels, but rather upper layers handle the new task is counter-intuitive and intriguing. The paper would be stronger if the authors tested this hypothesis.  Comments:   Experiments intro: a surprising omission was that prior to dropout L2 and L1 weight regularization   Figures 1 and 3: A better caption would help these intriguing figures to stand on their own. Is a lower-envelope on a scatter plot of randomly drawn models being drawn here? Is there a modelling reason that motivates connecting the dots, or is it simply a visual aide? Did you consider leaving all of the scatter-plots on the axes (though it might be too busy)?                                   I think the experimental protocol used here is not quite the same as the one used to label catastrophic forgetting in neural networks: there, task A is trained to mastery, then B, then A again. The question is how fast someone can go from A to B and back, without regard for how good they are at A or B in absolute terms. Here, an early stopping criterion was used during B-training so that the question of how fast A degrades may be confounded with the networks' raw ability to do tasks A and B.",0,5098
"In this work the author investigates how effective the vector representation of words is for the task of logical inference. A set of seven entailment relations from MaCartney are used, and a data set of 12,000 logical statements (of pairs of sentences) are generated from these relations and from 41 predicate tokens.  The task is multiclass classification, where given two sentences, the system must output the correct relation between them. A simple recursive tensor network is used. The study is limited to considering quantifiers like 'some' and 'all', which have clear monotonicity properties.  Results show that the model can learn but that generalization is limited.  Unfortunately because the training process converges to an inferior model, results are given after very early stopping, in which subsequent iterations can give widely different results.  This is an exciting direction for research and it's great to see it being tackled. Unfortunately, however, the paper is unclear in crucial places, and the training methodology is questionable.  I would encourage the author to clarify the paper (especially for the likely non-linguist audience) and strengthen the training algorithm (in order to demonstrate usefully reproducible results).  Even if the results remain negative, this would then still be of significant value to the community.  Specific comments:  Section 2 ---------  Your example of 'some dogs bark' seems confused.  For both arguments of 'some' (not just the first), the inference works if the argument is replaced by something more general. You write that 'some' is downward monotonic in its second argument, but your examples show upward monotonicity in both. (Specifically, you write 'The quantifier 'some' is upward monotone in its first argument because it permits substitution of more general terms, and downward monotone in its second argument because it permits the substitution of more specific terms.' - but in the same paragraph you also write that 'some' is upward monotonic in both arguments.) Readers who are asked to expend mental energy on disentangling unnecessary confusions like this can quickly lose motivation.  Table 1: this table is central to your work, but it needs more explanation.  What is calligraphic D?  You seem to be using the 'hat' operator in two different senses (column 2 versus column 3). What does 'else' mean in column 3 - how exactly is independence defined?   The whole paper rests on MacCartney's framework, so I think it's necessary to explain more about this scheme here. In particular, I do not understand your 'no animals bark | some dogs bark' example (and I fear most others won't, too).  Typo: much hold --> must hold  Section 3 ---------  'Several pretraining regimes meant to initialize the word vectors to reflect the relations between them were tried, but none offered a measurable improvement to the learned model' - please say which were tried.  In particular, did you try fixed, off-the-shelf vectors, for example from Socher's work, or trained using a large unlabeled dataset using Mikolov's Word2Vec?  I counted 4624 parameters in the composition parameters, 13,005 for the comparison layer (with dimension 45), and 800 for the 50 (16 dimensional) word vectors, giving a total of 18,429 parameters.  Your training set size is quite a bit smaller than this, and regularization can only help so much.  I wonder if the limited results and difficulty of training (converging, but to poor solutions) just indicates the need for more training data. You can test this by generating a training curve - that is, plot performance on a validation set for various training set sizes (when trained to convergence, and using whatever regularization you settle on).  If the curve is still steep when using all the training data, then more data will help.  If it's flat, then the task may not be learnable with the model used.  Section 4 ---------  Your description of the 'basic monotonicity' datasets was unclear to me.  Does 1, 2 refer to Table 2?  If so it's not clear how 'In some of the datasets (as in 1), this alternation is in the first argument, in some the second argument (as in 2), and in some both.'  Section 5 ---------  It is not very surprising that the model can learn the all-split data, since the training data is so tightly constrained.  Set-out is also very close to the training data.  I found that exactly how the data splits were done, and what was tested on, was unclear.  typo: 'the it is'  'I choose one of three target datasets' - how did you choose the three?  (From the 200?)  'potentially other similar datasets...' is imprecise.  How did you choose?  'The model did not converge well for any of these experiments: convergence can take hundreds or thousands of passes through the data, and the performance of the model at convergence on test data was generally worse than its performance during the first hundred or so iterations. To sidestep this problem somewhat, I report results here for the models learned after 64 passes through the data.'  I'm afraid that this greatly reduces the value of these results (they are close to being irreproducible).  The training algorithm should at least converge, or be more reproducible than this.  (If the test error is still fluctuating wildly on the stopping iteration, other small changes, e.g. in the data, may give completely different results).  Section 6 ---------  'Pessimistically... Optimistically... ' this is speculation (neither is supported by the experiments) and so I don't think it adds much value.The paper tries to determine whether representations constructed with recursive embeddings can be used to support simple reasoning operations.  The essential idea is to train an additional comparison layer that takes the representations of two sentences and produces an output that describes the relation between the two sentences (entailment, equivalence, etc.)  This approach is in fact closely related to the 'restricted entailment operator' suggested near the end of  Bottou's white paper http://arxiv.org/pdf/1312.6192v3.pdf.  Experiments are carried out using a vastly simplified language and Socher's supervised training technique. According to the author, the results are a mixed bag. On the one hand, the system can learn to reason on sentences whose structure matches that of the training sentences.  On the other hand, performance quickly degrades when using sentences whose structure did not appear in the training set.  My reading of these results is much more pessimistic. I find completely unsurprising that the system can learn to 'reason' on sentences with known structure. On the other hand, the inability of the system to reason on sentences with new structure indicates that the recursive embedding network did not perform what was expected.  The key of the recursive structure is to share weights across all applications of the grouping layer. This weight sharing was obviously insufficient to induce a bias that helps the system generalize to other structures. Whether this is a simple optimization issue or a more fundamental problem remains to be determined.  My understanding is that the author always trains the system using the correct parsing structure in a manner similar to Socher's initial work (please confirm).  It would be very interesting to investigate whether one obtains substantially different results if one trains the system using incorrect parsing structures (either a random structures or a left-to-right structure). Worse results would indicate that the structure of the recursive embeddings matters.  Similar results would confirm the results reported in http://arxiv.org/abs/1301.2811 and strongly suggest that recursive embeddings do not live up to expectations. This would of course be a negative results, but negative results are sometimes more informative than mixed bags (and in my opinion very worth publishing.)This paper investigates the use of a recurrent model for logical reasoning in short sentences. An important part of the paper is dedicated to the description on the task and the way the author simplifies the task of MacCartney to keep only entailment relations that are non ambiguous. For the model, a simple recurrent tensor (from Socher's work) network is used.  While the more general task defined by MacCartney is well described, the reduced task addressed in this paper is more unclear. The motivation stands: this is a great idea to reduce the task to non ambiguous cases, for which we could better interpret the experimental results. However, at the end, it is difficult to draw relevant conclusion from the experiments, and a lot of technical details are missing to yield the results reproducible. Maybe the author tried to lessen the negative aspects of the results, but it would be really more interesting to clearly describe negative results. My opinion is that this paper is not well suited for the conference track, and maybe it should be submitted to the workshop track.",0,5099
"The paper looks into representing words as (approximate) posterior distributions of states in a Factorial HMM estimated on a unlabeled dataset of a moderate size. Unlike standard word representations (e.g., obtained with the neural probabilistic model of Bengio et al. (2003)), these representations encode not only properties of a word but also properties of the word context (in theory unbounded) . For example, this may result in a form of word sense disambiguation.  Of course, this come at the cost of performing inference at test time.  The authors evaluate their approach on PoS tagging and chunking tasks.   I find the paper quite interesting, as using some form of disambiguation does seem like a good idea to me.   Comments:  1) The authors use a variational approximation which can be regarded as a form of the structured variational method for Factorial HMMs introduced in Ghahramani & Jordan (MLJ 1997) (not cited).  It would be good to explain which parts of the inference algorithm (e.g., relaxation for softmax) are novel, and which are borrowed from previous work. 2)  I do not quite understand how exactly the variational parameters phi are computed -- something like Newton-Raphson? (In the paper: 'these expectations are used to find the new set of variational parameters', par 1, page 6) 3) I wonder if Brown clusters are used as atomic variables or the features are paths in the binary tree (as in Koo et al. (ACL 2008)). Using paths may help substantially.  4) Actually, factorial HMMs induce other types of word representations as well: parameters associated with emission distributions can perhaps be regarded as such representations. I am wondering if the authors considered using them as an additional baseline. (Of course, this representation would not be affected by the context.) 5) It would be nice to see methods using word embeddings produced by more conventional methods as additional baselines. This would make the paper more convincing. 6) Stochastic neural network models (namely sigmoid belief networks) for syntactic parsing of Titov & Henderson (ACL 2007) and Henderson & Titov (JMLR 2010) may also be relevant. They learn representations of parsing states (and some of their states corresponds to word emissions). The posterior distribution of the state variables are affected by global context. They also use variational approximation methods somewhat similar to the ones considered in this submission.The author use a Factorial Hidden Markov Model (FHMM) on two Natural Language Processing tasks, namely POS tagging and Chunking.  Such a model associates a factorial state (a tuple of states) with each word position in the corpus. This can be interpreted as a left-context dependent word representation. In order to evaluate the quality of this representation, the authors train the representation on WSJ data from the Penn repository and test on biomedical text. This domain adaptation performance is compared with that of a variety of systems (no learned features, HMM features, Brown clusters).   Although the approach makes sense, the empirical evaluation leaves questions open.  For instance, before seeing domain adaptation performance results, I would have liked to know whether the FHMM representation leads to competitive performance when testing on similar data. Also the domain adaptation performance is not compared with the performance afforded by some of the readily available word embeddings (other than Brown clusters.)   In conclusion, I do not really know what to think of the performance of FHMMs on such tasks.The authors propose using a factorial hidden Markov model (FHMM), which is an HMM with multiple latent Markov chains cooperating to produce observations, to induce context-dependent word representations from text. They derive a variational EM algorithm for training these models efficiently and use a structured variational posterior consisting of independent Markov chains to capture the sequential structure of the exact posterior. Given a sequence of words, the authors obtain their representations by computing the variational posterior for the sequence and representing each word with the resulting distribution over the states of the latent variables associated with the timestep that generated the word.  Although HMMs have been used to induce word representations before, using factorial HMMs for this task is novel. The variational algorithm for training FHMMs on multinomial observations, such as words, derived in the paper appears to be new as well.  The idea of using FHMMs to obtain context-dependent word representations is a sensible one. Unfortunately, the paper is not particularly well written and the experimental evaluation is not sufficiently convincing. The variational EM algorithm derived by the authors is a relatively minor modification of the algorithm of Ghahramani and Jordan (1997), something the paper does not state clearly enough. The form of the variational posterior comes from the same paper. The bound in Eq. 15 has already been used in a very similar setting by Blei and Lafferty (2007). The presentation of the derivation is unclear and contains a number of small mistakes. For example: -The terms corresponding to S_1 are missing from P({S_t}|{Y_t}) and Q({S_t}|{Y_t}). -In equations (3), (12), and (13), the denominator contains S^m_{t-1,j} instead of S^m_{t,k'}. -It would be cleaner to parameterize the multinomial distributions directly in terms probabilities (as was done by Ghahramani and Jordan) instead of log-unnormalized-probabilities. -The parameterization of the variational posterior in equation 13 is odd. Why are the transition terms explicitly normalized but the observation terms are not?  The experimental section does have some interesting results but suffers from a non-standard experimental protocol and absence of some obvious baselines. The representations produced by FHMM are interesting because they are distributed and context-dependent. Since distributed but not context-dependent representations are not included in the evaluation, it is not possible to disentangle the effects of those two factors on the performance of the FHMM representations. Representations learned by neural language models as well as the 'non-temporal FHMM' (effectively a distributed mixture model) would be the natural baselines for this. Until this issue is addressed, it will remain unclear how much context dependence contributes and whether using FHMMs instead of simpler models is worth the effort. Finally, it is unfortunate that the authors chose not to use exactly the same training data as in [25] and [6], making it impossible to compare their results to those in the literature.",0,5100
"The paper presents a technique for accelerating the processing of CNNs by performing training and inference in the frequency (Fourier) domain. The work argues that at a certain scale, the overhead of applying FFT  and inverse-FFT is marginal relative to the overall speed gain.    As noted by the previous reviewers, the speedup is presented simply as that obtained for three functions that lie at the heart of each convolutional layer. It would be valuable if the speedup could also be presented in the context of a comparison to the overall training time of a CNN on a standard dataset.  Also noted is the lack of reference to the fact that Convolution Theorem refers to circular convolution and not linear (i.e. non-circular) convolution. It is assumed inconsequential since CNNs use neither circular convolution (weight filters do not wrap around images) nor linear convolution (weight filters are always fully contained within the image and do not 'hang off' the edges). Thus, the resulting differences between circular and linear convolution would not impact the feature map y_f. This seems to be hinted at by the n' term in section 2.2, but is not obvious.  The future work seems logical and would be interesting to pursue. One other direction to consider is approximations to the FFT (which there are many) that could retain most of the information needed in context of CNNs at a fraction of the computational cost.  Minor editorial issue: in figure 3 the axes are noted in the title of the figure rather than as labels for the x and y axis.The paper describes the use of FFTs to speed-up the computation during training for convolutional neural networks working on images. Essentially this is presented as a pure speed-up technique and doesn't change the learning algorithm, or (in an interesting way) the representation.   The idea of applying FFTs to speed up image processing systems, particularly 'sliding windows' systems, is far from new and there is a large literature on this.In particular combining FFTs with Neural networks is not new, e.g. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.1967 Some of this prior literature should be cited.   I am not aware of any work that applys the back-propagation in the Fourier domain too.   The resulting speed-ups are significant for the scenario the authors are considering, and it is useful to know that the practical implementation delivers these gains. As they conclude, these results may change the way such problems are formulated by removing the pressure to use small kernels.  Expand the caption for Figure 2. Total number of operations for what?   Figure 3 needs units for the y axis (text says seconds?), and for the x axes - ie areal or linear pixels? Also for each of the 3 sets of graphs, there needs to be an indication of what are the values of the parameters which are  held constant.  Please say in the text that all 3 systems (Torch, Authors' and Krizhevsky) were running on the same (which?) GPU.   Citation for Cooley-Tukey FFT?'Fast Training of Convolutional Networks through FFTs' compares Fourier-domain vs. spatial-domain convolutions in terms of speed in convnet applications.   The question of the relative speed of Fourier vs. Spatial convolutions is common among engineers and researchers, and to my knowledge no one has attempted to characterize (at least in convnet-specific terms) the settings when each approach is preferred.  Spatial domain convolutions have been the standard in multiple implementations over 30 years of research by scores of researchers.  This paper claims, surprisingly, that FFTs are nearly always better in modern convnets.  At the same time, the authors of the paper introduce a strategy for FFT parallelization on GPUs that is somewhat particular to the sorts of bulk FFTs that arise in convnet training, and the conclusions are based on that implementation running on GPU hardware.     CONTRIBUTIONS   1. Empirical comparison of spatial and Fourier convolutions for convnets   2. A fast Cooley-Tukey FFT implementation for GPU that's well-suited to convnet application.  The figures and formatting are not very polished.              PRO               1. The paper aims at an important issue for convnet researchers           2. The claim that FFT-based convolutions are better will be broadly interesting                                                 CON               1. The paper does not explain when spatial-domain calculations would be faster                                                2. The paper does not discuss how the trade-offs would be different on single-core or multi-core CPUs, or on different GPUs.   3. Details of the Cooley-Tukey implementation are not given   4. No mention is made of downloadable source code, this work might be hard to reproduce   COMMENTS   - What about non-square images?   - Why use big-O notation in 2.2 when the approximate number of FLOP/s is easy to compute? Asymptotic performance isn't really the issue at hand, the relevant values of n and k are not very large. Consider falling back on big-O notation only after making it clear that the main analysis will be done on more precise runtime expressions.   - The phrase 'Our Approach' is surprising on page 3, because it does not seem like you are inventing a new Fourier-domain approach to convolution.  Isn't the spatial domain faster sometimes, Fourier faster sometimes, and you're writing a paper about how to know which is which?   - The last paragraph of section 3 is confusing: which of your experiments use and do not use the memory-intensive FFT-reuse trick?  The following sentence in particular makes the reader feel he is being duped 'All of the analysis in the previous section assumes we are using this memory-efficient approach [which you now tell is is infeasible in important practical applications]; if memory is not a constraint, our algorithm becomes faster.'  Faster than what? Faster than the thing that doesn't require prohibitive amounts of memory?   - Page 4: when you speak of 'another means to save memory' what was the first way? (Was the first way to recompute things on demand?)   - Page 5: Figure 3: This figure is hard to understand. The axes should be labeled on the axes, and the title should contain the contents of the current caption (not the names of the axes), and the caption should help the reader to understand the significance of what is being shown.   - Why is the Torch7 implementation called Torch7(custom), and not just Torch7?   - The memory access patterns entailed by an algorithm is at least as important for GPU performance as the number of FLOP/s. How does the Cooley-Tukey FFT algorithm work, and how did you parallelize it? These implementation details are really important for anyone trying to reproduce your  experiments.   - What memory layout do you recommend for feature maps and filters? This ties in with a request for more detail on the algorithm you used.",1,5101
"This submission describes an approach for digit and sequence recognition that results in improved performance on the StreetView house number dataset using a simple structured output to recognize the entire sequence as an ordered set of detections and a very deep convolutional network (11 layers).  The approach is end-to-end, without requiring multiple networks or a composite of techniques.   The approach has very high accuracy and is attractive in its simplicity, although the authors make clear that it could not be extended to, for instance, general text recognition in images.  The relevance for ICLR is high, although the paper has some significant omissions that keep it from being a very strong candidate for acceptance.  First, the method is not clear, in particular the interface between the varia softmax digit classifiers and the output of the convnet. It is unclear whether there is any representation of locality in this interface beyond the assignment of different classifiers for each position.  In the introduction, it is stated that the subtasks of localization, segmentation, and recognition are solved in an integrated way, but section 3 introduces the dataset, which has localized inputs- numbers which fill at least ⅓ of the image. This needs to be clarified.  Second, to further the contribution of the paper, additional experiments or analysis could have been performed to understand the features, the architecture, the loss function, or other aspects of the approach. With these missing, the submission is somewhat thin and the contribution lessened.  Smaller issues: the variables used in the plate model in Fig. 1 need to be defined in the caption and supporting text as well as later when the method is explained, and DistBelief should not be used in the abstract and intro without citation or footnote.The paper presents an application of deep neural networks to the problem of reading multi-digit housenumbers from StreetView images.  The basic architecture is essentially standard (maxout units, ReLu units, convolution, and several dense layers), but is unusually deep (11 layers).  The output of the detector is a softmax predictor for the length of a sequence, as well as softmax predictors for each digit in the sequence.  This simple output encoding is sufficient to achieve a high recall at very high precision that is competitive with human labelers.  The authors conclude that this particular OCR application may regarded as solved at this stage.  There is relatively little new in terms of algorithms here, but the results are excellent.    The paper is clearly written, though the prose could be tightened up a bit.  If room can be made, I think a deeper analysis of the method’s success would be useful.  For example, is it possible that the “deeper” networks are fitting the training set better as a result of more model parameters?  Or is the depth truly the deciding factor?  Most surprising to me is the fact that there is no explicit need to model the label structure beyond the obvious:  a detector for sequence length and softmax outputs for digit classes, with a small tweak to choose the most likely length of the sequence during test time.  This follows along with recent work on detection systems that suggests sophisticated regressors are able to do something similar for object classes, so I think this otherwise simple component is a useful datapoint for that conversation.  Pros:  Simple off-the-shelf application with excellent performance;  perhaps high enough to count this task as “solved”.  A useful reference point for work on predicting structured outputs like character sequences.  Cons:  Essentially boiler-plate neural network.  A bit more detailed analysis of the result would be useful.The authors propose an integrated approach to sequence recognition in the case of limited number of characters (house numbers with 5 characters at most). Avoiding separating localization, segmentation and recognition is novel in this context. This is the right approach and results are good but the model is not well explained (see below).  Pros: - integrated sequence recognition rather than traditional localization/segmentation/recognition step. - new record on single digit - accuracy high enough for real world deployment (although the ability to pay human operators for remaining errors in the 98% regime means that real world deployment is possible even at very low accuracy, depending how much money the company is willing to spend).  Cons: - 'and has special code for handling much of the mechanics such as proposing candidate object regions' and 'we take a similar approach, but with less post-processing': this is misleading and debatable that there is less post-processing because the authors methods relies on a pre-detection step that gives relatively tight bounding boxes around the number, as indicated here: 'Beyond cropping the image close to the street number'. One could argue that there is as much post-processing in the other cited work once the detection is performed. - what happens at the top of the network is not clear at all to me. Are they using an HMM or not? Does the 64x64 input image yield a grid of probabilities? then what is the size of that grid? Or is the network directly predicting N outputs, and based on the value of L, uses only the first L values out of N? - 'a softmax classifier that is attached to intermediate features': digit softmax are located on the intermediate hidden layers? which ones? - 'On this task, due to the larger amount of training data, we did not need to train with dropout': it would have been nice to see the numbers with and without dropout instead of just relying on this claim.",1,5102
"The paper describes a method to augment pre-trained DBMs with phase variables and shows some demonstrations of binding, segmentation, and partitioning (based on latent variables).  Pros: The paper is very well written and introduces a number of concepts clearly. Phase is a curious neurophysiological phenomena and is deserving of modeling that addresses the representational consequences/implications. I could see how this ad-hoc approach could be used to understand DNNs (like extended Zeiler and Fergus' visualization work). The paper may educate the ICLR community on the binding problem and proposals from the neuroscience community that argue for phase as a solution to this problem.  Cons: A major issue with the described work is its similarity to the work for Rao and colleagues.  The authors provide some comments about how their work is distinguished.  However these are merely rhetorical (your approach is general and theirs is not) or actually contributions that are not made by this paper but by previous work (DBMs can be trained to learn representations of multiple-simultaneously presented object/patterns).  The two major limitations of the paper in its current form: 1. the introduction of phase is done in an ad-hoc way, without real justification from probabilistic goals.  The authors appear surprised that their hack worked at all.  It seems the more rigorous approach would be to either introduce phase as a proper latent variable and train the network to optimize the distribution to match the data distribution (the usual approach to modeling), or to explain more rigorously why this ad hoc extension does not interfere with the network (however it is not even clear from the experiments that the ad-hoc model preserves the properties of the original network).  A more rigorous mathematical approach might reveal that the basins of attraction are preserved with the introduction of phase, or that the phase variables are independent of the amplitude variables (I believe they are not). 2. The results of the experiments are mostly just pictures and lack quantitative assessment or any controls.  The resulting work provides a demonstration of the phase idea for binding/grouping/segmentation, which are not new ideas (although they are probably new ideas to the ICLR community).  Furthermore, the results in the appendix appear to indicate that the approach is not working very well in general, and the best results are the ones shown in the main text.  The procedures avoid some obvious issues: How is the phase distribution segmented?  Phase is a continuous variable, and the segmentation/partitioning seems to be done by hand for the examples.  This needs to be addressed. It is not clear that the segmentation is working for the bars experiment because multiple bars are colored by the same phase.  What is the goal here?  that each bar has a different, unique phase value?  Does the underlying phase distribution effectively partition the bars?  Also, what about the overlaps of the bars?  these areas seem to be mis- or ambiguously labeled.  is this a bug or a feature?  Overall, I think this is an interesting direction of research and the exposition is top notch, however the underlying work falls short of some obvious extensions and methodological rigor.  I think this would make for a nice workshop paper, so that it could receive some feedback from the community and educate the community of the phase binding idea, but it lacks some ingredients for a conference paper.  Some other relevant references you might want to include: S. Jankowski, et al. 1996. Complex-valued multistate neural associative memory. T. Nitta. 2009. Complex-Valued Neural Networks: Utilizing High-Dimensional Parameters. C. Cadieu & K. Koepsell 2010. Modeling Image Structure with Factorized Phase-Coupled Boltzmann Machines.SUMMARY                                                                                           The paper 'Neural Synchrony in Complex-Valued Deep Networks' tackles the important question of how it is that neural representations encode information about multiple objects independently and simultaneously. The idea that the relative phase of periodic neural responses has been in circulation for some time (and this paper provides a good overview of relevant literature), but to date the principle has not gained traction in the pattern recognition side of neural modeling. This paper aims to change that by showing that a complex-valued Deep Boltzmann Machine can naturally segment images (in some simple synthetic cases) according to the various visible objects, through the phase of latent responses.    The basis for the technical contribution of this paper is a novel response function.  The [complex-valued] response function described by equations 1 and 2 is a function of a complex-valued weight vector applied to a complex-valued feature vector. Output z_j = r_j e^{i 	heta_j} of each model neuron is determined by arg(z_j) = arg(w . x)        f( alpha  |sum_j w_j . x_j | + eta sum_j (w_j  |x_j|) )   Comment: The text leading up to equation 1 is confusing regarding z. Is it an output or an input?  It doesn't seem like we're dealing with a dynamical system, and the input was called x in the paragraph above.    Comment: the use of |x| in equation 2 is confusing because presumably |w . x| is a vector norm whereas in w . |x| it denotes elementwise magnitude of the complex elements of x. Right?   Comment: the authos mention that the two terms in f() can be weighted, but don't include those weights in Eq. 2, as I have done above (alpha, eta).    Why use this function? The authors make an intuitive argument in the text that these two terms capture salient aspects of a more detailed spiking network based on Hodgkin-Huxley neurons, and Figure 1 illutrates the effect quantitatively for a specific, simple 2-to-1 feedforward network of rhythmic neurons. The value of this transfer function as a surrogate for detailed compartmental models is interesting, but is not the focus of the remainder of the paper.      The paper's section 3 'Experiments: the case of binding by synchrony' was somewhat difficult for me to understand. A [conventional, real-valued] DBM was trained on small pictures with horizontal and vertical bars, and then 'converted' to a complex-valued network (and was the activation function changed to the one from Eq. 2? What does that mean in terms of inference in the DBM?) It was found that when clamping the visible-unit magnitudes to a particular picture, and 'sampling' (is this actually sampling from a probability distribution?) their phase and the hidden units' magnitude and phase, that there were groups of hidden units with phases that lined up with particular bars. This is good because it suggests a means of teasing apart the DBM's latent representation into groups that are 'working together' to represent something independently from other groups. (I wanted to see some sort of control trial, showing that a plain old real-valued DBM could not achieve the same thing, but I can't really think of the right thing to try.)   The demonstration in Figure 3 shows that already with this set of bars, that there is an issue of phase resolution: it appears that four different bars are all coded the same shade of green. Is this a problem? A readout mechanism might be confused and judge all these bars to be one object, even though bars occur independently in the training data. Figure 3 illustrates what happens after 100 iterations of sampling, what happens after more iterations? Do the co-incidentally green bars change color independently of one another?  Overall, this research is highly relevant to the aims of the ICLR conference. It is at an early stage of development, in that no learning algorithm has been adapted to work with these complex-valued neurons (although the authors might consider adapting the ssRBM), the images used in the experiments are simple and synthetic, and the authors themselves lament that 'conversion' of a DBM is unreliable. Still, the idea of phase-based coding has a lot of potential, and it is worth exploring.  This paper would be an important step in that process. I would strongly suggest that the authors upload their Pylearn2 code so that others can reproduce the effects presented in this paper, especially if training and conversion of DBMs is as unreliable as they suggest.     NOVELTY AND QUALITY   - the use of complex-valued phase to perform segmentation in a DBM is novel  - quality of presentation is very good     PRO & CON   pro: phase-based segmentation is an intriguing idea from theoretical neuroscience, it's great to see it put to the test in engineering terms   con: the stimuli are quite simple compared with other deep learning and vision applications  con: the model is at an early proof-of-concept stage  con: no natural learning algorithm yet for the modelI find this paper deeply fascinating.  It illustrates - I believe for the first time - the utility of binding by synchrony in a deep network architecture.  Although the general idea of binding by synchrony is an old one, it is mostly a vague idea that has never, to my mind, been put into a concrete computational framework.  Here, the authors propose that the phase of oscillation in neural ensembles acts as a kind of 'label' for objects being represented in a distributed fashion.  That is, no single unit represents an object, but when an object is presented to the network it activates features at each level which synchronize via two-way communication between levels.  The result is a kind of segmentation of objects within the image.  This could be useful for example in separating objects from clutter, or possibly in resolving occlusion (although that has not been demonstrated here).   Although mostly toy examples of patterns are used, I believe that this paper taps into a powerful idea, and that it will be of high interesting to the ICLR community, and certainly to the neuroscience community.",1,5103
"This paper presents a variational learning algorithm  for probabilistic graphical model embedded with a latent representation that is continuous-valued. The proposed algorithm is stochastic in nature. It is able to scale to large datasets and deal with models for which tractable EM is not an option. It is based on gradient descent on a Monte Carlo approximation of the standard variational lower-bound on the marginal likelihood. Crucially, in order to obtain an unbiased gradient estimate, the Monte Carlo approximation is taken such that the sampling of the MC points is independent of the parameters of the model, thanks to an 'alternative parametrization trick' of the variational posterior. An interesting connection is also made with autoencoders. Experiments show that the algorithm can improve over the wake-sleep algorithm.    I really like this paper. The idea is simple and non-trivial. The alternative parametrization trick is a nice tool to have in one's toolbox. The connection with autoencoders is also really nice.  The only 'cons' I see is that the experiments are somewhat limited, in the sense that they are performed only on MNIST and Frey Faces, and only log-likelihood or visualization results are reported (as opposed to experiments in the context of a real application). But I still think this is good first step in a potentially promising direction for training complex non-linear continuous representation models. So I'd argue that this work deserves to get in.  Minor comments: - Couldn't an online Monte Carlo EM approach be followed, where each update of the M step would be performed on a minibatch? - This is super minor, but NADE isn't just applicable to binary observations and actually has a real-valued extension (RNADE: The real-valued neural autoregressive density-estimator, by Uria, Murray and Larochelle, NIPS 2013). Otherwise however, as the authors mention, it's not directly related to the approach in this paper.This paper adresses the problem of learning the parameters of directed probabilistic models with continuous latent variables when the latent posterior is intractable.  The proposed method (Auto-Encoding Variational Bayes or AEVB for short) can be summarized as:  1 - Maximization of the variational lower bound as a proxy for the marginal log-likelihood.  2 - Estimation of this lower-bound using a Monte-Carlo estimation.  3 - Using a re-parametrization trick for the parametric variational inference approximation q_{	heta}(z|x), i.e. expressing it as a deterministic function f(x,e) of x and e, where e is a random variable.  The AEVB method is in theory applicable to a wide range of choices for p(x|z), p(z) and q(z|x). In their study, the authors propose a specific application where the prior p(z) is a simple gaussian centered on 0 and where p(x|z) and q(z|x) are both Gaussian distributions with their means and covariance matrices being determined by the output of a neural network. For instance q(z|x) is defined as 	q(z|x) = N(z;mu,sigma^2*I) with mu and sigma being determined from x using a neural network, i.e. mu = f_1(x) and sigma = f_2(x), with f_1 and f_2 deterministic.  This work is closely related to several recent contributions (Generative Stochastic Networks,Denoising Auto-Encoder theory) which are properly discussed.  Several experiments are run on both the Frey-faces dataset and the Mnist dataset:  - quantitatively comparing the lower bound with AEVB vs with the wake sleep algorithm.  - quantitatively comparing the marginal likelihood (with small latent space as is to be expected) with AEVB vs with the wake sleep algorithm.  - showing visualizations of 2D manifolds learned with AEVB  - showing random samples obtained by sampling from the learned generative model   The proposed approach (AEVB) offers a new and exciting solution to the well known problem of estimating the parameters of a graphical model. Both the variational inference and generative distribution can be quite complex as they are both represented using neural networks with hidden layers, however the training criterion is tractable and includes hyper-parameter-free regularization terms which can prevent overfitting, as supported by the experiments.  pros:  - Very good summary of previous work / very good discussion of related work.  - The approach is tractable even with intractable latent posteriors.  - The training criterion contains hyper-parameter free regularization.  - The experiments fully support the practical applicability of the approach. cons:  - Experiments on more complex datasets would have made for an even more convincing argument.This paper proposes a variational approach for training directed graphical models with continuous latent variables and intractable posteriors/marginals. The idea is to reparameterize the latent variables so that they can be written as a deterministic mapping followed by a stochastic perturbation. This allows Monte Carlo estimators of the variational lower bound to be differentiated with respect to the variational parameters.  The proposed method is novel and interesting, and seemingly more practical than previous approaches. The experiments favour comparably and show that this method is quite promising, however they still feel preliminary and don’t give the reader a good sense of the trade-offs and difficulties one might encounter using this approach. A simple way to rectify this would be a discussion of the training: how sensitive the model is to various hyperparameter choices, how robust the model is to overfitting, etc. Another way would be to compare against a tractable model such as PPCA, and show how the approach compares to EM and exact inference. The results here would give a sense of how the method might perform on intractable cases.  On a high level, the auto-encoder presented here reminds me of the back-constrained GP-LVM [1]. The goals are different, but the models seem similar.  [1] Local Distance Preservation in the GP-LVM through Back Constraints, Neil D. Lawrence and Joaquin Quinonero-Candela, ICML 2006  Questions: -Different noise distributions are discussed, but they are not compared. What are the implications of choosing different classes of perturbation?  -How powerful is the regularization effect of the variational bound? At what point does overfitting occur, or do things simply plateau when more latent variables are added?  -An important baseline would be to measure the effectiveness of this approach on held-out data. For example, rather than training this variational bound, one could perhaps use a regular auto-encoder with a similar architecture and train for reconstruction error. How would the authors expect the test-set reconstruction error of their approach to compare to this baseline? Is the only difference between the proposed approach and a standard auto-encoder the variational regularizer (first term of equation 10)?  -Intuitively, what is the regularizer provided by the variational bound doing to the encoding parameters? What would the regularization provided by the full variational approach do to the decoding parameters?  Typos and grammar: -The first sentence could perhaps be reworded slightly.  -Please remove “is” from “is even works” in the sentence ”Conversely, we are here interested in a general algorithm that is even works efficiently in the case of…”  -In equation (6), should z^{(l)} be z^{(i,l)}?",1,5104
"The paper introduces a generalization of the maxout unit, called probout. The output of a maxout unit is defined as the maximum of a set of linear filter responses. The output of a probout unit is sampled from a softmax defined on the linear responses. For vanishing temperature this turns into the maxout response.  While the idea is probably not revolutionary, it seems reasonable and it seems to work fairly well on the datasets tried in the paper.   It is a bit unfortunate that unlike for dropout/maxout, there does not seem to be a closed-form, deterministic activation function at test time that works well. At least the authors did not find any.  Instead they propose to average multiple outputs. This makes probout networks much slower at test time than a maxout network. It also puts into perspective the improved classification results over maxout. It seems unlikely that the common practice of halving weights at test time is exactly the optimal way of making predictions for a model trained with dropout. And it is conceivable that some kind of model averaging will be able to improve performance for those networks, too.  It is interesting that probout units with group size two tend to yield filter pairs in quadrature relationship, and much more clearly so than maxout. In that respect they behave similar to average pooling units. It would be interesting to investigate this further in future work.This manuscript extends the recently proposed “maxout” scheme for neural networks by making the linear subspace pooling stochastic, i.e. by parameterizing the probability of activating each filter as the softmax of the filter responses and then sampling from the resulting discrete distribution. Experimental results are presented on CIFAR10/CIFAR100/SVHN and contrasted with the original maxout work.  Novelty: low Quality: low to medium  Pros: - The idea is somewhat interesting and worth trying, and the manuscript itself is generally well-written - Experiments and hyperparameter choices are generally described in detail, including software packages used - Attempts a fair-handed comparison between the method and the one they are building off (though see below)  Cons: - The benchmark results are quite lackluster: the improvements are of questionable statistical significance (see below) - This questionable gain comes at a >=10x increase in computational cost.  - The experimental comparisons have several shortcomings (not all of them strictly advantageous to the proposed method, however -- see below). - The abstract mentions enhancing invariance properties as the goal but there are no attempts to quantify the degree of learned invariance as has been examined in the literature. See, for example, Goodfellow et al’s “Measuring Invariances in Deep Networks” from NIPS 2009 for one such attempt.   Detailed comments:  The procedure doesn’t yield a single deterministic model at test time, which may be a significant practical drawback as compared with conventional maxout or relu networks.  Still, a more significant drawback to the proposed method is that the test time computation involves several forward propagations through the entire network in order to have a noticeable (but statistically negligible) advantage over maxout. Figure 3b suggests that, on a relatively simple dataset, 10 or more fprops per example may be necessary to yield a significant advantage over the maxout baseline. This is in addition to the cost additional cost incurred by sampling pseudorandom variates as part of the inference process.  A quick computation of a confidence interval (based on the confidence interval of a Bernoulli parameter, i.e. the probability of classifying an example incorrectly) for both the results reported in Goodfellow et al (2013) and this work reveals that the confidence intervals can be seen to overlap significantly:  - CIFAR10 Maxout (no augmentation): 11.68% +/- 0.63%, Probout: 11.35% +/- 0.62% - CIFAR100 Maxout: 38.57% +/- 0.95%, Probout: 38.14% +/- 0.95% - SVHN Maxout: 2.47% +/- 0.19%, Probout: 2.39% +/- 0.19%  The same comparison between maxout and the competitors reported in the original work yields non-overlapping confidence intervals for all tasks above. The original maxout work does not achieve a statistically significant improvement over the existing state of the art for CIFAR10, and these authors report no improvement, suggesting the task is sufficiently well regularized by the data augmentation that their complementary regularization does not help.  Hyperparameter search: while the authors reused the exact same architectures and other hyperparameters employed by the original maxout manuscript in an attempt to be fair-handed, I believe this is, in truth, a mistake that disadvantages their method in the comparison. Certain hyperparameters, critically the learning rate and momentum, will be very sensitive to changes in the learning dynamics such as those introduced by the stochastic generalization of the activation function. Even the optimal network architecture may not be the same. The way I would suggest approaching this is with a randomized hyperparameter search (even one in the neighbourhood of the  original settings) wherein the same hyperparameters are tried for both methods, and each point in the shared hyperparameter space is further optimized via randomized search over the hyperparameters specific to probout. This gives the method in question a fairer shot by not insisting that the optimal hyperparameters for maxout be the same as those for probout (there is no a priori reason that they should be).  The choice of temperature schedules also seems like an area that should be further explored. It seems odd that increasing the temperature during training should help (the paper does not specify the length of the linear (inverse) decrease period, this should be noted). What is the intuition for why this helps? How could one validate these intuitions experimentally?  The claim that the stochastic procedure “prevents [filters] from being unused by the network” is dubious. Section 8.2 of the original maxout paper suggests that dropout SGD training alone is remarkably effective in this respect. This claim should be quantitatively investigated and verified if it is to be made at all.  Finally, the paper motivates around probout units learning better invariances, but no attempt is made at quantitatively validating this claim. As it stands, there is a qualitative assessment of the first layer convolutional filters learned, noting that they appear to resemble easily recognizable transformations & quadrature pairs moreso than those learned by vanilla maxout, but I find this somewhat unconvincing. Just because the invariances learned by vanilla maxout are not always obvious to the human practitioner does not mean they are not useful invariances for the model to encode.Authors propose replacing max operation of maxout with probabilistic version, same what Zeiler did for spatial max-pooling in 'stochastic pooling' paper.   For inference, they run the network 50 times using the same sampling procedure, and average the outputs to get probabilities.  They add a 'temperature' parameter to allow to interpolate between maxout and 'uniform random' sampling, and find that annealing the temperature helps.  Also the analyze the per-layer optimal setting of the temperature and found that stochasticity is most important in first 2 convolutional layers, whereas in the last 2 layers, using probout did not give any advantage over maxout.  There are a few minor corrections, but overall this is a solid submission with high relevance for ICLR.  Issues:  - Minor style issue: use 	ext or mbox for 'softmax' and 'multinomial' inside formula  - Table 3: could add 2.16% error rate obtained by another ICLR submission -- 'Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks'  - There's an explanation for why first layers benefit the most from stochasticity -- how stochasticity 'pulls the units in the subspace closer together.' This is unclear to me, and I would recommend expanding/explaining this view.",1,5105
"The authors investigate learning tensor representations of verbs using fixed vector representations of nouns, building on Steedman's CCGs and Coecke's work on modeling meaning representations using linear maps (so that, for example, a verb 3-tensor is right-contracted with its object noun vector and left-contracted with its subject noun vector, to output a sentence living in another vector space). In particular, they learn the noun/verb/noun mapping, turning the problem into a binary classification problem (where the binary labels correspond to legal combinations, and impostor combinations), using L2 regularized logistic regression.  Results are given on ten verbs, chosen for varying levels of frequency and 'concreteness', and using up to 2000 positive and 2000 negative N/V/N training sentences, constructed using the Google N-Gram corpus and Wikipedia.  The results are compared to a baseline where the verb tensors are constructed using the outer product of the vector representations of their subject and object nouns, for legal uses of that verb. Throughout the work the nouns are represented by fixed vectors using a statistical co-occurrence technique based on that of Turney and Pantel. The proposed technique beats the proposed baseline in accuracy.  This is an interesting and useful direction for research: to try to construct linear maps that more faithfully represent the functional aspects of language (e.g. a verb is a function that takes two nouns and outputs a sentence).  I wonder, however, why the authors didn't just treat the problem as a standard classification problem.  Why map to a two dimensional space, one axis of which models plausibility, and the other, implausibility? How is the point (1,1) in this space different from the point (0.5, 0.5) (i.e. what does simultaneously being 'more plausible' and 'less plausible' mean)?  Wouldn't it be more natural, and simpler, to just map to the real line?  (These questions also apply to Clark's paper from which this idea is inherited.)  Also the baseline seems quite weak, as it is constructed from only positive examples.  Furthermore the generalization performance of the resulting system was split by sentence, not by noun - that is, nouns that appear in the training set can also appear in the test set, suggesting other, simpler ways of constructing baselines (see below).  I do think the paper has some value as a step towards investigating more general linear maps to model language; the link between CCGs and linear maps seems intuitive.   Specific Comments *****************  Numbers are section numbers.  1.0  It seems a bit of a stretch to call the simple gradient descent method used here, 'deep learning'.  2.0  Clarity: I think it would help the reader's understanding to explain how the string 'with' in 'eat with a fork' maps to the category ((SNP)(SNP))/NP.  3.0  Again, why not use a one dimensional representation for plausibility?  What is the intuition for using two dimensions?  3.1  The problem appears to decompose into very small decoupled optimization problems (an objective function for each verb) since the verb parameters are independent and the noun vectors are fixed (right?).  It would have been interesting to allow the noun vectors to be learned also, perhaps starting from your initial values (but a much harder optimization problem).  3.2  The baseline - cosine similarity on the matrix elements of the Kronecker product matrix and that of the target subject/object pair - seems very ad-hoc, and so it's hard to see what we learn by comparing against it.  4.2  A little more clarification is needed here.  From what I understand, you took, for each noun, the top N most relevant context words, each of which is also a vector, thus forming the 'noun context matrix'.  But how were the vectors for the context words computed?  In the same way as the nouns, thus giving a 10,000 dimensional vector for each context word? Also, when you do the SVD, you will wind up with a matrix representation (constructed from either the left singular vectors or the right, depending on how the problem was set up). How do you wind up with a 20 dimensional vector?  If you just used the top 20 singular values, that would seem to throw away crucial information.   5.0  There appears to be a flaw in the experimental protocol: no attempt was made (or at least, mentioned) to keep the nouns that appear in the test set, different from those that appear in the training set.  If this is so it makes the results weaker, as methods that leverage such coincidences may do much better.  For example, we could construct a simple baseline for a given N1-V-N2 test triple by saying that if N1-V appears in the training set, and also if V-N2 does too, then output '1', else output '0'.  At least I think it's important to break results out by such a 'did noun occur in training set' stratification.This paper presents a method for learning 3-modes tensor for representing meaning of transitive verbs. The choice of this representation for verbs is motivated by the Combinatory Categorical Grammar framework; nouns are represented by low-dimensional continuous vectors obtained by a modified SVD on a co-occurrence matrix. Verbs tensors are learned for 10 verbs using subject-verb-object triples extracted from Wikipedia  and Google N-grams. Evaluation is carried out by asking the system to predict the verb given a pair (subject, object).  ---  The paper is clearly written and tackles an important and hot topic. The introduction and section 2 are actually very interesting to motivate this field of research: attempting to combine distributional and combinatorial semantics. However, I find the message a bit too ambitious and misleading w.r.t. the content of the remainder of the paper, and hence its actual contributions. Indeed, the paper ends up by learning representations for 10 verbs (nouns representations being obtained using methods developed in previous work). This might be an important first step, but this is not exactly what the introduction and title indicate.  The sentence of the introduction 'One goal of this paper is to introduce the problem of learning tensor-based semantic representations to the machine learning community' is a bit strong, since there exist many tensor learning work, perhaps not exactly in the same context but still, as well as the paper by Krishnamurthy and Mitchell from last year (cited in the paper).   Besides, the learned tensors are quite strange since one slice has only 2 dimensions for predicting plausible/implausible probabilities. What is different from using a single value to predict the plausibility only? This would imply simply representing verbs by matrices, but I wonder if the result would be much different. In test, I assume that the ranking of verbs given pairs of (subject, object) is carried out by sorting using the plausibility probability. The role of the second value destined to encode for implausibility is unclear (and hence the role of tensors with a mode of dimension 2).  Even if using such tensors was fully justified, the paper needs to propose a comparison with a baseline encoding verbs as matrices (the current baseline is too weak). For instance, the paper by Jenatton et al. 'A latent factor model for highly multi-relational data' (NIPS 2012) proposes very similar experiments by also training on subject-verb-object extracted from Wikipedia to learn matrices for representing verb meanings. I think this system or an adaptation of it would make a much better baseline.  Other remarks: - Which norm is used in Equation 1? L2-norm for tensors or matrix is ambiguous. How was set lambda? - In Equation 2, f_{w_i , c_j} should be defined. - It seems that there is a single N (and not 1 per noun). This should be made more clear. - Figure 2 is rather unclear. What is K? The size of the noun vectors? - In the beginning of Section 5, for the second experiments shouldn't it be 52 instead of 42?This paper introduces a method to train very low 2-dimensional representations for different syntactic types and evaluates it on a benchmark of selectional preferences.   Regarding 'assume that words, phrases and sentences all live in the same vector space. The tensor-based semantic framework is more flexible, in that it allows different spaces for different grammatical types, which results from it being tied more closely to a type driven syntactic description; however, this flexibility comes at a price, since there are many more paramaters to learn.' In fact, for non-toy tasks, the number of parameters will be too many. Realistically, to capture cooccurence counts of single words, single vectors need to be at least 100 dimensions, which will then require 1000000 parameters to represent a simple adverb.  Furthermore, one can argue that this particular tensor based framework is actually less flexible in that it could not compare the semantic similarity between simple phrases such as 'a rainy day', 'raining all day' and 'sunshine' or 'swim', 'swimmer' and 'swimming'. People are able to easily determine how similar or related these concepts are and a framework that assigns them representations of different dimensionality is not flexible enough to do this.  Regarding: 'plausibility space to have two dimensions, one corresponding to plausible and one corresponding to implausible.' This could be simply captured by a single number that is high for plausible and low for implausible? In fact, in standard machine learning a 2 class softmax (which is being used in this paper) is usually collapsed to a single sigmoid unit.  Since the MEN dataset has just been introduced 2 years ago and it is not very well known, it would be great if you described it in your paper.  Section 5.1 mentions problems with rare words. How could this approach be extended to learn representations for rare combinations or words? People can certainly understand a word from very few examples. On a related note, could this approach ever learn representations for all syntactic types in setups that move beyond trigrams? Presumably, the counts for most such n-tuples will be extremely sparse.  Since you used techniques from neural networks to train your model, why not compare to neural networks to predict the same statistics? How was your SVD word vector representations cross validated? Did you try alternative word vectors?  This paper works in the interesting area of combining discrete syntactic information with distributed representations. It is well written but some doubts remain about the approach.  While the paper is not very novel (approach A + approach B) it may expose the ml community to type and syntactic approaches.",0,5106
"Dear authors,  Let me reveal my identity before I continue. I'm Kyunghyun Cho who wrote the earlier review (by an unexpected coincidence). The previous comments reflect what I wanted/want to say as an official reviewer, and I will not write another separate review.   Instead, let me answer briefly to the response to my review from the authors:  (Authors) 'If you change the value of the hidden units in any way, this delicate equilibrium is broken and you expect to get much lower probabilities for the visible variables.'   => I cannot agree with this. Intuitively, if we believe that training makes latent variables learn potentially lower-dimensional manifold on which training samples lie, any small (or even large) change in the latent representation shouldn't correspond to a change in the input space that moves the point away from the manifold. I'd agree with your argument much more, if you were flipping all the bits of the input variable.  (Authors) 'CD_1 as it is the cheapest way we know'  => Computationally, I don't see why CD-1 should be cheaper than PCD. Memory-wise, if you follow the usual practice of maintaining only a few persistent samples, it shouldn't matter too much as well. Though, I agree that it's easier to train an RBM with CD-1 using a much higher learning rate, which may make learning progress faster.  - ChoThis paper investigates stopping criteria for training of RBMs by contrastive divergence (CD). Traditional reconstruction error is compared to a ratio of probabilities, namely the probabilities of the training data divided by the probabilities of an equal number of sampled points (with two variants of the sampling strategy). By dividing probabilities, the partition function cancels out, which makes computations tractable. Experiments on two toy datasets show that the two proposed variants are overall more useful than reconstruction error to identify the point of maximum likelihood on training data, even though they do not work on all cases investigated here.  The problem of early stopping of RBM training is definitely a relevant one, since the intractability of the partition function prevents properly monitoring the likelihood. The approach suggested here, however, lacks in both theoretical motivation and empirical evaluation, thus in my opinion is not quite ready for publication.  On the theoretical side, the proposed criterion (eq. 8) is only heuristicallymotivated. Note in particular that a model might be able to reach an arbitraryhigh value of this criterion if P(y_i) -> 0 for some y_i, regardless or howbadly it may perform on the training x_i's. I'm also not sure why one would necessarily take N y_i's: why not generalize it to M y_i's, using Pi_i P(y_i)^(N/M), so as to be able to choose the sample size based on available computational resources? Finally, I am not convinced by the choice of y_i's: first, the sampling rules (random h or '1 - h_i') are not well motivated (there is no guarantee that we will not sample training points), second, since they are defined from the model parameters they lead to a set of y_i's that evolves during training (thus the criterion may be unstable), third the y_i's are expectations and there is no explanation on whether it makes sense for binary RBMs.  On the empirical side, my first concern is that experiments are performed on low-dimensional toy datasets, and there is nothing to tell us that  behavior observed on such datasets will actually translate into higher dimensional tasks. In particular, sampling-based methods tend to behave rather nicely in low dimension, but may break horribly as the dimension increases... Thus it would have been good to add experiments in high dimension, for instance using AIS to estimate the partition function. My second concern is that only training errors are reported: although they are definitely interesting to monitor, someone using reconstruction error as a stopping criterion will always use a validation set for this, and will hope to stop at a point where validation log-likelihood is maximized. The comparisons in the paper are thus, for the most part, uninformative, since they only use the training data.  A few more minor points: - Eq. 5 is missing some characters - The number of hidden units is not mentioned in the experiments - Plots show 'reconstruction error' as something that is better when it increases,   which is counter-intuitive for an error - Something potentially worth discussing is that RBMs are often used for   pre-training purpose in deep networks, and it is not clear that better   likelihood => better pre-training (if there is work on this topic, it should   be cited, as it is important to motivate this direction of research) - Another application worth mentioning to this kind of technique is model   selection (which RBM is best?) => the proposed criterion may require a bit   of tweaking to answer this kind of question (common y_i's are needed)This paper introduces an early-stopping criteria, to address the potential degeneracy issues in CD-training of Restricted Boltzmann Machine. The method is closely related to the method of Free-Energy differences (FED), which computes the difference of log probabilities between the training set X and a held-out validation set Y. In lieu of a validation set however, the authors propose to sample Y to points far from the training data (for which the log-prob should monotonically decrease during training). To do this, the authors propose sampling Y from the conditional distribution p(x | h), where h is either drawn uniformly over binary states, or clamped to the 'complement' of the infered latent states when conditioned on training data, i.e. h = 1-h^(i) with $h^(i) ~ p(h | x^(i))$.  Given the similarities to FED, the method is not entirely novel. The idea of biasing the Monte Carlo approximation of log Z to states $v$ sampled from the conditional p(v|h) is interesting and reminiscent of [1,2]: a uniform prior over h could be a quick and dirty way to explore the space of visible configurations having some probability mass under the model. The second option seems less justified in my opinion: why use the complement of h^(i) instead of h^(i) itself ? The former would allow to quickly sample (local) perturbations of x^(i), which seems more inline with the CD training criteria (which only raises the energy of nearby configurations).  All in all the paper is interesting, but much better suited to the workshop track of the conference. The method is not entirely novel and the experiments are still very preliminary. (1) The datasets are very small and would need to be scaled to more realistic datasets, using AIS as a proxy to the true likelihood. (2) The evidence in favor of the proposed criteria is rather weak. The behavior of the first criterion changes completely based on the dataset, while the second criterion fails when using CD-10 (with no clear explanation why). Also, the early-stopping point obtained by (ii) on the LSE dataset remains rather approximate. Using this criterion would have yielded a much lower likelihood than possible. (3) The authors seem unaware of the FED method and do not compare against it.  As the proposed criteria are based on heuristics, they require very solid empirical evidence to justify their use. As it stands, the paper simply does not deliver.  Other: * paper glosses over another widely used early-stopping heuristic: classification error. * superfluous citation to (Bengio, 2009) for efficient block Gibbs sampling of RBMs. * latex bug for visible and hidden unit biases in Eq 5 * 'In doing so, drastic approximations [...] are performed'. Do not understand this sentence. * 'making zeta when learning is achieved' ?? Learning is an optimization process not an discrete time event. * 'A second possibility is to suitably compute [...] during data reconstruction'. Very clumsy way to say use 1-h^(i). The reconstruction phase conditions on h, therefore 'the value they should take during [...] reconstruction' is nonsensical. * 'Anyway, the behavior of the proposed criteria [...] should be further studied.'. Very uninformative statement whose writing style seems *very* inappropriate for a paper submission. * 'This westimator works well for CD1 but [not] for CD10, which is' - Missing [not]  [1] Grégoire Mesnil, Salah Rifai, Yann Dauphin, Yoshua Bengio and Pascal Vincent, Surfing on the Manifold, Learning Workshop, Snowbird, 2012. [2] Yoshua Bengio, Grégoire Mesnil, Yann Dauphin and Salah Rifai, Better Mixing via Deep Representations, in: Proceedings of the 30th International Conference on Machine Learning (ICML'13), ACM, 2013",1,5107
"This paper bravely proposes to test the empirical convergence of stochastic optimization algorithms using a vast collection of simple and relatively standardized tests. They explain how they construct the tests and perform experiments that lead to a striking visualization (figure 5).  Unfortunately none of the compared algorithms appear to solve all the problems robustly.   This idea could appear naïve because it is not supported by theoretical considerations and represents a purely empirical perspective. However there are many reasons to consider that this idea has great potential. First, similar ideas have worked in other fields. It is customary to compare general optimization codes on a collection of well known benchmark problems, not because it provides a guarantee, but because it provides a sanity check.  Second, we must recognize optimization in deep learning systems is still beyond the reach of theoretical analysis.  According to the current theoretical knowledge, it should not work. Therefore the best way to investigate such algorithms remains a well designed collection of empirical comparisons. The comparison described in this paper is a good start in that direction.Summary ------------  This paper introduces a suite of unit tests for optimization algorithms that attempt to analyze the algorithm performance in very specific and isolated cases (for example : how does it deal with a saddle point or a cliff like structure in the error). This work feels like a needed step in the right direction. As the authors pointed out, passing these tests is not sufficient to claim an algorithm is better than another, but it is necessary to understand some possible weaknesses of the algorithm and for doing a more proper comparison between different algorithms (compared to just looking at the error curve for some randomly selected task and model). Comments: --------------  I think the success of such an approach is in the details. The engineering effort to use a specific such framework to test some new algorithm will have a huge impact on whether these unit tests will be successful or not. Unfortunately some of these things are hard to predict right now (and in some sense are beside the point of the paper). The other thing that one has to keep in mind is the effect of high dimensional spaces on the problem we try to solve. E.g. one can prove that for some family of models (say auto encoders) on some bounded domain some given error will have saddle points regardless of the dimensionality of the input. However the distribution of this saddle points is a different story and it might be hard to say how important it is to deal with saddle points or not for this specific model.  Basically what am I trying to say is that the existence of some prototype in real data is easy to assert, but the probability that you will have to deal with said prototype is hard. And this is not a criticism to this work, but more a message that anyone should keep in mind. I guess what I'm trying to point out is that interpreting the results of these unit tests is far from trivial and might be even counter-intuitive.  Another important detail for any such suite of unit tests is the tools that one can use to investigate the results. If I think of the unit test metaphor for code development, usually the results of a large suite of tests is interpreted via failure (and how many tests failed) or success. Running such a suite of experiments on an optimization task is not as easy to interpret. First of all is hard to know which tests we care most about, and is hard to know how the correlation between the failures on different tests affect the algorithm on real task. While the paper mentions that there are tools for visualizing these results, there are not many details given about these tools. I think an interesting question on its own is what are the write visualization of the results and how to interpret them, a question which I don't think is fully answer by the current work.This paper looks at developing 'unit tests' for stochastic optimization algorithms, which consist of toy objective functions and corresponding gradient noise that are assembled randomly from a suite of various components.  The hope is that these tests would allow one to analyse the specific failure cases for various methods, perhaps in order to inform improvements to them.  This paper is mostly just engineering, but the authors seem to have created a fairly versatile tool for generating toy problems with many different characteristics which may well be quite useful in the future.  The paper itself doesn't contain much in the way of useful conclusions about the optimization algorithms (mostly different versions of SGD) that are tried.   I also have several issues with various aspects of the unit tests themselves, and am not fully convinced that they are testing the 'right' kind of thing or if these tests can tell us much of use about the optimization problems we really care about.   It would have been nice if the paper had demonstrated how these tests could actually inform algorithm design.  Nonetheless I would recommend that it be accepted.  I hope the authors can address some of the issues I've brought up.   Detailed comments:  Are these optimization surfaces unimodal?  If not, couldn't it be the case that some optimization methods simply will get 'lucky' and bounce their way into a better local basin, whereas other that might be more careful about remaining stable in the face of noise will miss these?  This seems to me like it might not be a realistic analogy of the kinds of optimizations we care about, where multimodal landscapes with multiple modes of highly variable quality don't seem to exist.  I'm thinking about neural network optimization in particular here, and going on my experience that typically the lowest achievable error from run to run doesn't vary all that much (if at all).  Another major issue I have is that by testing local convergence of methods you are really only testing one them in a single and likely not too important phase of optimization: fine convergence to a local min.  Often in these cases the method that can deal with the noise the best wins.  I'm not sure if this is the best analogy to what happens in deep net optimization, where fine convergence to a local min seems to be only a small and not particularly important phase of optimization (which is often just associated with overfitting).  I would wager that the most significant aspect of optimization is the journey towards a local min from very far away along a very curvy path that can possibly lead to other local mins of roughly equivalent quality.  It is not clear to me that your unit tests properly capture this aspect of optimization by focusing either on fine convergence to local optima, or the tendency for an optimizer to jump out of one local min into a closely situated and much higher quality one.   Page 1:  In what sense do you mean that these weaknesses are separate from 'raw performance'.  If an algorithm is performing well on some task, why would I care if it has some invisible issues on that task, as long as it appears to be working?  I think I'm misinterpreting what you meant here.  Page 1: I don't really understand what you are saying in the 'locally' paragraph.  Are you saying that you want to assume local optimization because your units tests, if they are to be run quickly, can only consider toy examples with a simple local structure (unlike a neural network which has a 'global structure')?   And what does this have to do with non-stationary algorithms (or do you mean non-stationary objective functions?) or 'initialization bias'?   Don't you still need to initialize the algorithms in the unit tests and won't this choice affect the relative performance of different methods?  Page 3: With these noise prototypes, is it obvious why their mean must recover the gradient (i.e. so they are unbiased)?   For the additive case this is clear, since the noise has mean 0, but it is less clear for multiplicative noise.   I suppose if the average scale is alpha the multiplicative noise would on average estimate alpha*gradient.    This is an important point, since the stochastic gradients need to be unbiased for many stochastic optimization algorithms to work, at least in theory.   Cauchy noise, for example, doesn't even have a mean, so this seems a bit problematic.  Although I suppose for certain choices of its parameters, the distribution is at least centred around 0.  Perhaps this might be good enough in practice.  Is this what you did?  The mask-out noise is unbiased as far as I can tell.  You really should discuss the issue of unbiasedness of your noise in general, as this is very important to the theory of these methods.  Page 4:  How are you generating these random rotations?  Are these just like random orthonormal matrices?  Generated how?  Page 4:  It is not clear to me why gradient descent should work at all when it is given vectors from a vector field that is not actually a gradient field.  And assuming it does work for certain such fields, the reasons are probably subtle and highly situation dependent.  I don't imagine that this can be easily simulated by applying a fixed rotation to the gradient.  In fact, I can't even imagine how gradient descent could ever converge if it were given gradient with a fixed rotation.  For example, you could just permute the coordinates.  That is a valid rotation, but surely this would cause most reasonable algorithms to fail catastrophically.   Also, you should explain concept of curl etc for this audience.  Page 5:  What do you mean in the sentence: 'However, non-stationary optimization can even be important in large stationary tasks, when the algorithm itself chooses to track a particular aspect of the problem, rather than solve the problem globally.'  What do you mean by 'aspect', 'track' and 'globally' here?  This goes back to my previous question.  Page 5:  I think a better way of simulating non-stationarity of the objective function would be to perhaps randomly jiggle or move the various shape and scale parameters that define your objective.  Mere translation doesn't seem particularly hard to deal with, or realistic.   Page 6:  Why didn't you test the method(s) from [9]??  Page 6:  Double the progress of vanilla SGD doesn't seem particularly 'excellent' to me.  Perhaps merely 'good'.  Page 7:  What are these 'groups'?  Where do you describe what they are?    Page 7:  So if I understand correctly, the vertical axis gives the different methods with different hyper parameter choices for each method?    Many of these methods, such as Nesterov's accelerated gradient, have automatically adapted hyperparameters, which is sometimes done according to a fixed schedule.  The momentum parameter in particular usually isn't supposed to be constant, at least in theory.   And in general, for stochastic optimization methods, there usually are no guarantees for a fixed learning rate.  Some methods like ADAGRAD implicitly anneal the learning rate, while with others, like plain SGD, or accelerated gradient SGD, you need to reduce the learning rate adaptively or at least with a schedule if you plan to have fine convergence to a local min.  Are you keeping these fixed, are you are instead varying the hyper-hyperparameters of the methods that adjust the hyperparameters?     Are methods with 'thicker' regions ones with more adjustable hyperparameters?  It seems to me that it doesn't really make sense to plot the values for all hyper-parameters, when some are clearly crazy.  If a method has lots of hyperparameters, it shouldn't be judged to be 'less robust' if for certain crazy choices of these it diverges.  And it isn't always true that we have no way of determining good hyperparameters for methods that have them.  Binary search, or perhaps Bayesian optimization are certainly better than exhaustive sweeps.  More simply than that, one can just adjust these things on the fly, with a heuristic or manually if needed, as partial progress with sub-optimal hyperparameter choices doesn't need to be thrown out (unless very bad divergence has taken place, and then you can always backtrack to a previous snap-shot of the parameters).",1,5108
"This work aims to perform simultaneous detection and viewpoint estimation in the face of having few training examples for many classes or viewpoints, but where many examples are available for a smaller or related set of source instances.  To accomplish this, two new types of weight regularization are described, for use with deformable parts models.  Both regularizers form a quadratic penalty on the weights by means of a covariance matrix, constructed using models trained on the sources.  The first, SVM-MV, constructs the covariance by averaging across all pairs of HoG cells that overlap between adjacent object viewpoints; overlaps are found by projecting to a (provided or guessed) 3D object model.  The second, SVM-Sigma, constructs an explicit all-to-all covariance of the weights by averaging across different model instances.  SVM-MV extends the ideas of Gao et al. to work across views, while SVM-Sigma breaks from these careful constructions and uses all pairwise interactions that arise.  The authors evaluate their effectiveness using two datasets, 3D-objects and KITTI, concluding that such regularization enables good performance in these tasks, with SVM-Sigma generally outperforming the other methods.  Unfortunately, the exposition is rather dry and can be hard to follow.  Illustrative examples and diagrams would help a lot here, though, particularly sketches of the projections and overlaps for SVM-MV.  I think it also would help to instantiate model early on, using one of the datasets from the experiments (i.e., in sec 3, linking sim_n, w^s, w^t to concrete instances).  I'm also a bit confused on what exactly comprised the source vs target data.  For sec 4.1 (3D-objects), how was the data divided between sources (used for the priors) and the targets?  Was there any overlap between these, either by datapoint or by object instance?  For sec 4.2, p.8 para 1 seems to say that for KITTI, the priors were trained from source data drawn from either from 3D-objects or KITTI (i.e. there are two different cases).  In the former case, did viewpoints need to be mapped to transfer between the datasets, and which object classes were used?  In the latter case, did the prior data overlap the target data?   Pros:  - Presents new regularizers that exploit structure relations, learned from the data in cases where there are dense subsets or aggregates  - Experiments are detailed  Cons:  - Dry and hard to grasp - Could use more illustrations of the method and problem setup - Results presentation confusing at times    Minor comments:  - Fig 1 right, TD2ND:  the labels along the rows (y-axis) appear swapped:  The text indicates the block with ones should connect with-data to no-data.  - I found the italics on occurrences of 'target' and 'source' somewhat distracting; it tended to take my eyes away from the parts of paragraphs I wanted to concentrate on much of the time.  - p.5 last para:  says there are 9 object classes, but the webpage for this dataset says there are 8?  - p.6 (Experiments sec):  k in {1,5,10,all} -- would be nice if this said how many 'all' is as well.  - Fig 2:  figures could use titles  - Fig 3:  I'm confused about which views were included/excluded for each plot -- are the included views progressive subsets?  It looks like the differences are more than that.  Maybe a key with on/off bitmap listing each view would help.Summary  This paper proposes to improve multi-view object detection and viewpoint estimation, particularly in the case where some viewpoints are undersampled, by introducing a multi-view prior into the standard SVM training framework used to learn many HOG-based object detectors. The work extends Gao et al. [16], which considers a specific case of the more general form of priors considered in this paper. (A citation is missing for a very relevant paper by Hariharan et al. [a] which also estimates covariances between HOG cells.) The paper presents an extensive empirical study of two newly proposed priors (SVM-Sigma and SVM-MV) compared with the SVM-SV prior of Gao et al. [16] and a standard SVM with no multi-view prior.  The experimental results are dense and at times hard to parse, but SVM-Sigma shows a clear advantage on several benchmarks.  Pros + The topic is quite interesting and relevant to researchers working on object detection and coarse viewpoint estimation. + The outline of the approach is clear. + The experimental results look quite good.  Cons / Questions for author feedback - While the outline of the approach is clear, some of the details are hard to follow. A main confusion throughout the paper is what data is used to estimate the prior? For example, when the target is KITTI and the prior comes from 3D-Objects, are only the car objects used from the 3D-Objects dataset (I would assume so, but this was unclear to me). - For the MV prior “...we first establish pairs of cells in the target model which satisfy a certain relation type ~n.” It’s clear how these pairs would be established when CAD data is used. How are these correspondences established in the case of KITTI data? - Sec. 3.2: I think it would be good to clarify what is meant by “bootstrapping” (i.e. training multiple models on bootstrapped samples) to avoid confusion with the ill-named hard negative “bootstrapping” for training SVMs. - Is setting a single value for C reasonable? The regularization (via the prior) is changing quite a bit and it’s not clear that keeping C constant is reasonable. That said, searching over C can only improve the already good results. - In Table 2 the ‘base’ SVM-Sigma results are the same for 3D-Objects and KITTI priors---perhaps a bug in the table?  [a] Bharath Hariharan, Jitendra Malik and Deva Ramanan. Discriminative decorrelation for clustering and classification. In ECCV 2012.The paper presents a method to learn a quadratic regularizer that improves the performance of multi-view object detectors when very little training data is available for each object or view.  The regularizer is computed using a sparse correlation matrix to identify similarity amongst feature weights in the detectors for different views of the same object (in some cases relying on a 3D model to help establish correspondence between features).  The similarity matrix is then used to define a Laplace regularization term for a standard SVM, thus requiring that a new multi-view detector trained with the regularizer share similar structure across views.  Results are presented on several detection tasks using the KITTI  (driving/urban) dataset and 3D Object Classes dataset.  It is shown that the main advantage is that the method can learn to detect views of novel target objects even when some views of these objects have few or no training examples.  Overall the paper is clearly written and the experiments are extensive.  The authors broke out all of the different cases [number of examples per view, and which views are missing] to make their point.  In the regime where the method is intended to help (where very few examples are available for a particular view of a target object) the regularizer does help for the multi-view DPM model considered in the paper.  The main caveat here is that the method seems to assume the use of separate detectors for every subclass and view (and thus each detector requires its own dataset).  That really exacerbates the problem of too little data where other methods might not have an issue.  This is a nice trick, but since the one-detector-per-object-per-view approach has multiple scalability issues, it looks like the proposed solution also suffers from those barriers.  If this approach to multi-view detection is not workable in the near future, what is the high-level idea that we should take from this work?  I do not see it.  Also, the “dense” sparsity pattern appears to be by far the best performer.  This is somewhat interesting by itself, but also detracts from the “sparse” proposals in the paper (and, of course, the dense approach is not very scalable).  Some candid discussion on the consequences of this result might clarify what parts of the system are important.  Pros: A relatively simple idea to exploit knowledge of regularity across views in multi-view detectors. Primary value is in cases where there are few or no examples for a particular view, in which case it does appear to help over baseline approaches. Cons: Relies on a fair amount of prior geometric knowledge. It is unclear how to apply this to cases where the number of features is much larger than DPM-style models [e.g., deep neural nets] or has deeper difficult-to-interpret layers where geometric information cannot be leveraged. Other: The SVM regularization penalty might need to be tuned for complete fairness of comparisons.  Since the regularizer is fundamental to performance and the number of training examples is varied, the penalty setting could alter the testing numbers.  (It is possible that the experiments/implementation are set up in such a way that this does not matter much;  a note to this effect, if true, would be helpful.)",1,5109
"The paper starts by describing the success of deep learning, which has been incredible, and from the very beginning sounds like the standard paper of 'let's try a successful technique from other areas in our problems.' While interesting, it has problems to start with, not all popular techniques should be applied to all problems.  The paper presents an interesting introduction, with philosophical components, and some I either don't understand or they are not accurate. Oracle is the theoretical limits of a problem, and if there is something we are fulling lacking is theory for deep learning (though some old one exists), so not clear what the authors mean.  Second, the authors should be aware of works on model compression that show that deep learning might actually not be needed and shallow nets can achieve almost same results, a paper on the topic even submitted to this conference.  I am not an RBM expert, only familiar with the subject, so I read it but I am not judging its correctness.  Was SM (Figure 1) ever defined? I can't find it even with my search function in the editor. This is critical.  Not critical if when comparing with ICA, you are using the last techniques developed by Smith and his team and available in the public code from Oxford (double ICA, etc). So are comparisons with the state-of-the-art? Also, while is stated for Fig. 3 that RBM results are more supported by biology, no reference is providing to support this claim.  The multiplayer comparison in Section 3 makes no sense since parameters are significantly increasing when adding layers. The authors should look at work on model compression to see how these comparisons need to be made, and they might be surprised with the results, or maybe not, but increasing parameters by orders of magnitude is not fair.  The paper has major issues then, but the authors have done a lot of experimentation, and while under normal circumstances I would certainly recommend to reject, I think that their presence at the conference can lead to interesting discussions. The authors have not convinced me at all that their approach or deep networks in general is useful for their task, but I do believe in open discussions and I believe the community will benefit much more from the science that might come out from them defending their work at the meeting than from us rejecting outright the paper. Hopefully after the meeting we might be able to conclude if this is a direction worth investigating or not.Summary:  The paper applies deep learning methods to various problems in MRI data analysis, showing that DBNs can recover similar features as other standard techniques, and can possibly improve classification performance in certain tasks.  Major comments:  The classification experiments in this paper are hard to follow, and as written it seems they may be flawed:  For the results in Section 3.2, it sounds like the models may possibly have been pretrained on the testing data, and worse, possibly fine-tuned on the testing data as well. In particular, the model is pretrained and fine-tuned on 335 of 389 examples, and then, subsequently, the top layer activations of these fine-tuned models are supplied to a new shallow classifier that is trained on a different train/test split of half the 389 subjects in each. Each of these splits will thus necessarily contain many examples from the training set used for pretraining/fine-tuning. This means there will be many test examples on which the underlying DBN model has already been fine-tuned with the correct label. Hence the performance numbers are inaccurate.   For the results in Section 3.3, all data examples are used for pretraining/finetuning, and hence it is not clear to what extent the learned classifiers will generalize to new examples. This is particularly important to check given that the three layer network attains perfect accuracy, which may indicate overfitting. These concerns are moderated by the fact that dimensionality reduction applied to the learned representation shows an interesting structuring by disease severity, and this data was not used in the training process. I also recognize that, in this data analysis application, there are other important goals beyond generalization accuracy. If the trained model identifies particular features that are interpretable in the light of other known data, that can be very helpful. Drawing out these other uses of DBNs is an interesting direction to pursue.  More details of the RBM model and training procedure should be included in the paper. RBMs typically use 0-1 valued inputs, and the sigmoidal form of the activation function arises by manipulating the energy function. Some details which would strengthen the paper: How was the switch to Tanh activations performed? Were weights updated using contrastive divergence to approximately maximize log likelihood? If so, how many sampling steps were used (e.g., CD-1, CD-10)? Was sampling used, or were the updates based on the mean field approximation? How were tanh units sampled from? How was L1 regularization added? E.g., was it ||tanh(Wx)||_1, or ||sigmoid(Wx)||_1 or ||Wx||_1?   Depending on how the switch to tanh units was performed, the RBM algorithm can be nearly identical to ICA. See, for example, Q.V. Le, A. Karpenko, J. Ngiam, A.Y. Ng. ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning. NIPS, 2011. This connection should be cited and discussed. In particular, the statement 'In general, RBM performed competitively with ICA, while providing--perhaps, not surprisingly due to the used L1 regularization--sharper and more localized features' may need some adjustment. L1 regularization is the objective to be minimized in ICA too--if anything, the difference is due to less stringent orthogonalization.  Some of the given parameter values are confusing. The learning rate is given as epsilon = 0.08, but this is not in the 'workable range' of [1e-4, 1e-3]. Were these parameters hand optimized? How were network sizes chosen?  Some claims are stated without providing the relevant data. E.g., 'Moreover, the block structure of the correlation matrix (not shown) of feature time courses provide a grouping that is more physiologically supported than that provided by ICA.' More discussion of this, maybe in a supplementary materials section after the main paper, would be welcome. It is also stated that RBM bases are more 'spatially concentrated' but this is not quantified or clearly established in the presented data.  The text is often unclear and hard to follow (particularly section 3.2).  Pros/cons: + Interesting new application area for DL + Some results that hold promise  - Flawed experiments mix train/test data - Insufficient details of model and training method   Minor comments:  The first page footnote should be removedThe English in the paper is poor making it hard to read.   There is a lot of material and the changing architectures, datasets and tasks make the whole thing hard to follow.   You introduce sMRI without explaining it.   You don't describe the temporal nature of your data. (I presume it's one volume every T ms, but you have to say that and tell us what T is.   The datasets are specialist and results aren't compared to other authors, so it's hard to really understand performance.   The task in 2.2 is not explained. What are the classes in the GT. is this really an unsupervised task? The results (what is being measured) in Fig 1c are not clear. Please elucidate.   Conventionally I believe a DBN is a stacked RBM - when you add a classification layer, please call it a DNN. For the RBM / DBN please specify the connectivity. Are inputs / hidden connected all:all?   By adding a second layer you don't explicitly show that depth helps the improvement could equally be because adding more parameters helps. You need to show the correct control experiment. Adding a 3rd layer may similarly be causing you to overfit because you added too many parameters.   You don't explain the reasoning behind applying ICA spatially and the DBN temporally.   Is the ICA implementation on a GPU too?   You say 'The training accuracy at the fine tuning stage for the 335 subjects was: 82%, 87%, and 86.5% respectively.' What are these 3 numbers? You can't say respectively in this sentence.",1,5110
"The authors claim that a simple two-layer fully connected neural net can outperform or meet state of the art accuracies on large, multi-label classification tasks, using rectified linear units and dropout.  They make novel use of L2 regression to compute a data-dependent threshold to map model outputs to class labels.  While the paper is interesting, the baselines seem weak, in particular, the comparison against a neural net ranker with exponential versus cross-entropy loss.  The authors make a point several times in the paper with which I disagree - that ranking approaches do not scale to large datasets.  The existence of effective web search engines is strong evidence to the contrary.   Specific comments *****************  There are many typos and grammatical errors but few that change the meaning, so below I only mention the latter.    Eq. (2): For clarity's sake it might be worth mentioning that f_0 has range [0,1] and that the labels y are in {0,1}.  Fig. (2): Yes, but the high slope of ReLU can also cause problems if there is noise in the data (which appears as outliers).  I think that a much better comparison than the one with BP-MLL would be to compare with RankNet, a neural net ranker that uses a cross entropy error function and thus is much closer to the cost function used in the paper.  It does seem to make sense to rank all outputs corresponding to positive labels above all outputs corresponding to negative; the question left open by your comparison is whether the problem lies with the use of the exponential cost in BP-MLL.  Unfortunately there are at least two confounding factors that you have not separated in comparing BP-MLL versus your method: the use of ranking at all, and the choice of ranking cost function.  So I think it's important to compare against a cross entropy ranking function.   'The ReLU disables negative activation so that the number of parameters to be learned decreases during the training.' - what do you mean?  Section 2.3: define the Delta used in Adagrad  Section 3: this is a nice idea, for choosing data dependent thresholds - is it novel (for multi label classification)?  Sec. 4.1: it would be useful to give brief definitions of the ranking measures used ('Rank loss, one-error, etc.'), since, except for MAP they (or at least, these names for them) are not well known.  Sec 4.2: my impression is that your choice of SVM baseline is weak (also, few details are given). Binary relevance seems like a poor choice for the multilabel task.  Why not compare to SVM rankers?  And since you're comparing to a nonlinear system, reporting results using nonlinear kernels would be good to be more complete (although linear SVMs usually do work well on text classification, this is a different task).  Table 2: 'B and R followed by BR are to represent' should read 'B and R following BR represent' and would be even better written as simply 'B and R subscripts represent'  The results in Table 2 seem to be mixed. Sometimes dropout helps, sometimes it hurts. Sometimes the SVMs win, often not.  The only take-away that seems safe to me, is that if you want the best performing system, then try all these methods (and other, stronger baselines - see above) on your data, and pick the best.  I think the most interesting result is that NNs (lumping with and without dropout together) beat linear SVMs, which is usually the strongest performing method for text classification.  But this task is different, and so it would have made more sense to compare against SVM rankers.  Given the use of ReLU transfer functions, what different did L1 regularization on the hidden activations make?  Figs 3a is great.  Fig 3b is a little misleading, though - you chose the one dataset where adding dropout helped.  What does that curve look like on a dataset where adding dropout hurt?  Section 6.1:  'the presence of a specific label may suppress or exhibit the probability of presence of other labels' - I think you just mean that some sets of labels tend to occur together, and this is ignored in binary relevance methods, but please explain this more clearly.This paper tackles the problem of multi-label classification using a single-layer neural network. Several options are considered such as thresholding, using various transfer functions or dropouts.  The paper is full of imprecisions, writing errors that make it hard to read (the paragraph 'Computational Expenses' of Section 2.2 is perhaps the worse from this point of view), even if it is possible to get most of the content.  The whole paper is based on a comparison with BP-MLL, which is quite problematic for two reasons. First, it is not clear that BP-MLL is the best suitable baseline. What motivated this choice? Their pairwise exponential loss function is not standard, most ranking methods instead choose the hinge loss). What is their architecture? We need more arguments to assess this as a strong benchmark. Besides, the comparison made in the paper between the proposed network and BP-MLL is not valid: influences of architecture, choice of transfer functions and loss are all mixed. In the end of the 'Plateaus' paragraph, it is suggested that ReLUs allow to outperform BP-MLL but no experience with BP-MLL with ReLU has been conducted. Perhaps the loss of BP-MLL (PWE) could be as efficient as cross entropy with ReLU. Is BP-MLL used as a benchmark as a network or simply as a loss (exponential)? What are the results with hinge loss?  Most results about neural networks are not particularly new. It is already quite well known that (1) Dropout prevent overfitting and (2) networks with ReLUs are easier to optimize than those with Tanh. In that sense, Figure 3, which is quite nice and sound, or Section 5 do not bring much new results to practitioners already used to Deep Learning (such as the ICLR audience I guess).   The curves form Figure 2 are more original but I don't agree with its creation. What justifies that what is observed with such a weird networks (a single hidden unit) can transfer to more generic feed-forward NNs? Besides, the conclusions regarding the 'plateaus' are not really obvious: it seems that there are plateaus for all settings. Is is expected that the curves with tanh appear to be symetric w.r.t. the origin?  Other comments: - I don't see what means 'Revisiting Neural Networks' in the title of the paper. - In Equation (2), what is the definition of $y_l$? - Legend of Fig 1(b): CE w/ ReLU -> CE W/ tanh - ReLU is used from the beginning of Section 2 but defined in Section 2.3. - How is defined Delta for ADAGRAD? - Couldn't it be interesting to try to learn thresholds using the class probabilities outputted by the network? - The metrics used in experiments could be described. - Discussion on the training efficiency of linear SVMs in Section 6.2 seems irrelevant here.The paper describes a series of experiments for multi-labeled text classification using neural networks. The classification model is a simple one hidden layer NN integrating rectifier units and dropout.  A comparison of this model with baselines (Binary relevance and a ranking NN) is performed on 6 multi-labeled datasets with different characteristics. This is an experimental paper. The NN model is a classical MLP and the only new algorithmic contribution concerns the prediction of decision thresholds for binary decisions. On the other hand, the experimental comparison is extensive and allows the authors to examine the benefits of recent improvements for NNs on the task of text classification. The datasets characteristics are representative of different text classification problems (dataset and vocabulary size, label cardinality). Different loss functions are also used for the evaluation. The paper could then be useful for popularizing NNs for text classification. I am not sure that the BP-MLL is a reference algorithm for learning to rank, and there are probably better candidates. On the other hand it is true that the simple binary framework (BR in the paper) is a strong baseline despite its simplicity, so that the results could be considered as significant. An extension of this work could be to consider large scale problems (both in the vocabulary size and in the number of categories, since several benchmarks are now available with a very large number of classes – see for example the LSHTC challenges). A deeper discussion on the respective complexity of the different approaches and of their behavior when the number of training examples varies (learning curves) would strengthen the paper.",1,5111
"This interesting submission links prior work on KTA with the question of representation learning.  I then provides learning theory to underpin this idea and suggests a nonlinear feature selection algorithm.  Finally, it shows its success for an ICML 2013 benchmark, where the method finished with a bronze medal. In addition it shows interesting/impressive results on bioinformatics, namely cleavage site prediction. This is an interesting paper which should be accepted.   Some suggestions/criticism that may help improving the ms:  1. it would be nice to briefly discuss runtimes  2. so far the LT results seem like a paper within a paper, it would be nice to interweave this better, and maybe show a/the toy simulation what the derived bounds mean on concrete data.  3. The algorithm seems slightly ad hoc, where do the 12.5% come from  4. It would be nice to leave more room to the applications, already one seems sufficiently convincing  5. Finally, I strongly suggest to further tone down that statement that the present approach is better than ref 14. I do not see this. For this statement extensive testing with p-values and a thorough understanding of the whys would be required.I fully confess in advance that I am not a very good reviewer for this paper. I have no real research experience with kernel learning or feature selection. As such, I’ll be reporting my scores with low confidence. I haven’t made any attempt to evaluate the sensibility or novelty of the proposed method itself or the related proofs. If none of the other reviewers is able to provide a more confident review I can take some time to study the literature and improve my review.  The main thing I do feel able to review somewhat confidently is the empirical results. It’s nice that the authors were able to get good accuracy on the Black Box Learning challenge. It looks like that was a fairly competitive challenge, with over 200 competitors, and they took 3rd place. It’s also nice that they were able to improve over SignalP’s state of the art result on the cleavage site prediction task.  One thing I’m a bit concerned about is whether the proposed method improves over the state of the art for feature selection methods. As the authors state, it is somewhat difficult to compare performance on the cleavage site prediction task. Beyond these issues, it’s also not clear to me that randSel improves over state of the art feature selection methods. It looks like it does improve over RFE, but it would be nice if there was a baseline run by other authors. It’s not clear to me that RFE is a state of the art method to beat though. It would be nice to have more explanation of the significance of beating SignalP.  Detailed comments:  Title: I consider 'Representation Learning' to be a superset of 'Learning Non-Linear Feature Maps'. So it doesn't make much sense for the former to be an application of the other. Also both are pretty generic. Pretty much any feature learning method except PCA learns non-linear feature maps. Seems like you really could come up with a more informative title. I'd argue your title should have the phrase 'Feature Selection' in it. And probably not 'Feature Learning.'  Abstract:  missing a space before the paren (this actually happens throughout the paper, not just in the abstract)  Fig 1: You might want to put all the different rows on the same scale so that it’s possible to visually compare the height of the bars across rows.   Page 4: “So far have”: there is a word missing here  Page 6: “The original data where projected” -> “were projected” 	“The organizers did not reveal the source of the dataset”: this sounds like the organizers did not reveal the source of the data was SVHN. You might want to specify that you didn’t know it was created by multiplication by a random projection matrix either. 	“where provided” -> “were provided” 	“ranking third in both cases”:  		Here’s the public leaderboard: http://www.kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge/leaderboard/public 		According to this, Bing Xu is 3rd. But according to the report put out by the organizers ( http://arxiv.org/pdf/1307.0414.pdf ) you guys actually were 3rd and it sounds like Bing Xu was ranked worse than 3rd originally. Did you delete your kaggle account and get taken off the leaderboard or something?   Supplementary Material T: The box around 'OVER-COMPLETE LEARNED REPRESENTATION' is too tight.The paper proposes a new and fast approach for selecting features using centered kernel target alignment.  As far as I can tell, there seems to be two contributions: 1. use kernel alignment as a way of selecting features 2. random subsample data so that the method can be made scalable. However, I think the algorithmic innovation seems to be thin.  In the description of the algorithm (section 4), the paper keeps referring to bootstrap (size). I think that is a misnomer --- the algorithm listing (Algorithm 1) changes to subsampling, which I think what the paper is using.  If the paper is indeed using boostrap (ie, sampling with replacement), the computational complexity would not be reduced as the # of samples (including repeated ones) will be unchanged.    I do not follow the reasoning in Theorem 3.6 to come up with the 'bottom 12.5% ' features need to be thrown away.  The paper does not seem to explain what are the methods 'deep with XYZ' in Table 1.   Overall, I think the writing of this paper could use some polishing.",1,5112
"This paper explores a computationally efficient way to learn an intermediate representation for action recognition. The technique is somewhat inspired by the approach of ‘action bank’ but focuses on a much more computationally efficient way to achieve a similar effect.  The technique takes advantages of ‘integral videos’ and has the flavor of a modern day Viola & Jones type of technique for recognizing activities in a way similar to the classic technique for face detection. I really like the fact that the authors have taken the issue of computational complexity seriously here. While some might argue that once the community has found techniques that work very well, one could focus on optimizing them for faster run-time performance. However, some choices imply complexity differences that would be very difficult to address in practical working systems and this paper starts by using a technique as a building block that is pretty close to practical right now.  The work here is also capable of learning intermediate representations with a particularly small amount of data, i.e. single exeamples of classes of interest and lots of negative examples, due to the use of Malisiewicz et al,’s exemplar-SVM technique.  This could have advantages in a number of practical situations.  I think this paper is both fairly well presented and executed in terms of the experiment work.  Importantly, it addresses some key practical issues that deserve more attention. The results are not absolutely at the state of the art, but the value added by this work is quite good, especially for those working in the area of action recognition where data processing considerations are particularly challenging.The paper describes a mid-level representation for videos that can be faster than existing representations and yields similar performance. The idea is to train many SVMs to detect predefined action instances on sub-blocks from the video, and then to aggregate the SVM responses into a representation for the whole video.  This work seems like a fairly straightforward extension of previous similar work that was done on images (Malisiewicz et al.), but there are some technical differences like the use of an integral video trick to compute SVM responses fast, which seems nice.  I don't really understand the mining of negative examples for training the exemplar SVMs. Why is it not possible to train the SVM, say with stochastic gradient descent, on all or many negative examples?  The method relies on extra, intermediate labels for training which are not used by bag-of-words. This makes it hard to judge the performance differences between the two and seems to be unfair towards bag-of-words.   3.3: Rather than learning to scale svm confidences within a sigmoid, why not train a regularized logistic regression classifier in the first place, instead of the svm?  The feature extraction time is reported as the total on the whole UT-I dataset. It would be much better to report it in frames per second to make it comparable with other datasets without having to dig up the description of this dataset. If I am not mistaken it amounts to approximately 5 frames (with more or less standard resolution) per second? If correct, this means that despite the improvement over action bank, the bottleneck is really feature exctraction not classification. So the speed up due to the linear classifier will be swamped by the feature extraction and is not really that relevant, unless I'm missing something.   pro: - Well-written, uses some nice engineering tricks like the integral video for computing SVM responses.   neg: - The paper seems like a slightly strange fit for this conference as it describes a vision system rather than investigating the learning of representations. That intermediate labels are useful on this data is well-known (and unsurprising). The paper does propose a faster way to use them, which is probably worthwhile.This paper proposes a novel method for human activity recognition in video. The main properties it seeks are: 1) developing invariance to the various types of intra-class variation; 2) efficiency in training - both in terms of complexity and minimizing human involvement, 3) efficiency at test time (e.g. able to handle YouTube-scale collections). The main idea is to train, from typical low-level bag-of-visual-words features, a series of exemplar-SVMs which learn on a single positive example and many negative examples, and use the collective output of the SVMs at multiple scales and positions as a mid-level descriptor. The final stage of the method is a linear SVM classifier. The method performs comparably, or better than other recent methods on three modern datasets: HMDB51, Hollywood2, UCF50/101, moreover it is up to two orders of magnitude more efficient than the other methods.  The main contribution is a simple yet effective method that can perform activity recognition on large video databases. It would be easy to re-implement, though I hope the authors release a baseline implementation. As the idea of developing invariance to intra-class variability is a key consideration in the motivation of the work, I'd like to see more discussion on this. To me, the fact that the SVMs are trained on single exemplars is an interesting feature and certainly important when human labeling is expensive. However, this seems at odds with learning invariance. For example, if we only ever see one example of someone performing a tennis swing, will this not make it much more difficult to recognize the tennis swing under different conditions: clothing, body type, scale, etc.?  Positives * Paper is well written and the description of the method is clear. In particular, the presentation of Algorithm 1 and its associated description in the text is good. * As stated earlier, it's a simple and effective method, and I see others using this as a baseline * Datasets considered are modern and challenging * Speedup over other methods is impressive: key to this is the use of the integral video, a nice idea  Negatives * Paper is very specific to an application and type of dataset, may not be as of wide of interest as some other papers * Related to above, the representation extracted likely would not be useful to other AI problems (e.g. those with structured output) * Method still requires first stage of engineered feature extraction pipeline  Overall, while it may not have the broad appeal across ICLR, I think this is a good paper which clearly presents an effective method for activity recognition.  Questions & Comments ==================== Emphasize at the beginning that 'exemplars' do not necessary have to be 1 per activity type. This was the impression I got at the beginning of the paper, but the description of the experiments (e.g. 188 exemplars and 50 actions with multiple exemplars per action) made this clear.   As mentioned, there is still some human effort in finding the 'negative' examples. For example, one needs to watch a video to determine for sure that it does not contain any tennis. Perhaps you could use tags or some other means to propose negative videos that you are very confident don't contain a particular activity. In that case, how sensitive would the exemplar-SVM be to label noise (if the negative videos contained volumes semantically very similar to that of the exemplar)?  The final dimensionality of the EXMOVE descriptor is 41,172 which is going to benefit the linear SVM. Is the dimensionality of the other methods comparable? You may want to indicate this somewhere, say in Table 1.  It would be nice to see Table 1 contain more low-level feature/mid-level descriptor pairs (e.g. the outer product of low-level features and mid-level descriptors).   With respect to the comment 'we were unable to test [Action Bank] on the large-scale datasets of Hollywood-2 and UCF101. For Hollywood2, I can understand that it's the case. But would UCF101 not just be roughly 2x the time to train on UCF50? Can you clarify this? I realize it may have just not been a situation of not enough time before the deadline to run the experiment, so could this be reported in the final version of the paper?",0,5113
"The paper explores a variety of ways of making RNNs 'deep'. Some of these techniques, such as stacking multiple RNN hidden layers on top of each other, and adding additional feedforward layers between the input and hidden layers, or between hidden and output, have been explored in the literature before. Others, such as adding extra layers between the hidden to hidden transitions, are more unusual. In its simplest form this would be more or less equivalent to inserting a 'null input' at every second step in the input sequences. But as the authors point out, this would exacerbate vanishing gradients, so they add skip connections too.  As with all neural networks, the space of RNN topologies is vast. This paper doesn't cover a huge amount of new territory, but it does do a good job of exploring a few promising variants.   The experimental results don't suggest a massive benefit from deeper architectures, or a consistent advantage of one over the other, but they do show that adding depth can make a difference. Interestingly, the effect of using dropout, maxout and other generic neural network tricks seems to be much greater than the effect of changing the architecture.  The results on the Penn Treebank data are very good. But it isn't entirely clear why they're so good. In particular, the standard shallow RNN with a small hidden layer appears to perform much better than similar networks elsewhere in the literature, e.g. by Tomas Mikolov. Is this a consequence of the training method, the regularisation, or something else? A few more experimental details would help to clarify this.  Small comments:  abstract: - the paper doesn't really propose 'a novel way' to extend RNNs so much as explore several possible ways. - 'modelling variable-length sequences' is a bit too broad to be a 'task'... maybe 'evaluated on two sequence modelling tasks'? - The 'In Sec. 2' paragraph is more confusing than helpful - Eq. (3): do the denominators (1/N and 1/T_n) really belong in the loss? I'd say not... otherwise training would be biased towards shorter sequences and smaller datasets. The dataset factor can be incorporated into the learning rate, but I don't think you want to normalise by sequence length. - I found the repetition of the h_t and y_t terms in eqs (4) and (5) confusing - How many hidden layers did the 'sRNN' have? I couldn't find this. - 'Practical Variational Inference for Neural Networks' would be a better reference for adaptive/fixed weight noise.Solid paper describing a variety of 'deep' RNN architectures constructed by adding additional layers between input and hidden, hidden and hidden and hidden and output. The authors test their models on two types of data (polyphonic music prediction and word and character-based language modeling) and obtain good results (at least on the LM task). While the arguments they make seem compelling, we feel that the experimental results are not very convincing. In particular, on the music task there are a few problems: - There is no best architecture so there's not much one can learn except to try them all which is not very informative.  - To compete with an RNN with fast dropout, the authors add a lot of bells and whistles to their model (L_p units, maxout units, dropout). What happens if you train a standard RNN with the same bells and whistles ? Conversely, which of the 3 techniques made the deep models better than the RNN? A breakdown of the improvements would be helpful. - The authors describe a deep input to hidden function model (3.2.1) however no experimental evidence of its utility is shown. - Adding columns with the number of parameters to Table 1 would help the reader compare the sizes of the different models. - What happens if you train a DO-RNN?This paper proposes three techniques to make recurrent neural networks (RNN) 'deeper'. In addition to an inherent deepness over time, the authors propose to consider additional hidden layers for all the transition matrices, i.e. input-to-hidden, hidden-to-hidden and hidden-to-output.  To implement those ideas, neural operators are introduced:  - predicting the next hidden state in function of the previous one and the input  - predicting the output given the current hidden state  those operators are implemented with multi-layer neural networks, in principle of any arbitrary depth.  However, only one-hidden layer operator networks are considered.  I think that it would be really interesting to consider even deeper operators. Do we achieve additional improvements ?  Is it possible to train these networks with back-prop (through time) ?  In my opinion, you do not consider deep input-to-hidden RNN. In Figure 2b you propagate h_t-1 and x_t through the SAME hidden layer to h_t. I propose to use separate hidden layers for these tasks, i.e. one deep operator to map the input to the hidden state, and an INDEPENDENT deep operator to map from one hidden to the next hidden state. The input-to-hidden operator could be quite deep since you won't have the problem of vanishing gradients over time.  You write that you initialize the sRNN and DOTS-RNN with the weights of a normal RNN to simulate layer-wise pretraining. How do you initialize the weights of the additional hidden layers ?  Is this really comparable to layer-wise pre-training of deep MLPs ? Layer-wise pre-training is unsupervised while you need targets. Maybe we should say that you initialize the weights so that the network is easier to train ? Can you elaborate how important this initialization is ? Do the deep RNN still converge when starting from random weights ?  Language modeling:   - please add the results of Graves and Mikolov to Table 3  - please explain 'dynamic evaluation'  In general, it would be helpful that the authors provide more details on the architectures which do NOT work very well.  This would allow the reader to assess the importance of the different tricks, e.g. short cut connection, initialization scheme, etc.  By these means, one could more easily reproduce the experiments and apply the same ideas to other tasks.  The authors should discuss the relation to other works in more detail, in particular Pinheiro and Collobert (2013)  Overall, this paper presents interesting ideas and opens directions for further research.",0,5114
"This paper studies the representation power of deep nets with piecewise linear activation functions. The main idea is to show that deep net with the same number units can represent (in terms of generating linear regions) more complex mappings than the shallow model.  This is an interesting paper: it leverages known results in arranging hyperplanes in the space and then cleverly show how those can be used to show how linear regions can be learnt by multiple layers.   While the theoretical results seem right, I could not help but wondering whether the comparison is 'fair':  using the same number of units does not necessarily imply the deep net is 'restricted' in any way -- in fact, the deep net has more parameters than the shallow model. Is not that enough to argue (at least qualitatively) that deep net must have more representation power than the shallow model? (Of course, the value of the analysis is to show more precisely how many regions there are.)  Additionally, the learning process does not necessarily mean that the deep net indeed constructs that many regions --- thus, purely comparing the number of regions is unlikely to explain how well the model is able to generalize well than shallow model or how the model is prevented from overfitting.  Nonetheless, the paper presents a novel direction to pursuit to instigate further research.This is a very interesting and relevant paper, attempting to prove that a deep neural net composed of rectified linear units and a linear output layer is potentially significantly more powerful than a single layer net with the same number of units.  A strength of the paper is its constructive approach, building up an understanding of the expressiveness of a model, in terms of the number of regions it represents. It is also notable that the authors pull  in techniques from computational geometry for this construction.  But there are several problems with the paper.  The writing is unclear, and overall the paper feels like a preliminary draft, not ready for prime  time.  The introduction can be tightened up.  A more significant comment concerns the important attempt to give some intuition, at the top of page 3.  The paragraph starting with 'Specifically' doesn't make sense to me. How can all but one hidden unit be 0 for different intervals along the real line?  Each hidden unit will be active above (or below) its threshold value, so many positive ones will be active at the same time.  We could compose two hidden units to construct one that is only active within an interval in R, but I don't see how to do this in one layer.  I must be missing something simple here.   I think the basic intuition is that a higher-level unit can act as an OR of several lower level regions, and gain expressivity by repeating its operation on these regions, is the idea.  But the construction is not clear.  Also, one would expect that the ability to represent AND operations would also lead to significant expressivity gains.  Is this also part of the construction?  These basic issues should be clarified.  In addition, it would be very helpful to have some concrete example of a function that can be computed by a deep net of the sort analyzed here, which cannot be computed by a shallow net of the same size.   As it stands the characterization of the difference between the deep  and equivalent one-layer network is too abstract to be very compelling.  I also found the proof of Theorem 8 very hard to understand.  This is not a key problem, as the authors do a good job building up to this main theorem, in sections 3 and 4.  But it does mean that I am not confident that the proof is correct.  Finally, I would recommend exploring the relationship between the ideas in this paper and the extensive work in circuit complexity that deals  with multi-level circuits, for example the paper by Hajnal et al on 'Threshold circuits of bounded depth'.The authors of this paper analyse feed-forward networks of linear rectifier units (RELUs) in terms of the number of regions in which they act linearly.  They give an upper bound on the number of regions for networks with a single hidden layer based on known results in geometry, and then show how deeper networks can have a much larger number of regions by constructing examples.   The constructions form the main novel technical contribution, and they seem non-trivial and interesting.  Overall I think this is a good and interesting paper.  It is well written with the notable exception of the proof of theorem 8 and the latter half of the introduction.  In most spots, the math is precise and accessible (to me anyway), the results nicely broken into lemmas, and the diagrams are very useful for providing intuition.  These results can be interpreted as separating networks with a single hidden layer from deep networks in terms of the types of functions they can efficiently compute.  However, number of linear regions is a pretty abstract notion, and it isn't obvious what these results can say about the expressibility by neural nets of functions that we can actually write down.  Do you know of any natural examples of functions that require a finite but super-exponential number of regions?    Unfortunately, region counting can't say anything about the representability of functions defined on such input spaces of the form S^n0 where S is a finite set, since there are only |S|^n0 input values, and |S|^n0 << n^n0 = region upper bound.   About Theorem 8:   After hours trying to understand the proof of Theorem 8 I gave up.  However, I was able to use Prop 7, and intuition provided from the diagrams, to prove a slightly different version of Thm 8 myself, and so I think the result is correct, and the proof is probably trying to describe basically the same thing I came up with (except my proof went from the top layer down, instead of the bottom layer up).  So while I don't doubt the correctness of the statement of Thm 8, but the write-up of the proof of Thm 8 needs to be completely redone to be understandable and intuitive.  I don't think you need to make it 100% formal (Prop 7 isn't completely formal either, but it's fine as is), but you need to make it possible to understand with a reasonable amount of effort.     Detailed comments: ---  Title: Please pick a different title.  These are feed-forward networks so calling the regions 'inference regions' doesn't make sense.  Abs: Why is it 'computational geometry' and not just 'geometry'?  What is specifically computational about arrangements of hyperplanes?  Page 2: Missing from the review of previous results about the power networks is all of the work done on threshold units (see the papers of Wolfgang Maass for example, or the seminal of Hajnal et al. proving lower bounds for shallow threshold networks).  Unlike the single paper by Hastad et al. cited, none of these require the weights to be non-negative.  Moreover, these results are hardly non-realistic, as neural networks with sigmoids can easily simulate thresholds, and under certain assumptions the reverse simulation can be done approximately and reasonably efficiently too.  Also missing from this review is recent work of Montufar et al. and Martens et al. analysing the expressive power of generative models (RBMs).  Beginning of page 3: I have a hard time following this high-level discussion.  I think this doesn't belong in the introduction, as it is too long and convoluted.  Instead, I think you should include such discussion as intuition about your formal constructions *as you give them*.  The way it is written right now, the discussion tries to be intuitive, precise, and comprehensive, and it doesn't really succeed at being any of these.   Page 3: You should formally define what you mean by 'hyperplane' and 'arrangement'.   In particular, a hyperplane is the set of points defined by the equation, not the equation itself.  And if an arrangement is taken to be a set of hyperplanes (as per the usual definition), then the statement in Prop 6 isn't formal (although its meaning is still obvious).  In particular, how does a ball S 'intersect' with a set of hyperplanes?  Do you mean that it intersects with the union of the hyperplanes in the arrangement?  I know these are nit-picky points, but if you should try to be technically precise.  Page 5: There is a missing reference here: 'Zaslavsky's theorem (?, Theorem A)'  Page 5: You should explain the concept of general position precisely.  I don't know what 'generic weights' is supposed to mean, the actual definition has to do with lack of colinearity.  You might want to point out that any choice of hyperplanes can be infinitesimally perturbed so that they end up in general position.  Page 6: 'Relative position' is never formally defined, and it's not immediately obvious what it means.  Page 6: The explanation after the statement of Prop 6 is much clearer.  Perhaps you should just prove this (stronger) statement directly, and not the fairly opaque and abstract statement made in Prop 6.  Figure 3: Why are there 2 dotted circles instead of just 1?  Page 7: 'arrangements 2-dimensional essentialization'?  Page 7: '{0,...,n}, a < b'  ->  '{0,...,n} s.t. a < b'  Page 7: What do you mean by 'independent groups of n0 units within the n units of a layer'?  How can a unit be 'within' a unit?  Page 7: What do you mean by an 'enumeration' of a 2-dimensional arrangement?     Page 9:  Below the statement of prop 7, it was suggested that the construction would be top down, where each group in a lower layer 'duplicates' the number of regions constructed by the layer above.  But the construction given here seems to proceed bottom up... at least in the structure of the proof.  This makes it less intuitive.  Page 9:  The proof starts out very confusingly.  I wasn't able to follow the sentence 'Then we find...'.  These 'groups' haven't been formally defined at this stage, only informally alluded to.    And I have another question:  how is Prop 7 actually used here?  Merely to establish the existence of network where there is n/n0 regions that each only turn on for different groups Ii?  Isn't it trivial to construct such a thing?  i.e. take the input weights to the units in a given group to be all the same, and make sure the square n0xn0 matrix formed by taking the weight vector for each group is full-rank (e.g. the identity matrix)?   I suspect the reason this doesn't work is that the dimensions would collapse to 1 since each unit in a group would behave identically.  However, one could then perturb the final expanded weight matrix to get a general position matrix, so that the subspace associated with each group wouldn't collapse to dimension 1.  Is there anything wrong with this?     Page 9: Don't say 'decompose' here.  'Decompose' implies you already have weights and are decomposing them into factors.  Instead, what you are doing is defining some weights to be a product of particular matrices.  You should emphasize that it is a common V shared by all the i.  This is easy to miss.   Page 9: What does it mean for a linear map to be 'with Ii-coordinates equal to...'?  The 'Ii-coordinates' are the inputs to this map?  The outputs?  Page 9: What is R_i^(1)?  It is never defined.  Is it different from the version without the superscript?  Is it defined implicitly the first time it is used?  If so, what is a 'region of activation values'?  What is very confusing is that you use x to define this function, when I had the impression that x was for the original inputs only.  This isn't consistent with the h notation you use in figure 1 for example.   Page 9: You say that something 'passed through' rho^2 is the 'input' to the second layer.  This is the input to the rectifier units without their weights (which I'm guessing are contained in rho^2)?  Or is it these units with the 'V' factor part of their weights only, but not the 'U' part?  This is all extremely confusing.  Without careful definitions of your notation it requires a lot more work on the part of the reader to understand what is going on.   Page 9: How can rho_i^(2)(R_i^(1)) be a 'subset' of an subspace that lives in R^(n1)?  rho_i^(2)(R_i^(1)) is going to be a set of vectors living in R^(n0)!   Page 10:  What do you mean when you say that 'this arrangement is repeated once in each region'?  What does it mean for an arrangement to be 'repeated in a region'?   I feel like the proof becomes mostly a proof-by-diagram as this point.  Maybe you should have started off with this kind of diagram and the intuition of 'duplicating regions', explaining how composing piece-wise linear functions can achieve this kind of thing (which is really the critical point that gets glossed over), and then proceeding to show that you could formally construct each 'piece' required to do this.   And you should have done the construction starting at the top layer going down.  Having reconstructed the proof in a way that I actually understood it, it seemed that one could also proof that one can (prod_{i=1}^{k-1} n_i)/2^{k-1} * sum_i=0^2 (n_k choose i) regions, which in some cases might be a lot larger than the expression you arrived at.   Unlike your Thm 8 does, this version would actually need to use the fact the constructions are 2-dimensional in Prop 7.  Page 11:  The asymptotic analysis is just very routine and uninteresting computations and should be in the appendix.  It breaks the flow of your paper.  I would much prefer to see more detailed commentary about the implications of Thm 8.",0,5115
"The authors use a Gaussian-binary deep Boltzmann machine (GDBM) to model aspects of visual cortex. Training of the GDBM involves the centering trick of [8]. The comparison to visual cortex is in terms of learned receptive field properties, and by reproducing experimentally observed properties of activity in V1 as reported by [1]. In particular, these findings relate spontaneous activity in the absence of external stimulation to evoked activity.  On the positive side, I think the issue of the nature of spontaneous activity is interesting, and the authors put effort into reproducing the experimental findings of [1]. On the negative side, the significance of the main contributions seems lacking to me, and the authors need to motivate better why their work is important or relevant. Quality and clarity need to be improved in several points as well.   Details:  To expand on the above, I'll discuss three main points: 1) The centered GDBM. 2) Reproducing aspects of visual cortex. 3) The connection to homeostasis.  1) I wouldn't see this part as a major contribution of the paper. The centering trick was originally applied to a fully binary DBM. Applying the same trick to the GDBM (which only differs in having a Gaussian visible layer) seems a very natural thing to do. Moreover, there is no further analysis on the efficacy of the centering trick. The authors say that centering makes the GDBM easy to train compared to [15], but they don't actually evaluate the performance of the GDBM (other than comparing it to biological phenomenology), and only apply it to image patches. In [15], the GDBM was trained on images of faces and evaluated in terms of filling in missing parts of the image. Hence, it is not clear whether centering makes the more complicated training of [15] obsolete, as suggested by the authors.  Also, when it comes to clarity: given that the authors emphasize the importance of centering, they need to better explain what it is, why it works, and how the centering parameters are computed (the latter is only described in the algorithm float). These things are unclear to the reader unless they look at the reference.   2) Reproducing aspects of visual cortex is the main contribution of the paper. First, the learned receptive fields are suggested to qualitatively resemble those in V1 and V2. Learning V1-like Gabor filters is of course a quite common result nowadays. I don't think it's possible to conclude that the model captures receptive field properties V2 well, just from looking at Figure 1b.  Hence, the main contribution here is the analysis of activity. I think the results are fine and somewhat interesting, though not surprising. What is missing is better motivating/explaining why these results are interesting/relevant. Sure, the GDBM reproduces certain experimental findings. But have we learned something new about the brain? Is the GDBM particularly well suited as a general model of visual cortex? Does the model make predictions? What about alternative, perhaps simpler models that could have been used instead? Etc.  Also, are there related models or theoretical approaches to spontaneous activity? For example, this comes to mind:  Berkes, P., Orbán, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science (New York, N.Y.), 331(6013), 83–7. doi:10.1126/science.1195870  As for clarity: when collecting the spontaneous frames, presumably you are sampling from the full model distribution P(x,y,z) via Gibbs-sampling, with all layers unclamped, correct? Because writing that samples were collected from P(y|x,z) seems to suggest that you clamp to a specific x and z and collect multiple samples from the same conditional (not just as a step during Gibbs sampling).    3) Lastly, the connection between homeostasis and the author's model is unclear. The authors mention homeostasis in the abstract, introduction and discussion, but do not explain homeostasis or how their results relate to it specifically. This needs to be explained better, especially to this audience. Personally, I know the literature somewhat (e.g. [4]), but am nevertheless unclear about what exactly the authors intend to say.  Sentences such as 'we are able to make the model learn the homeostasis from natural image patches' are unclear.  When I first read the paper, I thought the authors were referring to the centering trick as something that can be understood as a homeostatic mechanism (i.e., a neuronal mechanism that maintains a certain average firing rate, see [4], [6,7]), which would make sense to me. However, on further reading it seems to me that the authors refer to the fact that spontaneous activity resembles evoked activity as an aspect of homeostasis? Why? For comparison, [6,7] clamped the input to empty images (to simulate blindness), and had an active homeostatic mechanism at play that led to spontaneous activity resembling evoked activity. In the authors' paper, spontaneous activity resembles evoked activity simply because the former is taken to be sampled from the unconditioned model distribution..? I'm not sure where homeostasis, i.e. being subject to an active self-regularity mechanism, comes into play at this point.  (As an aside, I don't think the Friston reference [3] clears up what the authors' notion of homeostasis is, in particular as Friston talks about states of agents in their environments. Frankly, Friston's theoretical claims are often unclear to me, to say the least, in particular when it comes to mixing together internal models in the brain, and methods that should apply to the latter such as variational inference, and external probabilistic descriptions of agents and environments. Either way, if the authors would like to make a connection to Friston's theories in particular, then that connection should be explained better. Generative/predictive brain models and homeostatic mechanisms per se are not exclusive to Friston's theory.)   Further comments:  * Abstract, 'Spontaneous cortical activity [...] are' -> is  * 2.1 first para, 'consisted' -> consisting  * Not sure why x,y,z are sometimes capitalized, sometimes not.  * 3.1 'sparse DBNs show worse match to the biological findings as reported in [1]. A quantitative comparison between centered GDBMs and sparse DBNs is still open for future studies.' Where was it shown to be a worse match then?  * Figure 2: shouldn't the angles (figure titles) go from 0 to 180?   In conclusion, I think this work could potentially be interesting, but in its current form quality and clarity are somewhat lacking and significance is not quite clear.In this paper the authors train a DBM model on natural image patches, conduct a pseudo-physiology experiment on the model, and show that the spontaneous correlation structure of the model resembles that in a V1 experiment.  The claimed results of the paper are: + Receptive fields that resemble V1 and V2 (only a qualitative statement is made) + Correlation structure that resembles spontaneous activity  My biggest concern relates to the question, 'Why are any of these results unexpected?'  The V1 and V2 receptive field results have been previously reported (ad nauseam for V1), and are not the focus of the paper, so I will ignore these.  The correlation structure also appears to be a result entirely expected by the model construction.  Isn't it the case that the DBM should be capturing statistical relationships in natural images and that co-linearity (and feature non-orthogonality, or high inner product) is a well known property of natural scene statistics?  Taking the results at face value, why is this model unique to the finding, or what does this teach me about the brain, or about the model?  Another concern, is that the main result in Figure 3 is not very convincing.  Is there any statistical test to show the difference between the distributions in 3a?  Why is the uncorrelated noise distribution the proper control?  For Figure 3b, the max (solid blue line) looks pretty close to the relative occurrence line (solid red line), while the neural data seems to have larger divergence in these two measures.  What is the important characteristic of this curve?  Is there anything surprising about it?  It peaks at 0 and 90, just like the neural data, but what makes this model unique to capturing this phenomena?  Some of the most interesting statements in the paper are given in the footnotes or stated as ongoing work.  For example, I find it most interesting that certain models show the observed behavior, while others do not (footnote 6).  In fact, I would have preferred to see the entire paper revolve around this finding.  Far too many papers of this type propose one and only one model to account for the data.  The advancement of this type of work requires the comparison of models and the invalidation of models.  Also, your title seems to indicate that a 'centered Gaussian-binary DBM' is necessary to fit the findings.  I am very confused about your use of the term 'homeostatic'.  You seem to refer to homeostatic as the condition where you sample from your model with the input unclamped.  However, in biology homeostasis is the regulation of variables (or machinery) such that some state is held fixed.  I thought that homeostasis in your model would be more related to learning, i.e. weight updates.  Why, in your model, isn't the spontaneous activity condition a situation where the input is black (or all zeros)?  Isn't this the 'stimulus' condition that resembles the experiment?  How should we model closing the eyes, going into a pitch black room, or severing the input from retina to LGN (or LGN to V1)?  Why did you use mean-field to approximate the response to the stimuli?  This means for one condition you run mean-field and the other you run sampling.  It seems that this just introduces discrepancy between the conditions.  A note on exposition, I would prefer less detail on already published models and training algorithms, e.g. section 2.1 and Algorithm 1. and more detail on the experiment that you ran (sampling procedure, and explaining better the background of Figure 3b).This paper revisits on a simulated model learned from data, previous work by Kenet et al. 2003 in the journal Nature, on the statistics of spontaneous neural activities in the visual cortex, in particular, their correlations to orientation maps. The authors first learn to model image patches in an unsupervised fashion. In a second step, the spontaneous hidden activities produced by the learned model are recorded and compared to the ones observed by Kenet et al. 2003.  The authors build on the centered deep Boltzmann machines, adapting them to real-valued Gaussian-like inputs. An important aspect of the solution learned by a DBM is the amount of sparsity. Authors do not use an explicit sparse penalty, but biases are shown in Algorithm 1 to be initialized to -4, suggesting that sparsity is still important.  Activation patterns produced by the proposed model are shown to be similar to measurements by Kenet et al. 2003 on real neural activity. The use of an unconstrained Gibbs sampler to compute spontaneous activities is questionable, as to my understanding, input should be deactivated. A more powerful result would be to show similar activation patterns when the input units are explicitly constrained to be inactive. Conditional DBMs might be useful in this regard.",0,5116
"Review Summary This work empirically show the unsupervised pretraining encourage moderate-sparseness in a deep neural network, which could lead better classification performance. The author also proposed that ReLU DNNs without pretraining could also lead to moderate-sparseness.   Though empirically studying sparseness in DNNs is interesting and important, this submission could potentially be improved.   Pros -- well-written and organized -- an interesting idea for exploring the role of unsupervised pretraining in deep learning Cons -- I am not convinced that the unsupervised pretraining itself indeed encourage sparseness in higher layers of networks. As mentioned in your paper, those pretraining algorithms try to decrease the reconstruction errors under some constrains. If those models were trained longer enough, the resulting networks could try their best to exploit their capacity to reconstruct data (in other word, the networks could become less and less sparse).  -- the author did not explain how the hyperparameters (of SPM, AIM and AOM) were chosen. The different hyperparameters could lead to very different conclusions.  -- the sparseness measurement used in this paper is improper. The activations of sigmoid and tanh are unlike ReLU, at a certain scale. You could consider Hoyer’s sparseness measure (Hoyer 2004) which is on a normalized scale and avoids the hyperparameter epsilon in the SPM. -- AOM should also consider the activation overlapping between samples from any number of classes'.   Minor comments Paragraph 4, page 2: The results of your DNNs are not state of the art for those data set. Section 3: I cannot understand assumption 2. In equation 2, what $x_{ij}$ represent? In equation 3, m?Brief summary of paper:  The paper is an experimental evaluation of the sparsity levels of representations in deep networks learned through several approaches. These include 3 types of non-linearity (sigmoid, tanh and ReLU), with and without pretraining, and dropout. The main findings/conclusions are that pretraining yields a moderate level of representation sparsity, and that using their brand of purely supervised ReLU with dropout yields even sparser solutions and better classification on MNIST/CIFAR10/NORB.  Assessment: Overall I see very little originality in this paper, the conclusions are not really novel, and the paper is very unclearly written. Contrary to its title it does not bring any insight as to '*WHY*  unsupervised pretraining encourages moderate sparseness'. A thorough and clearly presented empirical study of the sparsity properties of representations learned by different algorithms could have been interesting. But unfortunately, in its current state this work is extremely unclearly written, with confusing terminology, vague reasoning, poor structure, and insufficiently described experimental setup. The classification performance obtained with 'their ReLU' appears rather good, but the paper does not investigate in depth what would explain the improvement over the other ReLU results from the literature they report. If their approach/implementation indeed has an advantage, its specificity should be explained and evaluated in detail. It is unclear from the writeup whether hyper-parameters were chosen/tuned using proper methodology.  Pros & Cons: - lack of originality - very unclear, confusing, writeupThe authors study the effect of unsupervised pre-training on the levels of sparsity that hidden nodes have. The claimed contributions are that unsupervised pre-training encourages what they call “moderate-sparseness” and that their implementation of relus + fully-connected nets achieves state of the art performance on a number benchmarks such as MNIST, CIFAR-10 and JC-NORB.  I have found the beginning of Section 3.1 very confusing to read -- it requires some rephrasing (what is 1/R?) especially since it seems relatively crucial in terms of understanding the authors’ main point. The forward-references at the end of that Section are also confusing the reader. In Section 3.2 -- where does the 0.15 threshold come from? Why is there a jump in Figure 1? What happened at epoch 25?  It would be best if the authors put the best comparable numbers in the tables along with their *citations*, rather than just presenting their own results or other results without references attached.  It is unclear how the authors’ implementation of relus + fully-connected nets differs from the one initially proposed by Hinton and collaborators (as well as the others cited)? Why are the authors’ results better?  I’d say the paper is strangely structured/motivated: the claim that ReLUs produce sparser activations is believable and they measure it. The authors also measure that rectified units achieve better test errors (well-known before that). But I don’t see any evidence that sparsity of rectified units causes better generalization -- rectified units have other properties (such as the absence of saturation) that could be hypothesized to make them generalize better. I would say that the authors should work on finding a mechanism for why sparsity specifically is beneficial in supervised-backprop nets (the fact that unsupervised pre-training *also* leads to sparse representations, according to their metrics, is not a mechanism in and of itself).   Generally the paper has a number of typos, awkward phrasings (e.g., “this paper does not intend to obliterate the contribution”)",0,5117
"Summary: This paper describes a simple generative model for audio spectra from a single sound source. It explains them as products of spectral bases, i.e., sums of log-spectral bases.  The paper describes a mean field variational approximation for the posterior of the activations of each basis given an observed spectrum, and a variational EM algorithm for learning the model parameters.  It applies the model to performing bandwidth extension for unknown talkers and frame-based speaker identification, outperforming reasonable baseline approaches in both.  Novelty and quality: As far as I know, this method is novel.  The background literature review correctly points to the connections with traditional methods of homomorphic filtering and non-negative matrix factorization.  While it is not entirely clear how widely this method can be applied, especially with the high current computational cost of learning the model parameters compared to a traditional fixed basis like mel frequency cepstral coefficients (MFCCs), it does significantly outperform baseline systems on the two experimental tasks described.  Specifically, real sounds only rarely involve a single source, but, as mentioned in the paper, this model could provide a prior on each source in a multi-source mixture.  Pros:  * Interesting model  * Shows good experimental results  * Well written  Cons:  * Some mathematical details could be spelled out more explicitly  * Possibly limited applicability   Minor comments:  Page 2: 'Another attractive feature is the symmetry between the excitation signal e[n] and the impulse response h[n] of the vocal-tract filter—convolution commutes.'  Please explain why this is an attractive feature of the model. It seems to be an additional nuisance degree of freedom in the model.  Page 4: The definition of q(a_{tl}) uses a_{tl}, where everything else uses a_{lt}. Is this intentional?  Page 6: 'The periodic “excitation” filters tend to be used more rarely, ... the excitation signal generated by a speaker’s vocal folds.'  This statement seems to assume that multiple excitation bases cannot be combined into a valid excitation, while multiple vocal tract shape bases can be combined to create a valid vocal tract shape.  Can you prove or demonstrate that this is the case?  Page 6: 'We can approximate this posterior using the variational inference algorithm from section 4.1'  Can you show this more explicitly?Very interesting, well written paper describing a novel model for explaining speech using a set of filters trained in a generative fashion. The paper describes a “product of filters” model, proposes a mean-field method for posterior inference and a variational EM algorithm to estimate the model’s free parameters. The paper draws parallels with the proposed approach and homomorphic filtering methods, NMF and its variants for speech analysis.  The paper however does not have a comprehensive experimental framework to evaluate the usefulness of the proposed model. Bandwidth expansion is a natural candidate for generative models and is an example used with similar techniques in vision - for example the recently proposed sum-product networks [Poon and Domingos, 2011]. The gains using the proposed model are however not very significant.   Although the authors do state that they are not attempting to build a state-of-art speaker identification system, the experiment is nevertheless performed in a limited setting. It is hence not possible to completely evaluate the usefulness of features derived using the proposed technique. Can the features derived using the proposed framework generalize to be useful for more realistic speaker identification experiments? Do they generalize well? A natural additional question to ask would be – are these features useful in recognizing speech sounds since this is a generative model of speech? Can phoneme recognition experiments be performed using TIMIT using the derived features? Do the learnt bases represent classes of sound?",0,5118
"Recently, (Bengio et al. 2013b,2013c) proposed a novel way to estimate a data distribution indirectly, by training the transition operator T(x_{t}|x_{t-1}) of a Markov chain instead of trying to estimate the data distribution directly. These Generative Stochastic Networks (GSN) have several theoretical benefits, mainly avoiding the costly process of sampling in a highly multi-modal distribution (either when trying to sample from the model to estimate the so called 'negative gradient update', or when trying to perform MAP inference) as is traditionally required when training highly multi-modal models.  This paper proposes using a Neural Auto-Regressive Density Estimator (NADE) to learn a *multimodal* transitition operator, thus hopefully improving results compared to the above works in which the transition operator is unimodal.  The authors make a very good review of recent advances related to GSNs and argue that using a multi-modal transition operator could allow for better mixing of the markov chain and deal more gracefully with huge numbers of modes.  Two sets of experiments are performed, the first trying to estimate a 'spiral' distribution in 2D (a 2D slice of the traditional Swiss roll seen in manifold learning studies), the second on the Mnist dataset of handwritten digits. These experiments compare GSNs using the proposed multi-modal NADE transition operator with GSNs based on uni-modal transition operators which were used in previous works.  The results show that GSN-NADE fare much better at representing the spiral dataset, but fall a little short of expectations on the Mnist dataset where the walkback trick proposed in (Bengio et al. 2013c) allows a GSN with a uni-modal transition operator to fare better than the proposed GSN-NADE. In other words, the results support the hypothesis that a multi-modal NADE transition operator can be helpful, but using the walkback trick to deal with spurious modes appears to be even more helpful. As a possible future work, the authors propose to try and find a way to combine both benefits since walkback is not trivially applicable to the proposed GSN-NADE.  This study presents a new approach to a problem clearly identified in an earlier study, gives a very good summary of the state-of-the-art, clearly identifies key issues to overcome, and honestly presents its own limitations. Although the results fall a little short of expectations, they do indeed point to the fact that a multi-modal transition operator can be helpful in GSNs. Regrettably the experiments do not try to assess the quality of the representations obtained with GSN-NADE in a classification setting and on more complex datasets than Mnist.The authors present work that tackles learning data distributions indirectly, by learning a neural-network based transition operator (a Generative Stochastic Neural Network, GSN), whose stationary distribution is the desired distribution. Specifically, they introduce a GSN where the transition distribution is multimodal. The proposed model is evaluated in two experiments (spiral distribution and generating MNIST) and shown to produce samples that better capture the data distribution.  I'm not enough of an expert to give an in-depth analysis of the proposed method, however, I review the paper in general terms.   Pros: addressing sampling issues is worthwhile, and I would like to learn more about the proposed approach. Cons: the initial part of the paper is way too wordy. At the same time, details and explanations regarding the model and experimental procedures are missing. There's few actual results; this should have been written as a short workshop paper.   The introduction/motivation section is very long and has some content that is not directly relevant to the paper, or detail that is unnecessary. The authors write over a page to motivate unsupervised learning in general, and to describe well-known facts (Boltzmann machines are difficult to handle; the existence of many, separated modes makes MCMC difficult). This strikes me as too general for the paper and could have been one or two short paragraphs instead. Similarly, what is the benefit of spelling out in full Theorem 1, from an earlier reference? Etc.   The writing should be made more succinct and be structured better. In Section 1, the general introduction of the topic and overview of the paper should be separated. Wording could be improved (e.g., '[...] MAP approximations could all fall on their face.'; or 'In a sense, this is something that we are directly testing in this paper.'--in a sense?).   In Section 5, I take it that the NADE factorization is now over pixels rather than frames? The notation should make this clearer (x_{} is used for both). RNADE was not defined (should be with ref [12]). A figure would have been helpful for understanding the GSN-NADE architecture (and the other models).  Results are not presented until Section 6, more than halfway through the paper. Here, the methods used are not properly explained and not enough details are given.   In the first experiment, what are the model parameters and training procedure? For the second experiment, the authors write 'The training of model uses the same procedure as in [13]', with no further explanation. Similarly: 'To evaluate the quality of trained GSNs as generative models, we adopt the Conservative Sampling-based Log-likelihood (CSL) estimator proposed in [9]' and 'The GSN-1-w is [...] trained with the walkback procedure proposed in Bengio et al. [11].' No explanation is given for any of this. The references are all very recent (2013), so it's not like the general reader can be expected to be familiar with these methods. Again, model parameters are missing ('For the details of models being compared, please refer to [9].').   In conclusion, if the authors were to expand the experimental section and better explain the methods used, and cut out unnecessary content, this could make for a workshop track paper. But I don't see enough content for a conference paper.  Further notes:  Page 3: 'Monte-Carlo Markov Chain (MCMC) methods' -> Markov chain Monte Carlo (MCMC) methods Page 4: 'Figure 2' should be Figure 1?  It looks like the paper was updated after the review period had started (24 Jan.). It would be good in such cases to notify reviewers by posting a comment here, to make sure they are reviewing the latest version.Generative Stochastic Networks have recently been introduced as a new generative model formalism. Instead of directly parameterizing the data distribution, GSNs learn the transition distribution for a Markov chain which generates samples from the learned distribution. Previous results on GSNs, namely using the denoising criterion, have used a transition distribution that is factorized or unimodal. This makes it easier to learn the GSN, but limits the expressivity of the model. This paper proposes a type of GSN which uses conditional NADE to represent multi-modal output distributions for the transition operator. Results show it performing better than the uni-modal approach on MNIST and a toy 2d dataset.  The paper is novel. The GSN is a relatively recent formalism, with essentially all work coming from the same research group. This is the first work to propose a multi-modal transition operator. It's also an interesting use of NADE and bears some similarity to the recent use of NADE as an output distribution for recurrent neural networks. It is an interesting paper: well motivated and well written.  Pros: * In my opinion, GSNs are a fascinating framework and this paper addresses one of the limitations of previous work * The paper is clear and gives a great deal of background; in summarizing the work to-date on GSNs, it's complimentary to other published work  Cons: * Experiments are limited to a toy dataset (2d - a few hundred (?) examples) and MNIST; the former is ok for visualization and the later is used to compare methods based on an approximation to test log likelihood proposed by the same group * The comparisons aren't very extensive - essentially the model without NADE and the baseline model using 'walkback' training * Although the background and review of GSNs are welcome, more than half of the paper is devoted to background. I feel that the paper makes a good contribution, but not as substantial as other ICLR papers. It could certainly be strengthened by exploring other types of output distribution models beyond NADE or experiments beyond MNIST.  Comments --------  In the experiments section, I suggest giving a pointer that discussion re: walkback not being applied to GSN-NADE will be addressed later (i.e. in next section).  Section 4 refers to a Figure 2, but I believe it should be Figure 1.  Section 6 'fatorial' -> 'factorial'  Even though the CSL method is referenced, you could give a short description of it in the experiments section.  Is R-NADE being used for the 2d dataset and regular NADE being used for MNIST? I didn't see this explicitly stated.  First sentence on page 7 reads 'On the contrary, GSN-NADE with the walkback training alleviates this issue.' This is confusing, GSN-NADE didn't use walkback training. Should it say 'GSN-NADE without the walkback training?'",0,5119
"The paper contains some interesting analysis showing how certain previous methods can be viewed as implementations of Natural Gradient Descent.  The paper is novel and is on an important topic, I believe. There are parts where I believe the paper could be clearer (see detailed comments below)- parts could be expanded upon.  I believe the ICLR length guidelines are not strict.  There are also some interesting experiments that compare NG (and its variants) to SGD.  (it would be nice to have some more details on where the time is taken in NG, and what happens when you change the size of the subset used to compute the metric, and the number of iterations used in approximate inversion of G).  Detailed comments follow.   ----------- density probability function -> probability density function  not sure how p_{	heta}(z) relates to DNNs-- what is z?  The input features?  The class-label output?  Both?  Definition of Fisher information matrix?  In Sec. 2.1 when you talk about 'the probabilistic interpreation p_{	heta}(t | x)', this is very unclear.  I figured out what you meant only after looking at the Appendix.  In fact, the notation is inherently quite unclear because for me, the canonical example of deep learning is with softmax output, and here you view t as a multinomial distribution over a single categorical variable, which would be a scalar not a vector, so the use of bold-face t is misleading, because it's more like p_{	heta}(y | x) where y is the discrete label.  I think if you want people to understand this paper it would make sense to devote a paragraph or two  here to explaining what you mean.  computed by model -> computed by the model   Around Eq. (15), where you talk about the Levenburg-Marquardt heuristic, it was very unclear to me.  I think you need to take a step back and explain what it is and how you are applying it.  Otherwise (and this is a risk with other parts of the paper too) there is a danger that the only people who can understand it are those who already understand the material so deeply that the paper's conclusions would already be obvious.  betweent -> between  Eucladian -> Euclidean  batch model -> batch mode  Riebiere -> Ribiere (twice)  in Sec. 10-- I don't think you address how many iterations you use in the solver for  approximately mutiplying by G^{-1}, and how much of the total time this takes.    -- Appendix:  be obtain -> be obtained  should sum_i^o be instead sum_{i=1}^o ?  (this appears more than once)  In the last line of Eq. 27, the notation doesn't really work, you are treating the vector y like a scalar, e.g. dividing by it.  This same notation appears in the main text. It might be necessary to put, say, diag({f p}), and explain that p_i = 1/(y_i (1-y_i)). Or maybe you can point out somewhere in the text that these equations should be interpreted elementwise. In eqs 28 and 29 you should have non-bold in y_i, and also there is an odd inconsistency of notation with eq. 27-- you drop the J [something] J^T notation which I think is still applicable here.  certain extend -> certain extent  The text 'Following the functional manifold interpretation..' with eqs. 30 and 31 in the appendix, is quite unclear.  I think you are maybe assuming a deep familiarity with Martens' Hessian-Free paper and/or the Levenburg-Marquardt algorithm.This paper explores the link between natural gradient and techniques such as Hessian-free or Krylov subspace descent. It then proposes a novel way of computing the new search direction and experimentally shows its value.  I appreciate getting additional interpretations of these learning techniques and this is a definite value of the paper. The experiments, on the other hand, are weak and bring little value.  Additional comments: - To my knowledge, Krylov subspace descent is much older than 2012. It is for instance mentioned in Numerical Optimization (Nocedal) or 'Iterative scaled trust-region learning in Krylov subspaces via Peralmutter's implicit sparse Hessian-vector multiply' (Mizutani and Demmel). As it is, the literature looks very deep learning centric.  - I appreciate the comment about using different examples for the computation of the Fisher information matrix and the gradient. However, it would be interesting to explore why using the same examples also hurts the training error. This also goes against the fact that using other examples from the training set yields a lower training error than using examples from the unlabeled set.  - Figure 1: why do you use a minibatch of size 256 in the first case and a mainibatch of size 384 in the second? This introduces an unnecessary discrepancy. - Figure 1: 'top' and 'bottom' should be 'left' and 'right'.  - Figure 2 is not very convincing as the red curve seems to drop really low, much lower than the blue one. Thus, you prove that NGD introduces less variance than MSGD, but you do not prove that the ratio of the importance of each example is different in both cases. Could you use a log scale instead?  - Finally, the experimental section is weak. The only 'deep' part, which appears prominently in the title, consists of an experiment ran on one deep model. This is a bit short to provide a convincing argument about the proposed method.  Pros: nice interpretation of natural gradient and connections with existing algorithm  Cons: Weak experimental section The paper could be better written It feels a bit too 'deep' centric to me, whether in the references or the title despite its little influence on the actual paper.This paper looks at some connections between the natural gradient and updates based on the Gauss-Newton matrix which have appeared in various recent optimization algorithms for deep networks.  Some of these connections appear to be already mapped out, but the paper presents a unified picture and fills in additional details.  There are some interesting insights about how this interpretation can allow unlabelled data to be used in truncated Newton style methods like Hessian-free optimization (HF).    Later in the paper connections between Krylov Subspace Descent (KSD) and natural conjugate gradient algorithms, and a new algorithm is proposed which tries to combine the supposed advantages of several of these approaches.    This algorithm, which is a potentially interesting twist on HF that involves taking the previous update and the new proposed update (which will be an approximation of the natural gradient) and jointly optimizing over both of these.  However, I'm not convinced these is a better thing to do than just initialize CG from using previous iterations. While the experiments did demonstrate a distinct advantage to doing it this way in the setting involving no preconditioning, I remain skeptical.    Overall I think this is a solid paper, and should be accepted.  However, I really want the authors to address my various concerns before I can strongly recommend it.   Detailed comments:  Page 2: The notation in the second paragraph of section 2 is odd.  Although I understand what you mean, I don't think it is proper to use 'R^P->(R^N->[0,inf))'.   Also, 'p_theta(z) = F(theta)' is odd.  Shouldn't it be something like 'p_theta(z) = F(theta)(z)'?  Page 2:  You should explain how a matrix-valued function of theta defines a metric on the space.  This is well known to mathematicians, but many in this audience won't be familiar with these concepts.  Page 2:  Why use row vectors instead of columns vectors?  While technically valid, this will make reading your paper require more mental effort on part of the reader.  Page 2:  'Natural gradient' is not an algorithm, it is a vector quantity.  You should say 'natural gradient descent' or something like that.   I know it is not uncommon to hear people say 'natural gradients' when implicitly referring to the algorithm, not the quantity, but I think this is a bad habit that should be avoided.  Page 2:  'moving in the direction' -> 'moving some given distance in the direction'  Page 2:  What do you mean by 'the change induced in our model is constant'?  Natural gradient descent type algorithms can use dynamically changing step sizes.  Did you mean to say, 'some given value'?  Moreover, this statement doesn't distinguish natural gradient descent from gradient descent.  Gradient descent can also be thought of as (approximately) minimizing L w.r.t. some metric of change.  It may happen to be a parametrization-dependent metric, but the statement is still valid.  Moreover, eqn. 2 isn't even what happens with algorithms that use the natural gradient.  They don't minimize L w.r.t. to some target change in KL, since this is typically intractable.  Instead, they use an approximation.  I know later on you talk about how this approximation is realized, but the point is that algorithms that use the natural gradient are not *defined* in terms of eqn. 2.  They are *defined* in terms of the iteration which can be interpreted as an approximation of this.  Page 4:  This matrix used by Le Roux and others I think is called the 'empirical Fisher', perhaps due to its connection with the empirical distribution over t.  I think it was in a paper of Schraudolph that I remember hearing this term.  Page 4: I would suggest you call it 'Hessian-free optimization'.  It sounds weird to call it just 'Hessian-free', despite the lack of O in the commonly used acronym.  Page 6:   Eqn. 16 isn't exactly the same as the one from Martens (2010).  In particular, you seem to be implicitly using update'*G*update = grad'*inv(G)C*grad in the denominator.  This is only true if update = inv(G)*grad.  But this will only be true if the conjugate gradient method used converges, and in general it won't.  Page 6: The citation here (and also likely the corresponding one in the intro) is wrong.  'Structural damping' is from: 'Learning Recurrent Neural Networks with Hessian-Free Optimization', not the paper cited.  Page 6: Could you explain what you mean by 'fixed coefficient of 1 for the regularization term'?  As far as I understand, the structural damping term can have an arbitrary coefficient in front of it in the original paper.  Page 6: 'just initializing the linear solver close to a solution'.  Actually, this is arguably more complex type of initialization than the one used in KSD.  It actually changes each basis vector of the Krylov subspace, instead of augmenting the space with just one extra direction.   Page 6: I'm a bit confused about what is going on in section 6.  In particular, what is being said that is particular to the Fisher  information matrix interpretation of G?  Any relationship of KSD to kind of non-linear CG type algorithm seems to be a completely different issue (and one which is not really explored in this section anyway).  Page 7:  You talk about using the same data to estimate the gradient and curvature/metric matrix introducing a bias.    However, this is going to be true for *any* choice of partial data for these computation.  In particular, your suggestion of taking independent samples for both will NOT make the estimate unbiased.  This is because the inverse of an unbiased estimator is not unbiased estimator of the inverse, and you need to take inverses of the matrix.  Thus this doesn't address the biasedness problem, it just gives a different biased result.  Moreover, there are intuitive and technical reasons not to use different data for these estimates.  See the article 'Training Deep and Recurrent Neural Networks with Hessian-Free Optimization', section 12.1.  Despite the potential issues for optimization, I think it is nonetheless an interesting observation that unlabelled data can be used to compute G in situations where there is a lot more of it than labelled data.  But keep in mind that, in the end, the optimization is driven by the gradient and not by G, and as long as the objective function is convex, this can't possibly help the overfitting problem.  Insofar as it seems to help for non-convex objectives, it is a bit mysterious.  It would be good to understand what is going on here, although I suspect that a theoretical account of this phenomenon would be nearly impossible to give due to the complexity of all of the pieces involved.   Page 8:  It is hard to follow what is happenning in the pseudo code.  Some questions I had were:  Where are these 'newly sampled examples' coming from?  What is the purpose of the first split into 'two large chunks'?  Are the 'heldout examples' from the segment that was 'replaced'?  By 'output of the trained model' you mean the output of the network or the objective functions scores for different cases?  Figure 2:  In figure 8, what is being plotted on the x-axis?  Is that the segment index?  Why should this quantity be monotonic in the index, which seems completely arbitrary due to the presumably random assignment of cases to segments?  Page 8:  Were you running batch NGD here?   Wouldn't comparing a batch method to a stochastic method account for the 'variance' effects?  Also, how can you be sure that you are being fair when comparing the two methods in terms of learning rates.  It's probably not fair to compare them with the same learning rate, since both methods have completely different characteristics.  But then what values can you choose?   Maybe the only fair way would be to pick values that let both methods get to some destination error in roughly the same amount of time?  Even this seems not completely fair, but might be the best compromise.   Page 8:  I'm confused about what this section is saying.  The hypothesis about the 'path that induces least variance' is never clearly articulated until after the results are presented.  Also, you should emphasize here that you are talking about variance of the final outputs.   Page 8:  Why even run these variance experiments with these different chunks/segments at all?  Why not just take multiple runs with some fixed dataset and observe the variance?  What does this cross-validation style chunking actually add to the experiment?   Page 9:  I don't know if it is correct to call NCG a 2nd-order method.  In some sense, it meets that exact definition of a first order method: the iterates are linear combinations of gradients from previous iterations.  Page 9:  What do you mean by 'real conjugate direction'?  Conjugacy, as far as I know, is only strictly defined for quadratic functions.  Perhaps there is a natural way of talking about this in a more general setting, but I don't see how.  Maybe you could achieve a kind of local conjugacy to the immediately previous update direction w.r.t. the current value of G, although is it obvious that if G were constant (e.g. the objective is a quadratic), that a new direction would be conjugate to *all* previous directions?   If this is true, it seems like it would be a well known interpretation of the conjugate gradient algorithm, and would be a much more general statement than what you showed (i.e. because you are replacing the function everywhere with its 2nd-order Taylor series with a fixed Hessian, so it might as well be quadratic).  Actually, isn't the proof of this fact obvious?  That is, one just has to look at the usual linear CG iterates and see that the new direction is a linear combination of the gradient and the old direction, and is optimally chosen from vectors within a subspace which contains the current gradient and the previous direction vector.  Page 9:  'We can show' -> 'As we will show below'  Page 9:  Typo: 'Eucladian'   Page 10:  The notation for the natural gradient in the two equations at the top is not consistent.  Page 10: Typo: 'batch model'  Page 10: How many CG steps were used per update computation?  How did you handle the selection of the damping constant lambda here?  Page 10:  If the baseline NGD is basically just HF, then it seems to be quite severely underperforming here.  In particular, an error of about 0.8 after about 10^4.5 s = 9 hours is far far too slow for that dataset given that you are using a GPU.  Even if measured in iterations, 316 is way too many.  On the other hand, the red curve is in the figures is consistent with what I've seen from the standard HF method.  Perhaps this is due to lack of preconditioning.  However, why not use this in the experiments?  This is very easy to implement, and would make your results more directly comparable to previous ones.  I suppose the fact that NCG-L is performing as well as it is without preconditioning is impressive, but something doesn't sit right with me about this.  I would be really interested to hear what happens if you incorporate preconditioning into your scheme.",0,5120
"The paper presents an architecture to learn simultaneously multiple image rankers, all of which sharing low-level layers of a deep neural network, while keeping their last layer separate for each ranking task. Since the distribution of images per query is far from uniform, this helps 'poor' queries learning good image representations from 'rich' queries. The query specific model is a 2-way softmax (good or bad image for that query). This is not exactly what I would call a ranker, but rather a classifier. In fact, I didn't see how this architecture differs from a DNN trained with logistic regression (so that each query has one output unit, which should be 1 if the image corresponds to the query, and 0 otherwise); it seems that a softmax with 2 units is similar to a single sigmoid unit, no? The paper shows two series of experiments, one on CIFAR-10, and one on MSR-Bing Image Retrieval challenge dataset, which is a real image ranking problem. Results show that the proposed approach works better than the two compared approaches (the Binary DNN consists of one DNN per query, which cannot work when the data is heavy tail; the multi-class DNN, on the other hand, seems very similar to the proposed approach, and I did not understand why this one would not scale to millions of queries, while the proposed approach would). Finally, apparently, an SVM is trained after the features are fixed. I did not understand this part neither. Why is the SVM needed anyway? The paper is not very well written, with a poor level of english, and several details missing, both in describing the algorithm and in the experimental section.Pros:  •	The use of multi-task DNNs for clickthrough data makes sense and provides good results   Cons:  •	The idea of multi-task DNNs has been explored before in the literature and is not new. For example, in speech in multi-lingual speech processing this has been explored before.  o	This paper does joint training of the shared and task-specific components: K. Vesley et al, “The Language-independent bottleneck features,” in Proc. ICASSP 2012.  o	This paper also does joint training: Z. Tuske et al, “Investigation on Cross-and Multilingual MLP Features under Matched and Mismatched Acoustical Conditions,” in Proc. ICASSP 2013. o	This paper does separate training of shared and task-specific components: Samuel Thomas, Sriram Ganapathy and Hynek Hermansky, Multilingual MLP Features For Low-resource LVCSR Systems, ICASSP, Kyoto, Japan, March 2012 •	Details of and Ring training are not clearly described. Also, relating this to prior art and justifying why this is the best approach is missing. •	The paper could be written much better. There are many spelling/grammar mistakes in the paper   Here are my comments per section: •	Section 1, page 1: expand on SIFT, HOG and LBP •	Section 1, page 1: Your statement on CNNs being good doesn’t fit with the rest of the sentence •	Section 1, page 2: exiting → existing •	Section 2: There needs to be a discussion on Multi-task DNNs relation to prior work and why your idea is novel. For example, see the papers listed above for speech processing. •	Section 2, page 3: What is the loss function that you use.  •	Section 3, page 3: I found the discussion of Ring Training very confusing. You should describe the algorithm 1 in more detail in the text. Also, ring training is not the only way to train Multi-task DNNs. Why did you use this approach as opposed to other ideas in the literature, and why is it potentially better? •	Section 4, page 3 CIFRA→ CIFAR •	Section 5.2, page 6: Why did you only add dropout to the first fully connect layer? Did you tune this optimally? Sometimes it helps to add dropout to multiple layers. You should state if it’s tuned properly or not. •	Section 5.3 page 6: Provide a reference for DCGThis paper proposes a multi-task deep neural network approach to solve the task of image retrieval. A dataset of clickthrough  data is used to train the network.  The main contribution of this paper is the proposed multi-task DNN method for image retrieval and the ring training. This multi-task approach consists of adding weights specific to each query (basically a supplemental fully-connected layer) before binary classification. The authors compare their approach to binary DNNs and multi-class DNN. However, I would have like to see a multi-label DNN as well for comparison.  Quality: I found the quality of the language passable. There were several syntactic, grammatical errors and typos which made the paper hard to understand. The paper could have used more polishing. In general, I have found the paper hard to read, and the authors did not get their points through clearly.  General comments: What loss function was used? I don't understand why the authors did not give it explicitly.  In the ring training pseudo code, it is not mentioned how j is defined, is there an iteration on all values, or is it chosen randomly? It makes a lot of difference how j is treated if the classes are unbalanced. This should be explained clearly.  In section 4, the architecture of the network is presented, but nothing is mentioned about which weights were query-specific in the multi-task case for the CIFAR-10 experiment.  section 4.2: 'In general, binary DNN performs consistently worse for the severe overfitting problem.' This sentence does not make a lot of sense, and does not reflect the results very well.  I don't really see the point of using dataset 2. The difference in results between d1 and d2 are small, and do not, in my opinion, demonstrate what the authors claim it shows. I think it only clutters the paper.  The authors use the term significantly without showing confidence intervals. Perhaps another term should be used.  Section 5.1: 'multi-class DNN is infeasible for such large number of queries'. This would require more explanation. Why does the multi-task approach scale better than the multi-class approach? Since the parameters are not shared, doesn't the multi-task DNN require even more parameters than the multi-class? If this is not the case, please explain more clearly your point.  In section 5.3, why not use the activation of the classification layer as an affinity measure instead of training SVMs",1,5121
"Generative NeuroEvolution for Deep Learning Phillip Verbancsics & Josh Harguess  Summary: Hyperneat neuroevolution is applied to MNIST, then fine-tuned through backprop. Results are perhaps not that overwhelming in terms of accuracy, but interesting.  Lots of relevant work seems to be missing though. For example, earlier indirect encodings were proposed by the following works:  A. Lindenmayer: Mathematical models for cellular interaction in development. In: J. Theoret. Biology. 18. 1968, 280-315.  H. Kitano. Designing neural networks using genetic algorithms with graph generation system. Complex Systems, 4:461-476, 1990.  C. Jacob , A. Lindenmayer , G. Rozenberg. Genetic L-System Programming, Parallel Problem Solving from Nature III, Lecture Notes in Computer Science, 1994  A universal approach to indirect encoding based on a universal programming language for encoding weight matrices with low Kolmogorov complexity:  J.  Schmidhuber. Discovering solutions with low Kolmogorov complexity and high generalization capability. In A. Prieditis and S. Russell, editors, Machine Learning: Proceedings of the Twelfth International Conference (ICML 1995), pages 488-496. Morgan Kaufmann Publishers, San Francisco, CA, 1995.  p 3:  'Such object recognition tasks where deep learning has achieved the best results include the MNIST hand-written digit dataset [30, 31], … and the ImageNet Large-Scale Visual Recognition Challenge [33]'   But refs [30, 31] do not have best results on MNIST - as of 2012, the best result was in:  D. Ciresan, U. Meier, J. Schmidhuber. Multi-column Deep Neural Networks for Image Classification. Proc. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012, p 3642-3649, 2012.  There may be even better results of 2013? And to my knowledge the best result on ImageNet is here:  M. D. Zeiler, R. Fergus. Visualizing and Understanding Convolutional Networks. TR arXiv:1311.2901 [cs.CV], 2013.  p 3: 'vanishing gradient' - why not cite the man who first discovered this:  S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, TUM, 1991  p 3: 'auto-encoders' - I think the authors mean stacks of auto-encoders - here one can cite:  [35] D. H. Ballard. Modular learning in neural networks. Proc. AAAI-87, Seattle, WA, p 279-284, 1987  p 4: 'In this way, HyperNEAT acts as a reinforcement learning approach that determines the best features to extract for another machine learning approach to maximize performance on the task'  So it is like Evolino, where evolution is used to determine the best features to extract for another machine learning approach to maximize performance - what is the main difference to the present approach? See:  J. Schmidhuber, D. Wierstra, M. Gagliolo, F. Gomez. Training Recurrent Networks by Evolino. Neural Computation, 19(3): 757-779, 2007.  p 7: 'NeuroEvolution approaches have been challenged in effectively training ANNs order of magnitude smaller than those found in nature'  Compare, however, vision-based RNN controllers with a million weights evolved through Compressed Network Search:  J. Koutnik, G. Cuccu, J. Schmidhuber, F. Gomez. Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), Amsterdam, 2013. Also see lots of earlier work on this: http://www.idsia.ch/~juergen/compressednetworksearch.html  General recommendation: Interesting, but it should be made clear how this work is related to or goes beyond the previous work mentioned above.The paper proposes to automatically learn the structure of a deep neural network using Neuro Evolution (NE) techniques. In particular the authors apply this technique called HyperNEAT (Hypercube based Neuro Evolution of Augmenting Topologies) to learn the structure of a deep convolutional network. This is achieved by learning an indirect encoding which encodes the weight pattern of the neural network. Once learnt, the final network is fine tuned using backpropagation. The technique is applied to the task of identifying hand written numbers in the MNIST dataset.   In deep learning community, in a number of papers it has been shown that the choice of architecture plays an important role in the performance of the final trained model on any task. However, the question of 'how to choose the right architecture' has not got much attention. In that sense this paper tries to address this important question by automatically learning the network architecture using Neuro Evolution techniques. Unfortunately however, I think the paper falls way short of making any significant contribution in that direction.   Novelty: I think there are not a lot novel ideas presented in the paper. Its a straight-forward application of HyperNEAT to CNNs. There is no new technique/model being proposed here.   Quality of results: First, I feel the MNIST dataset is beaten to death and claiming it to be a real world task is probably not true any more. Second, the results reported on MNIST are quite underwhelming. The best accuracy achieved is 92.1% whereas the state-of-the-art results are in the range of 99+%. I would expect that for a technique to be considered promising it should have performance at least in the ball-park of state-of-the-art, if not best.   Previous work: Since I'm not an expert in the area it is hard for me to point any (if there are so) missing references to the previous work in the field. That said a couple of claims by the authors are not true. For instance, references 30 and 31 do not provide the state-of-the-art.   Clarity of presentation: I think the quality of the write-up is quite average. For instance, the authors could have explained the HyperNEAT architecture (section 2.2) much better, especially for people who are not experts in the area. I don't see the point of putting the HyperNEAT-LEO algorithm in section 2.2, since it is not used anywhere in the paper. Also the paper is littered with typos.   Summary: Though the paper tries to address an important problem in deep learning research, I believe its falls short of delivering. The proposed approach is a straight-forward application of HyperNEAT, and the results on the standard dataset are quite poor. This to me suggests that some work needs to be done before one can consider it as a conference publication.Many straightforward variants of combinations of neuroevolution and backprop have been tried in the past, never with very convincing results. Unfortunately, the same holds true here. Don't get me wrong, I do not think the approach is in principle flawed, but in my opinion, very convincing, robust-to-outstanding performance is necessary for this type of paper to be allowed on a conference track like this. Sorry.",0,5122
"In this paper, the authors propose a new sparse autoencoder. At training time, the input vector is projected onto a set of filters to produce code values. These code values are sorted and only the top k values are retained, the rest is set to 0 to achieve an exact k-sparse code. Then, the code is used to reconstruct the input by multiplying this code by the transpose of the encoding matrix. The parameters are learned via backpropagation of the squared reconstruction error. The authors relate this algorithm to sparse coding and demonstrate its effectiveness in terms of classification accuracy on the MNIST and NORB datasets.  The paper is fairly incremental in its novelty. There are several other papers that used similar ideas. Examples of the most recent ones: - R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, J. Schmidhuber. Compete to Compute. In Proc. Neural Information Processing Systems (NIPS) 2013, Lake Tahoe. - Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations  K Balasubramanian, K Yu, G Lebanon Proceedings of the 30th International Conference on Machine Learning 2013  The theoretical analysis is good but straightforward. What worries me the most is that a very important detail of the method (how to prevent 'dead' units) is only slightly mentioned. The problem is that the algorithm guarantees to find k-sparse codes for every input but not to use different codes for different inputs. As the code becomes more and more overcomplete (and filters more and more correlated), there will be more and more units that are not used making the algorithm rather inefficient. The authors propose to have a schedule on 'k' but that seems rather hacky to me. What happens if the authors use twice as many codes? what happens if they use 4 times as many codes? My guess is that it will break down easily.  My intuition is that this is a very simple and effective method when the code has about the same dimensionality of the input but it is less effective in overcomplete settings. This is an issue that can be rather important in practical applications and that should be discussed and better addressed.  Pros: - simplicity - clearly written paper  Cons: - lack of novelty (see comments above) - lack of comparison   - I would add a comparison to A. Coates method and to K. Gregor's LISTA (or K. Kavukcuoglu's PSD) (software for these methods is publicly available). These are the most direct competitors of the proposed method because they also try to compute a good and fast approximation to sparse codes. - the method is a bit flawed because it does not control sparsity across samples (yielding to possibly many dead units). It would be very helpful to add experiments with a few different values of code dimensionality. For instance on MNIST, it would be interesting to try: 1000, 2000 and 5000.  Overall, this is a nicely written paper proposing a simple method that seems to work fairly well. I have concerns about the novelty of this method and its robustness to highly overcomplete settings.* Brief summary of the paper:  The paper investigates a very simple heuristic technique for a simple autoencoder to learn a sparse encoding: the nonlinearity consists in only retaining the top-k linear activations among the hidden units and setting the others to zero. With the thus trained k-sparse autoencoers, the authors were able to outperform (on MNIST and NORB) features learned with denoising autoencoders, RBMs or dropout, as well as deep networks pre-trained with those techniques and fine-tuned.   * Assessment:  This is an interesting investigation of a very simple approach to obtaining sparse representations that empirically seem to perform very well. It does have a number of weaknesses:  - I find it misleading to call this a *linear model* as in the abstract. A piecewise linear function is *not* a linear function. The model does use a non-linear sparsification operation. - For such a simple approach, I find the actual description of the algorithm (especially in the algorithm box) disappointingly fuzzy, unclear, confusing and probably wrong:   What is the exact objective being optimized? Is is always squared reconstruction error? It is not written in the box. Also do you really reconstruct x^ from a z that has not been sparsified (as is written in step 1)?? This is contrary to my understanding from reading the rest of the paper. I believe it would be much clearer to introduce an explicit sparsification step before the reconstruction. Similarly, with your definition of supp, it looks like the result of your sparse encoding h is a *set of indices* rather than a sparse vector. Is this intended? Wouldn't it be clearer to define an operation that returns a sparse vector rather than a set of indices? The algorithm box should be rewritten more formally, removing any ambiguity. - Section 3.3: While I find the discussion on the importance of decoherence interesting, I do not believe you can formally draw your conclusion from it, since you do not have the strict equality x = Wz (perfect reconstruction) that your theorem depends on but only an approximate reconstruction. So I would mitigate the final claims.  - I wonder how thoroughly you have explored the hyper-parameter space for the other pre-training algorithms you compare yourself with, especially those that are expected to influence sparsity or control capacity somehow, as e.g. the noise level for denoising autoencoders and dropout?? Did you but try a single a-priori chosen value? If so the comparisons might be a little unfair since you hyper-optimized your alpha on the validation set.  * Pros and Cons:  Pros:  + interesting approach due to its simplicity  + very good empirical classification performance.  Cons: - confusing description of the algorithm (in algorithm box);  - possibly insufficient exploration of hyper-parameters of competing algorithms (relative to the amount of tweakings of the proposed approach).The authors propose an auto encoder with linear encoder and decoder, but with sparsity that keeps only k elements in the hidden layer nonzero. They show that it works as well or better then more complicated methods.  Novelty: Simple but works Quality: Good  Details: - The paper introduces a very simple idea which I am sure many people not only thought of but implemented, including me. However the main point here is that the authors actually made it work well and made a connection to a sparse coding algorithm. One of tricks of making it work seems to be to start with a large number of allowed nonzero elements and then decrease it, otherwise, many filters would not ever be used.   - Is there a mistake in the algorithm box as presented x = Wz+b'. Shouldn't the z be replaced by something like z_Gamma where the latter is obtained from z by setting elements that are not in the group of k largest to zero? Because that's what the description in the rest of the paper implies, for example in 2.2.  - Table - it would be good to explain that the net is in Table 3's caption.",1,5123
"An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning Patrick C. Connor, Thomas P. Trappenberg  Summary: The authors discuss benefits of reinforcement learners with a supervised module that predicts primary reinforcements and a reinforcement learner which sums their values.  Quite a few references to relevant previous work seem to be missing, e.g., see below.  p 2: 'The notion of predicting future observations or states is not new to RL [9, 10].'  These references are of 2004 and 2005. Two much older references from 1990 on this:  R. S. Sutton. First Results with DYNA, an Integrated Architecture for Learning, Planning and Reacting. Proceedings of the AAAI Spring Symposium on Planning in Uncertain, Unpredictable, or Changing Environments, 1990.  J.  Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In Proc. IEEE/INNS International Joint Conference on Neural Networks, San Diego, volume 2, pages 253-258, 1990.  p 6: 'the architecture we propose is not trained using a scalar reward prediction error, but rather a vector of state feature-speciﬁc prediction errors'  So it is like the vector-valued adaptive critic for multi-dimensional reinforcement signals of the old system from 1990 below - please discuss the differences:   J. Schmidhuber. Recurrent networks adjusted by adaptive critics. In Proc. IEEE/INNS International Joint Conference on Neural Networks, Washington, D. C., volume 1, pages 719–722, 1990  J. Schmidhuber. Additional remarks on G. Lukes’ review of Schmidhuber’s paper ‘Recurrent networks adjusted by adaptive critics’. Neural Network Reviews, 4(1), 1990.  Below other old predictors of state feature-speciﬁc prediction errors. RL is based on training a recurrent system that combines the world model and the actor. Which are the relative advantages or drawbacks of the approach of the authors?:  N. Nguyen and B. Widrow. The truck backer-upper: An example of self learning in neural networks. Proceedings of the International Joint Conference on Neural Networks, 357-363, IEEE Press, Piscataway, NU, 1989  J. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 2(1 & 2):135-141, 1991  Experiments: this is a very simple RL task.   General recommendation: It is not quite clear to this reviewer how this work goes beyond the previous work from 20 years ago mentioned above. At the very least, the authors should make the differences very clear.This paper proposes to break value prediction into prediction of a specific outcome (e.g., food) and a value for that outcome allows reinforcement learning to make the correct predictions for inhibitors -- as opposed to standard RL which predicts negative value for a reward inhibitor. In practice, this is implemented by separately predicting positive (reward) and negative (punishment) values using rectified predictors. Tests on a toy problem with 16 training points show that this setting indeed avoids the usual prediction of opposite valence for an inhibitor.  Inhibitors are a difficulty for RL and present an interesting puzzle, however this paper fails to make a good case:  1) There is no real technical contribution here: separating positive from negative values while using standard predictors has been done often (e.g. in computer vision); the toy test is exceedingly simple. The bulk of the technical part of the paper is a lengthy explanation of simple maximum likelihood estimation of a thresholded linear function.  2) The paper is motivated as 'tak[ing] a cue from biological systems', but there are problems with this too: - 'we train function approximators to predict specific primary reinforcements (e.g., food, charging station, etc.).' This makes the job of an inhibitor easier indeed, but predicts that a function approximator should not be able to produce trans-reinforcer blocking, where a predictor trained on one specific primary reinforcement (e.g., shock) blocks conditioning to another reinforcement (e.g., loud noise). See: Bakal, C. W., Johnson, R. D. & Rescorla, R. A. The effect of change in US quality on the blocking effect. Pavlov. J. Biol. Sci. 9, 97–103 (1974). - 'we might take a cue from biological systems, where it seems that the dynamic revaluation of a primary reinforcer due to satiety is second nature': while this is true of primary reinforcers, this is not always true of secondary reinforcers, that RL needs to train as well. There is a lot of literarture on sensitivity to outcome devaluation, e.g.:  Motivational control of goal-directed action Anthony Dickinson, Bernard Balleine Animal Learning & Behavior March 1994, Volume 22, Issue 1, pp 1-18  In its current form, the paper does not seem to offer enough for acceptance, in terms of either technical novelty or insight into natural behavior.Summary This paper proposes a value function approximation architecture which explicitly models inhibition effects for reinforcement learning. In this context, inhibition refers to a stimulus that eliminates a reward when presented along with a stimulus which usually produces a reward. For example, the stimulus pair ‘NP’ does not produce a reward whereas ‘P’ alone does, and ‘N’ alone produces a reward of 0. The authors address the issue of value function approximators not correctly modeling ‘N’ in isolation having reward 0, as opposed to a negative reward. The authors propose decomposing the value approximation problem into two halves, one to estimate negative rewards and another to estimate positive rewards. The outputs of these functions are then combined to form a final value estimate. The authors derive a linear regression function approximator which rectifies its output to properly fit into the proposed two-stage scheme. The method is evaluated on a toy dataset of 16 examples which illustrates the problem.  Review The abstract is very hard to follow. Please rewrite it to more clearly describe the problem and your contribution.  The problem of confusing negative reward stimuli with inhibitors does not seem inherent to the reinforcement learning problem. Instead it seems to be a property of the chosen value function approximation algorithm. The authors should discuss, and perhaps test, whether a more expressive class of value function approximators exhibit this property. Given the conference, a neural network function approximator could be appropriate. Additionally, why not decompose the estimator into positive / negative reward estimates but use neural nets to approximate each half? They can naturally output rectified rewards without additional derivation.   The derivation equations are easy to follow, but their motivation is unclear. Please add more text discussing why it is necessary to make such derivations instead of choosing a different estimator  The experimental evaluation leaves much to be desired. The toy example is nice as a control experiment, but far from comprehensive. I would like to see a larger problem in which your approach performs better or discovers inhibition structure other value function approximators can not discover.  Key points + Proposed approach of decomposing value function approximation seems interesting and could lead to approximating functions which are easier to understand or debug - Unclear whether a more expressive class of value function approximators would fail to learn about inhibition stimuli automatically - Toy dataset evaluation - Writing is okay but does not clearly introduce the problem and motivate the approach",1,5124
"This work presents a similarity-preserving hashing scheme to produce binary codes that work well for small-radius hamming search.  The approach restricts hash codes to be sparse, thereby limiting the number of possible codes for a given length of m bits.  A small-radius search within the set of valid codes will therefore have more hits, yet the total bit length can be lengthened to allow better representation of similarities.  According to the authors, this is the first application of sparsity constraints in the context of binary similarity hashes.  Overall I think this is a nice, well-motivated idea and corresponding implementation, with experiments on two datasets that demonstrate its effectiveness and mostly support the claims made in the motivation.  As a bonus, the authors describe a further adaptation to cross-modal data.  I still feel there are some links missing between the analysis, implementation and evaluation that could be made more explicit, and have quite a few questions (see below).   Pros:  - Well-argued idea for improving hashing by restricting the code set to enable targeting small search radii and large bit lengths  - Experiments show good system performance   Cons:  - Links between motivational analysis, implementation and evaluation could be made more explicit  - Related to that, some claims alluded to in the motivation don't appear fully supported, e.g. more efficient search for k-sparse codes doesn't seem realized beyond keeping r small   Questions:  - You mention comparing between k-sparse codes of length m and dense codes with the same degrees of freedom, i.e. of length log(m choose k).  This seems very appropriate, but the evaluation seems to compare same-length codes between methods.  Or, do the values of m reflect this comparison?  m=128 v. m=48 may work if k is around 10.  But I also don't see anything showing the distribution of nonzeros in the codes.  - pg. 4:  'retrieving partial collisions of sparse binary vectors is ... less complex ... compared to their dense counterparts':   Could this be explained in more detail?  It seems a regular exhaustive hamming search is used in the implemented system (especially since there appears to be no hard limit on k, so any small change can valid for most codes).  - The ISTA-net uses a sign-preserving shrink, and the outputs are binarized with tanh (also preserving sign) -- thus nonzeros of the ISTA-net can saturate to either +/- 1 depending on sign, while zeros are mapped to 0.  These are 3 values, not 2, so how are they converted to a binary vector, and how does this align with the xi(x) - xi(x') in the loss function (which seems to count a penalty for comparing +1 with 0 and a double-penalty for comparing +1 with -1)?  - Eqn. 4:  Distance loss between codes is L1 instead of L2, and there is no max(0, .) on the negatives (but still a margin M).  Are these errors or intended?  - I'm not sure how the PR curves were generated:  What ranking was used?  - Table 4 last line: Says alpha=0; it seems the sparsity term would be disabled if alpha=0, so not sure why results here are better than NN-hash instead of about the same?   Minor comments:  - Would have liked to see more comparing level of sparsity vs. precision/recall for different small r.  There is a bit of this in the tables, but it would be interesting to see more comprehensive measurements here.  It would be great if there was a 2d array with e.g. r on the x axis and avg number of nonzeros on the y axis, for one or more fixed m.  - How many layers/iterations were used in the ISTA-net?  - Fig 6 left, curves for m=128 appear missing for nnhash and agh2The authors propose to use a to use a sparse locally sensitive hash along with an appropriate hashing function. The retrieval of sparse codes has different behaviour then dense code, which results in better performance then other other methods especially at low retrieval radius where computation is cheaper.  Novelty: Good (as far as I know).  Quality: Good, clearly explained except few details (see below), with experimental evaluation.  Details: - How do you define retrieval at a given radius for sparse codes? With two bit flips say, there are the same number of neighbours whether the code is dense or sparse. With the encoding proposed you don't have a guaranteed that only k values will be nonzero - it is not a strict bound. How do you define the neighbours - as only those that have the same or lower sparsity?    - Is formula (4) correct, specifically line 2? I assume it should be more like the line 2 of eq. (3).  - In the experiments, how did you calculate neighbours of such large radii - the number of neighbours grows as Choose(m, r).   - There are a lot of hyper parameters: eq.4: lambda, alpha, M + eq. 5 mu_1, mu_2. How did you choose these? If your answer is 'cross validation' - how theses are a lot of parameters to cross validate. Do you have any good ways to set these?  - Even though this is basic it would be good to explain in section 2(Efficient Retrieval) how is the retrieval defined - what is the situation?This paper builds upon Siamese neural networks [Hadsell et al, CVPR06] and (f)ISta networks [Gregor et al, ICML10] to learn to embed inputs into sparse code such that code distance reflect semantic similarity. The paper is clear and refers to related work appropriately. It is relevant to the conference. Extensive empirical comparisons over CIFAR-10 and NUS/Flickr are reported. I am mainly concerned by the computational efficiency motivation and the experimental methodology. I am also surprised that no attention to sparsity is given in the experiments given the paper title.   The introduction states that your motivation for sparse code mainly comes from computational efficiency. It seems of marginal importance. The r-radius search in an m dimensional space is nchoosek(m,r). With k sparse vectors, the same search now amount to flipping r/2 bits in the 1 bits and r/2 bits in the 0 bits in the query, i.e. nchoose(k,r/2)+nchoose(m-k, r/2). Both are comparable combinatorial problems. I feel that your motivation for sparse coding could come from the type of work you allude to at the end of column 1 in page 4 (by the way could you add some references there?).    I have concerns about the evaluation methodology. In particular, it is not clear to me why you compare different methods with a fixed code size and radius. It seems that, in an application setting, one might get some requirement in terms of mean average precision, recall at a given precision, precision at a fix recall, expected/worst search time, etc and would validate m and r to fit these performance requirement. Fixing m and r a priori and looking at the specific precision, recall point resulting from this arbitrary choice seems far from optimal for any methods. Moreover, I would also like to stress that m, r are also a poor predictor of a method running time given that different tree balance (number of points in each node and in particular the number of empty nodes) might yield very different search time at the same (m,r). In summary, I see little motivation for picking (m,r) a priori.    I am also surprised that no attention to sparsity is given in the experiments given the paper title. How was alpha validated? What is the impact of alpha on validation performance, in particular how does the validation error surface look like wrt alpha, m and r? It might also be interesting to look at the same type of results replacing L1 with L2 regularization of the representations to further justify your work. Also reporting the impact of sparsity on search time would be a must.",0,5125
"Speech and Signal Processing, vol. 37, pp. 1641–1648, 1989  •	Section 5, page 10: I’m not convinced by the experimental results, as indicated above. Further comparison should be done with other RNN training methods, as well as with stronger DNN baselines.Using Hidden Markov Models,” IEEE Transacations on Acoustics,  Pro:  •	The concept of using AR/ARMA for RNN training, and formulating this as a convex optimization problem, and interesting. Cons: •	A new idea (AR/ARMA) is introduced as yet another technique to deal with the vanish gradient problems when training RNNs. You touch in the related work on how this is different than other ideas in this space. I would have liked to see empirical comparisons in the experimental section, at least with a few of the ideas (i.e. Martens’ HF approach) to better understand the value of this technique. •	ARMA seems similar to the bidirectional RNN proposed by Graves (ICASSP 2013) and experiments or discussion comparing this should be done. •	The experimental results are not convincing.  o	DNNs trained with filter-banks on TIMIT are around 20% (see Abdel-rahman Mohamed, George Dahl, Geoffrey Hinton, 'Acoustic Modeling using Deep Belief Networks'. Accepted for publication in IEEE Trans. on Audio, Speech and Language Processing) while your RNN results are around 28% o	It seems that most of the gains you are getting are by using DNN-based features as input in to the RNN. The idea of extracting features from the DNN and then training another network is not new in speech recognition (see Z. Tüske, M. Sundermeyer, R. Schlüter, and H. Ney. “Context-Dependent MLPs for LVCSR: TANDEM, Hybrid or Both?”, in Proc. Interspeech, September 2012). What happens if you train your DNN in the same way, how would this compare to the RNN? •	References could be improved, citing the most relevant papers in the field rather. Will indicate this in my comments below.  Here are my comments per section: •	Page 1, section 1: When describing papers that cut recognition errors for speech, the main papers are Hinton 2012, Dahl 2011 which you have. In addition, pls include F. Seide’s DNN SWB paper from Interspeech 2011, B. Kingbury’s HF paper from Interspeech 2012 and T. Sainath’s paper from ICSASSP 2013. These 3 papers showed the biggest impacts in reducing WER across many LVCSR tasks. Pls remove Deng2013b. •	Page 1, section 1: Its not clear from the intro how AR/ARMA ideas address the vanishing gradient issues with RNNs discussed in para 1. Pls elaborate on this more •	Section 2, page 2: Oriyal Vinayls RNN paper should be cited as well along with Maas, Graves,  work. •	Section 2, page 2: in describing DNNs ability to extract high-level features, Yann Lecun has a paper on this and should be cited: Y.LeCun,“LearningInvariantFeatureHierarchies,”inEuropeanCon- ference on Computer Vision (ECCV). 2012, vol. 7583 of Lecture Notes in Computer Science, pp. 496–505, Springer. •	Section 3.2, page 3: Why did you use sigmoid instead of ReLU ,which has been shown extensively to work much better on TIMIT •	Section 3.2, page 3: you are move eq (3) as you don’t use it in the paper •	Section 3.2, page 3: The ARMA model, which looks into the future, related to the biodirectinal RNN proposed by Graves (ICASSP 2013). The motiviation seems similar and thus differences should be clarified. •	Section 3.3, page 4: most of this is review from other papers and does not need 2 pages of explanation. Just summarize the main high level points so more of the paper can be focused in Section 4 on your novel contributions •	Section 4, page 6: I’m not sure why you say RNNs require relatively short memory of just 30 frames. Oriyal Vinyals’ RNN paper shows that 60 is more reasonable. In addition, graves uses an LSTM framework which allows for more long-range dependencies which he shows is better than an RNN. •	Section 5, page 9: The TIMIT paper to be cited is Kai Fu Lee’s paper which describes the standard protocol, not (Hintin 2012 and Deng 2013a): K. F. Lee and H. W. Hon, “Speaker-independent Phone Recognition  Summary: ------------  The paper introduces a primal-dual method for training RNNs as well as a few other structural changes to the base model. The algorithm is tested on the TIMIT dataset. Comments: --------------   I think my main observation about the paper is that it misinterprets the exploding gradient problem/vanishing gradient problem as well as the echo state property.  Simply put, comparing the norm of the recurrent weight with 1 (for tanh) just gives either a necessary condition for gradients to explode. That means that there is a large volume of parameter configuration that do not satisfy this condition and yet the gradients do not explode. This constraint, in some sense, over-restricts the capacity of the model. It is true that the empirical evidence shows that the model still performs well (arguably better than the unconstrained one) but this is just a data point (a single dataset) and is hard to draw conclusion such as the new method has `superior performance`. It is far from clear to me that this is true, I would argue that by excluding that large volume of possible values, with some positive probability this new learning algorithm is inferior on several tasks. The echo state property is also an approximation of what exactly we want from the model, as most of the analysis of recurrent networks. Basically the echo state property assumes that if **no input** is presented the, now, dynamical system will converge to a point attractor at the origin. In reality we care about the behavior of the model in the presence of input. This is mathematically much more difficult to analyze though there is some effort done e.g.Manjunath, G., Jaeger, H. (2013): Echo State Property Linked to an Input: Exploring a Fundamental Characteristic of Recurrent Neural Networks.  Further more, I would argue that the model trained with the primal-dual method will suffer a lot more from the vanishing gradient problem and it is potentially one of the crucial factor that makes the ARMA variant outperform the other variations (as it explicitly uses a time window)   Other comments -------------------- (1) I feel wordings like 'superior performance', 'demonstrate the effectiveness' can be misleading. Providing the number 18.86% in the abstract, without a point of comparison, is also not very useful. (2) I'm not sure I understand why stacked RNN suffer from an overfitting problem ? And would there be a reason, if they do suffer, for this overfitting to not be addressed by say weight noise or some other regularization ?  (3) Eq. (10). You do not want to take an average over the sequence length. That will bias you towards short sequences (4) Most of the equation on page 5 are not very useful (showing the cost and gradients for softmax and cross-entropy vs linear units and square error). In general I feel there are more equation then necessary, making the text harder to read (6) Minimizing the sum of each column is not the same as minimizing only the column with the maximal sum. You are putting pressure on certain columns even when they are not the maximal one. (7) There are hardly any details about the experiments that you run.This work presents a method for training RNNs that achieves good results on TIMIT.   The method applies a deep recurrent neural network similar to Graves et al. (ICASSP 2013) and achieves good results on TIMIT.  The main novelty here is the introduction of a new training method which enforces a constraint of a certain easily-computable matirx norm using lagrange multipliers.  As a result, the network improves its performance from 19.05% to 18.86%.   The work also introduces a number of RNN variants that get their information from a fairly wide range of frames.   The main idea is related to previous analyses of the exploding gradient problem.  The approach taken by this work is to force the RNN's weights to be small at all times, thus ensuring that the RNN never has exploding gradinets.    There are several weaknesses in the paper.  First, it is quite verbose, spending pages on standard definitions of RNNs and derivations of their learning rules.  Second, the paper chose to use lagrange multipliers, with another lengthy derivation, but did not compare with the much simpler projected gradient descent, where we simply shrink any weights that become too large (i.e., if ||w||_1 is too large, scale it down until it is of the right size).  And third, the improvement over previous work is quite small.",1,5126
"Dear authors,    Let me reveal my identity before I continue. I'm Kyunghyun Cho who wrote the earlier review (by an unexpected coincidence). The previous comments reflect what I wanted/want to say as an official reviewer, and I will not write another separate review.    - ChoThis paper is in the domain of using GRBMs to train filters which can then be convolved and pooled before an L2-SVM is used for classification. The overall idea of finding degraded classification results after prolonged RBM training is an interesting one.  The authors then proposes an information criterion for early stopping. The paper is well written and makes link between AMI and filter quality and classification performance.  The fact that GRBM lose useful filters might be due to contrastive divergence training. Also, the concept of overfitting is hard to decipher here because learning GRBM filters is not for directly reducing classification error.  The argument that validation error is too expensive to compute is only unique to a framework where a probabilistic model is used to pretrain filters, which are then used in a discriminative pipeline.  For majority of discriminative based models, this argument would not apply and early stopping could simply be performed based on validation error.In this paper the authors observe an interesting training dynamics when training GRBMs on CIFAR patches: filters first become selective to the usual Gabor looking features but then many faint and decay to 0 towards the end of training. From this observation they propose to measure an approximation to the mutual information between input and hidden units to detect the time when most features are used. They demonstrate that this is an effective method to automatically select the best checkpoint to use for discrimination on CIFAR dataset.  Pros - sufficient novelty: I am not aware of other works making this observation and proposing this measurement of goodness of a trained GRBM - relevance: the problem of automatically selecting good features is very relevant many working on unsupervised learning (one of the main themes of this conference).  Cons - the paper is not very clear. I find the authors' use of words like 'overcomplete', 'useful', etc. very vague. I will explain better below what I did not quite understand. - technically I am not sure the paper is correct; I did not follow how the authors measure mutual information exactly. More details below. - the impact of this work seems rather limited: the empirical validation is done on CIFAR only showing modest improvements.  I will expand here on what I did not understand or did not agree. First of all, the fluency of English should be improved. The choice of words is often too vague. For instance, in the abstract the authors say: “ gain non-overcomplete representations which include uniform ﬁlters that do not represent useful image features”. What does this mean? What are 'uniform filters' or “ useful ﬁlters“? It would be nice if every part of the paper was self-explanatory, concise yet precise in its meaning.  Second, I am not convinced by the arguments about why filters decay to zero. From fig. 1, it seems to me that filters do not decay to zero at all. The range of values of filters that become low-pass shrinks, which is rather different and perhaps expected. Most energy is in the low frequencies, however hidden units are forced to be in {0,1} which forces the low-pass (less localized) filters to rescale their values to a smaller range. If the authors rescaled each filter independently they may see a different picture (please, verify if this is correct). Besides, it would be useful to get an estimate of the log-probability of the training data during training: is that steadily increasing? The monotonic behavior of FED seems to show an overfitting effect. I am not sure the hypothesis chosen by the authors in the introduction is the only one that can explain their observations. Similarly in sec. 3, I do not understand the authors' conclusions. The number of hidden units need not to match the dimensionality of the data. Without any regularization, the highest likelihood on the training set would be achieved by having each hidden unit represent a single training sample, and therefore, the highest likelihood can be achieved by having as many hidden units as training samples (although this would be horrible on the test set). From this perspective, I did not understand what happened in the example of fig. 2A. Did the authors use any regularization? How did the authors measured 'overcompleteness' and 'fitness'?   Third, I did not understand how the authors computed mutual information. First, mutual information assumes a joint distribution over data and hidden units. Are the authors replacing p_GRBM(v) with  p_data(v)?  Moreover, I did not follow how the authors compute S_D(H). Shouldn't it be the entropy of the hidden units? which means marginalizing the visibles to get p_GRBM(h) and then computing the entropy. The authors should report the formula they used and explicitly mention the approximations used. In the current draft, I do not understand how S_D(H) is computed or differs from the conditional entropy term. For the sake of clarity, I would recommend to include in the main body of the paper the definition of bar{AMI} and tilda{AMI} as well.  Finally, empirical evidence is shown only on the CIFAR dataset. It is not obvious this is a general finding/method.   Overall, this is an interesting paper. Unfortunately, I did not fully understand it nor I was fully convinced by its claims. I recommend a good rewrite of the paper to address these concerns. In general, the authors mention that this method is able to make GRBM a better model of natural images and contrast it to other models like SS-RBM, gMRF, etc. But actually, what they propose has little to do with these other methods (and it could be applied to these methods as well). The proposed method is about extracting discriminative features by measuring a quantity that correlates with number of non-dead filters. The question is then: are there even simpler measures (like variance across samples of the corresponding filter responses or variance of the coefficients in the filters)?",1,5127
"This paper details some training tricks such as transformations, combining multiple scales and different views for ILSVRC challenge which performed pretty well compared to last years winner.  While there is nothing dramatically novel in terms of deep learning model or theory, empirical performance is the invisible hand that guides deep learning and generates buzz. So it is important to publish practical tricks-of-the trade and implementation details that can benefit all researchers.This paper goes over data augmentation techniques that give 20% relative improvement over last year's winner of ILSVRC.  First set of improvements is to augment training data with more kinds of distortions -- 2x zoom, different cropping approach. The second set of improvements is to combine predictions on multiple transformed versions of the image.  Because there are 90 transformed versions of the image, computing all 90 is too expensive, so they do a greedy approach where they combine predictions until there is 'no additional improvement.'  The combination approach is missing important details. How are predictions combined? What does 'no additional improvement' mean in regards to greedy method? This greedy method works on individual examples, but without knowing true labels, what's an 'improvement'?  Graph in Figure 3 shows that greedy method slightly outperforms the method which uses 'full 90 predictions', which seems counter-intuitive, but then I can't tell if it's an error because precise description of combining method is missing  Overall this paper is slightly interesting because it measures the impact of additional transformations, but hardly novel since these approaches were tried before. Also, it is interesting because it shows that ImageNet is going the way of 'MNIST research', where authors focus on incremental improvements tailored for a specific dataset (in this case -- better cropping) and might not transfer to other datasetsThis paper presents a few simple tricks to improve over Krizhevsky's model: - use full image instead of cropped square by Krizhevsky. - jitter in image contrast, brightness and color. - add scales at testing time (+ greedy algorithm to keep it fast). - extra high-resolution model trained on scaled up patches (essentially adding scales at training time). - turning off dropout for the last learning rate decrease.  Novelty: nothing new here, just the application of existing techniques.  Pros: - very good result on ILSVRC13 classification.  Cons: - the improvements proposed are not novel but the paper can be made interesting if details are given. For example, instead of just mentioning contrast, brightness and color jitter, explain precisely what operations are taking place here. And what do the .5 and 1.5 values correspond to? - the greedy algorithm is missing some details: 'starts with the best prediction', how is best defined here? highest confidence? out of which views? how many views are initially computed? - the training images for the high-res model differs from run-time, scaling up patches of 128x128 is in my opinion not as good as using the full-resolution image directly like at run-time. Although this approach possibly introduces noise that is beneficial for the case of images smaller than 224x224 which need to be scaled up at test time. And as mentioned, this augments the data considerably so it is a good thing.",1,5128
"The manuscript presents a novel approach to random forests in which a linear discriminative transformation is learned at each split node in order to enhance class separation of the weak learners. The work offers a merger of existing ideas in an innovative way. The experimental results, which include synthetic and real-world datasets, adequately support the theoretical claims. Also noted is the comparison to other classification tree models, clearly differentiating the proposed scheme from prior state-of-the-art subspace learning methods. A core argument made is that the proposed method can perform better with fewer and more shallow trees than other popular, yet relatively simpler, classification tree methods.  Weaknesses: (1) A single classifier type coupled with the linear transformation was described. It would be valuable if other classifier/transformation pairs were explored (e.g. SVNs). Does the framework easily extend to other classification techniques?  (2) All dataset images used were scaled down to 16x16. How does this method perform on truly high-dimensional data?  (3) Is there a reason that not all experiments were performed with each of the datasets? (4) Timing and accuracy comparison results would be valuable if presented for additional datasets, thereby establishing the scalability properties of the algorithm; timing results aren't too informative when reported for a single dataset.  (5) Experiments showing the effect of noise/mislabeld examples would be interesting as the impact of such imperfections on this scheme is unclear. (6) Section 3.2, Paragraph 2, Sentence 1: What is the reason for mentioning that classes arriving at a node are randomly split into two categories? Why is it being introduced here as a new source of randomness?  Minor editorial comments:  - Section 3.1, Paragraph 1, Sentence 1: Could be worded to flow better.  - Section 3.1, Paragraph 2, Sentence 2: '... the paper ...' ->    '...  this paper ...  - Section 3.1, Paragraph 2, Sentence 4: Missing word (termination 'criteria'?)  - Section 3.2, Paragraph 2, Sentence 2: Missing period.  - Section 3.2, Paragraph 2, Sentence 3: '... the paper ...' ->    '...  this paper ...  - Section 3.2, Paragraph 2, Sentence 5: '... in details ...' ->    ' ... in detail ...'  - Section 3.2, Paragraph 2, Sentence 1: Not clear that its actually the    samples from the dataset that the referenced figure contains.Summary  This paper introduces a new split node learner for use in classification trees. The learner is parameterized by a d x d dimensional matrix T (d is the input dimension) and two subspaces, one for each class in a binary classification problem. At test time, a data point is transformed by T and assigned to the class whose subspace is best able to approximate the transformed point (least L2 residual). For each split node, the transformation T is learned by (locally) minimizing a non-convex objective function that is a difference of nuclear norms of transformed data matrices. Theoretical justification is given for the objective, showing that if the objective is 0 then the transformed subspaces for the two classes is orthogonal.  Empirical results are shown on four datasets and indicate that the proposed method obtains good performance.  Novelty and Quality  To my knowledge, the proposed split objective is novel. The paper is very well written and the method is explained in a clear manner.    Pros  + The split objective and learner is novel and well motivated. + Experimental performance is convincing compared to other split learners. + The theoretical justification for the method is compelling and the presentation is clear.  Cons / Questions for author feedback  Details of the experimental setup are missing in some cases:  - What d-dimensional feature vectors are computed for each experiment? And what is d in each case? This is explained for the Kinect experiment, but not for the others.  - Sec. 3.1 “we further evaluate the effect of randomness introduced by randomly dividing classes arriving at each split node into two categories.” I don’t follow how the experiment described here evaluates this source of randomness. This simply reports accuracy on the 15-scenes dataset. I was expected an experiment that controlled for this source of randomness.  - For the Kinect experiment, why did you use only test images (450 for training/50 for testing)? It would have been better to follow the setup in (Denil et al., 2013) so that the results in the paper are comparable to Denil et al.’s results.  The paper is missing a discussion of the drawbacks of the proposed approach. For example, decision trees with decision stumps are often applied to problems where the feature space is very large (or even infinite). How could the proposed approach be modified to work in these cases?  Did you consider applying a difference of convex programming approach to (1) instead of gradient descent?  Sec 2.4: “... only involves matrix multiplication, which is virtually computationally free at the testing time.” This seems like a odd claim. If d is large, then these matrix multiplications are far from free.This paper studies a new split rule for classification trees, based on the distances to two linear subspaces constructed at each node.  Subspaces are constructed by partitioning a data subset by class, and finding a linear transformation that attempts to concentrate each same-class space, while maximizing angle distance between the two (different-class) spaces.  At test time, a left/right decision is made based on which subspace the test point is closer to (based on projected reconstruction).  For multiclass problems, classes are randomly assigned at each node to one of the two subspaces.  The method is supported by theoretical analysis, toy examples, simple image classification datasets, and a pixel labeling task.  I'm not very knowledgeable in random forests, but this seems like a decent proposal to me, effectively integrating a linear subspace method into the decision rule.  The image classification tasks are on fairly simple datasets, but nonetheless may be good fits for the approach.  One question I have is how dependent this method is on image alignment.  The three datasets all have fairly well aligned images, potentially lending themselves to gains by integrating linear subspace methods (in pixel-space).  Notably, the proposed method performs quite well on 15-Scenes -- I wonder if there may be any additional take-aways concerning alignment of the images in this dataset and how else this might be used in classification of such scene data.  The results from the kinect pixel labeling look pretty good, and strengthens the paper with an application different from classifying aligned images, but it would be nice to include a comparison to Denil et al (the dataset providers).   Pros:  - Method is well motivated and theoretically analyzed - Demonstrated to be effective in several tasks - Very well written  Cons:  - Applications are somewhat simple   More questions:   - Fig. 5:  It would be interesting to put the 'Identity learner' (subspace assignment without the transformation T) in all 3 of these plots.  I'd be particularly interested to know the effect of this step on MNIST, which according to Fig 6 seems reasonably separated in the original space as well.  - Pixel descriptors for kinect:  I think they are depth differences from the center pixel obtained at 32 equally spaced points along circles at 3 different radii (8, 32, and 64), forming a 96-dim descriptor -- is this correct?  I wasn't entirely sure based on the description.  - 'All the images were resized to 16 x 16':  It's ambiguous whether this was for just YaleB, or all the datasets?",1,5129
"The idea presented in this paper consists in iteratively building new features from products of so-called 'super features', obtained from a PCA in parameter space of multiple models trained on a subset of the data. The theoretical analysis is done with logistic regression, and no experiments are performed.  In its current form, this paper is definitely not ready for publication. The fact that there is no experiment is already a pretty big downside, and the theory does not seem complete enough to me to justify it. In particular: - There is no proof or even intuition for the convergence of the procedure. To   be fair, there is no claim that it will converge either, but what is the   point of Section 5 if this is not the case? - The abstract mentions a guarantee of increasing likelihood which is not shown    in the paper. - There is no analysis of the complexity of the algorithm (it seems costly,    especially since each iteration involves sampling multiple datasets). - There is no discussion of extensions beyond logistic regression. - There is no justification for using PCA in parameter space (which works best    with Gaussian-like unimodal distributions, but may fail in other situations:   how can we tell it makes sense for a specific application / model?)  That being said, this is an intriguing idea, and I am hoping the author will be able to investigate it more thoroughly so as to be able to present it in a more fleshed out paper in the future.  There are several points that were unclear to me and may be worth mentioning in  addition to the above high level comments: - The equation below eq. 1 does not seem to be used anywhere. - 'Values of regularization parameters must be found by maximizing Bayesian    integral in Equation 1.' => this integral is the likelihood -- optimizing   regularization parameters to maximize the likelihood seems wrong to me. - '[eq. 1] is estimated by maximum likelihood approximation': I do not see how    MLA is a good approximation here. - eq. 5 seems arbitrary to me, especially since the datasets are of different   sizes,  so each L_w_s involves a different number of terms. - It is not mentioned exactly how the 'sample data' step works (for instance in   bootstrap one typically samples t_max points with replacement, but it does   not seem to be the case here). - I do not understand how G_alpha,beta can be non-zero in eq. 16, and still   lead to a linear combination. - Not clear what are func and F in eq. 19.This paper presents an iterative way of defining increasingly complex feature spaces for prediction model selection.  It is argued that in the limit, this sequence of features results in a feature algebra, meaning that products of features are linear combinations of features within the feature space.   The mathematical ideas could be interested if explained in more clarity.  The nonlinear super features look like an interesting way of reasoning of feature compositions, however,   * I am not sure about the usefulness of the construction.   * Version 1 of the paper is quite unpolished.   MINOR:  In eq. (4) `maxarg' should be `argmax'.  In eq. (9) use `langle' and `angle' instead of `<' and `>'.  Eq. (6) seems not to be used in Section 3.> - A brief summary of the paper's contributions, in the context of prior work.  The way we represent data -- the features we use -- has proven essential to a wide variety of tasks in machine learning. Often, the trivial features we naturally get from the data perform poorly. One approach is to hand-engineer features, having humans carefully construct features based on their understanding of the data and the task. Another approach, taken in deep learning, is to learn features as part of the optimization process of a multi-layer model. This paper proposes an alternative approach in which 'super-features' are generated by an iterative exploration process.  As the reviewer understands it, this iterative process begins with random sigmoidial features. Features are evaluated on several subsets of the dataset and good features are selected. Principal component analysis is performed on the parameters of the good features in order to define a lower-dimensional space of good features. The next iteration proceeds on products of the principal components discovered in the previous step. In the limit, the discovered features form a space with nice algebraic properties.  > - An assessment of novelty and quality.  The reviewer is not an expert but to the best of their knowledge this approach is novel.  The reviewer is concerned that this paper may be a bit rushed: it suffers from many grammatical errors, and some parts of the paper are hard to follow. The reviewer thinks that a bit of further revision would greatly benefit this paper.  > - A list of pros and cons (reasons to accept/reject)  Pros: * The paper introduces some interesting, novel ideas. In particular, attempting to do dimensionality reduction in parameter space seems like a really interesting strategy. (I'm concerned that PCA may not be a very good way to do this, but the idea is very interesting.)  Cons: * The paper does not do any experiments to test the efficacy of the proposed approach. Given how radically different the proposed approach is from things that have been previously tried, it seems really difficult to have any significant confidence in it without experimental results. (Not having experimental results for this would seem more appropriate if this were a workshop track submission.) * The paper suffers from many grammatical issues. These really need to be fixed in a final version of this paper. * The paper is quite hard to follow at some points.  Testing this approach on a standard dataset would also provide an opportunity for the author to walk the reader through a concrete application of this approach, in addition to providing experimental validation of it.",0,5130
"Summary of contributions: Proposes a new activation function for backprop nets. Advocates using global mean pooling instead of densely connected layers at the output of convolutional nets.  Novelty: moderate Quality: moderate  Pros: 	-Very impressive results on CIFAR-10 and CIFAR-100 	-Acceptable results on SVHN and MNIST 	-Experiments distinguish between performance improvements due to NIN structure and performance improvements due to global average pooling Cons: 	-Explanation of why NIN works well doesn’t make a lot of sense  I suspect that NIN’s performance has more to do with the way you apply dropout to the model, rather than the explanations you give in the paper. I elaborate more below in the detailed comments.  Did you ever try NIN without dropout?  Maxout without dropout generally does not work all that well, except in cases where each maxout unit has few filters, or the dataset size is very large. I suspect your NIN units don’t work well without dropout either, unless the micro-net is very small or you have a lot of data. I find it very weird that you don’t explore how well NIN works without dropout, and that your explanation of NIN’s performance doesn’t involve dropout at all.  This paper has strong results but I think a lot of the presentation is misleading. It should be published after being edited to take out some of the less certain stuff from the explanations. It could be a really great paper if you had a better story for why NIN works well, including experiments to back up this story. I suspect the story you have now is wrong though, and I suspect the correct story involves the interaction between NIN and dropout.  I’ve hear Geoff Hinton proposed using some kind of unit similar to this during a talk at the CIFAR summer school this year. I’ll ask one of the summer school students to comment on this paper. I don’t think this subtracts from your originality but it might be worth acknowledging his talk, depending on what the summer school student says.  Detailed comments:  Abstract: 	I don’t understand what it means “to enhance the model discriminability for local receptive fields.” 	  Introduction 	Paragraph 1: I don’t think we can confidently say that convolutional net features are generally related to binary discrimination of whether a specific feature is present, or that they are related to probabilities. For example, some of them might be related to measurements (“how red is this patch?” rather than “what is the probability that this patch is red?”) In general, our knowledge of what features are doing is fairly primitive, informal, and ad hoc. Note that the other ICLR submission “Intriguing Properties of Neural Networks” has some strong arguments against the idea of looking at the meaning of individual features in isolutian, or interpreting them as probabilistic detectors. Basically I think you could describe conv nets in the intro without committing to these less well-established ideas about how conv nets work. 	 	Paragraph 2: I understand that many interesting features can’t be detected by a GLM. But why does the first layer of the global architecture need to be a nonlinear feature detector? Your NIN architecture still is built out of GLM primitives. It seems like it’s a bit arbitrary which things you say can be linear versus non-linear. i.e., why does it matter that you group all of the functionality of the micro-networks and say that together those are non-linear? Couldn’t we just group the first two layers of a standard deep network and say they form a non-linear layer? Can’t we derive a NIN layer just by restricting the connective of multiple layers of a regular network in the right way?  	Paragraph 3: Why call it an mlpconv layer? Why not call it a NIN layer for consistency with the title of the paper?  	Last paragraph: why average pooling? Doesn’t it get hard for this to have a high confidence output if the spatial extent of the layer gets large?  Section 2: Convolutional Neural Networks  	eqn 1: use 	ext{max} so that the word “max” doesn’t appear in italics. italics are for variable names.  	Rest of the section: I don’t really buy your argument that people use overcompleteness to avoid the limitations of linear feature detectors. I’d say instead they use multiple layers of features. When you use two layers of any kind of MLP, the second layer can include / exclude any kind of set, regardless of whether the MLP is using sigmoid or maxout units, so I’m not sure why it matters that the first layer can only include / exclude linear half-spaces for sigmoid units and can only exclude convex sets for maxout units.  	Regarding maxout: I think the argument here could use a little bit more detail / precision. I think what you’re saying is that if you divide input space into an included set and an excluded set by comparing the value of a single unit against some threshold t, then traditional GLM feature detectors can only divide the input into two half-spaces with a linear boundary, while maxout can divide the input space into a convex set and its complement. Your presentation is a little weird though because it makes it sound like maxout units are active (have value > threshold) within a convex region, when in fact the opposite is true. Maxout units are active *outside* a convex region. It also doesn’t make a lot of sense to refer to “separating hyperplanes” anymore when you’re talking about this kind of convex region discrimination.   Section 3.1 	Par 1: I’d argue that an RBF network is just an MLP with a specific kind of unit.  	Equation 2: again, “max” should not be in italics  Section 4.1 	Let me be sure I understand how you’re applying dropout. You drop the output of each micro-MLP, but you don’t drop the hidden units within the micro-MLP, right? I bet this is what leads to your performance improvement: you’ve made the unit of dropping have higher capacity. The way you group things to be dropped for the droput algorithm actually has a functional consequence. The way you group things when looking for linear versus non-linear feature detectors is relatively arbitrary. So I don’t really buy your story in sections 1-3 about why NIN performs better, but I bet the way you use dropout could explain why it works so well.  Section 4.2 	These results are very impressive! 	While reading this section I wanted to know how much of the improvements were due to global averaging pooling versus NIN. I see you’ve done those experiemnts later in section 4.6. I’d suggest bringing Table 5 into this section so all the CIFAR-10 experiments are together and readers won’t think of this objection without knowing you’ve addressed it. 	 Section 4.3 	Convolutional maxout is actually not the previous state of the art for this dataset. The previous state of the art is 36.85% error, in this paper: http://www.cs.toronto.edu/~nitish/treebasedpriors.pdf  	Speaking of which, you probably want to ask to have your results added to this page, to make sure you get cited: http://rodrigob.github.io/are_we_there_yet/build/  Section 4.4 	http://arxiv.org/pdf/1312.6082.pdf gets an error rate of only 2.16% with convolutional maxout + convolutional rectifiers + dropout. Also, when averaging the output of many nets, the DropConnect paper gets down to 1.94% (even when not using dropout / DropConnect). Your results are still impressive but I think it’s worth including these results in the table for most accurate context.  Section 4.5 	I think the table entries should be sorted by accuracy, even if that means your method won’t be at the bottom.  Section 4.6 	It’s good that you’ve shown that the majority of the performance improvement comes from NIN rather than global average pooling. 	It’s also interesting that you’ve shown that moving from a densely connected layer to GAP regularizes the net more than adding dropout to the densely connected layer does.  Section 4.7 	What is the difference between the left panel and the right panel? Are these just examples of different images, or is there a difference in the experimental setup?> - A brief summary of the paper's contributions, in the context of prior work.  Convolutional neural networks have been an essential part of the recent breakthroughs deep learning has made on pattern recognition problems such as object detection and speech recognition. Typically, such networks consist of convolutional layers (where copies of the same neuron look at different patches of the same image), pooling layers, normal fully-connected layers, and finally a softmax layer.  This paper modifies the architecture in two ways. Firstly, the authors explore an extremely natural generalization of convolutional layers by changing the unit of convolution: instead of running a neuron in lots of locations, they run a 'micro network.' Secondly, instead of having fully-connected layers, they have features generated by the final convolutional layer correspond to categories, and perform global average pooling before feeding the features into a softmax layer. Dropout is used between mlpconv layers.  The paper reports new state-of-the-art results with this modified architecture on a variety of benchmark datasets: CIFAR-10, CIFAR-100, and SVHN. They also achieve near state-of-the-art performance on MNIST.  > - An assessment of novelty and quality.  The reviewer is not an expert but believes this to be the first use of the 'Network In Network' architecture in the literature. The most similar thing the reviewer is aware of is work designing more flexible neurons and using them in convolutional layers (eg. maxout by Goodfellow et al, cited in this paper). The difference between a very flexible neuron and a small network with only one output may become a matter of interpretation at some point.  The paper very clearly outlines the new architecture, the experiments performed, and the results.  > - A list of pros and cons (reasons to accept/reject).  Pros: * The work is performed in an important and active area. * The paper explores a very natural generalization of convolutional layers. It's really nice to have this so thoroughly explored. * The authors perform experiments to understand how global average pooling affect networks independently of mlpconv layers. * The paper reports new state-of-the-art results on several standard datasets. * The paper is clearly written.  Cons: * All the datasets the model is tested on are classification of rather small images (32x32 and 28x28). One could imagine a few stories where the mlpconv layers would have a comparative advantage on small images (eg. the small size makes having lots of convolutional layers tricky, so it's really helpful to have each layer be more powerful individually). If this was the case, mlpconv would still be useful and worth publishing, but it would be a bit less exciting. That said, it clearly wouldn't be reasonable to demand the architecture be tested on ImageNet -- the reviewer is just very curious. * It would also be nice to know what happens if you apply dropout to the entire model instead of just between mlpconv layers. (Again, the reviewer is just curious.)Authors propose the following modification to standard architecture:  Replace: convolution -> relu with convolution -> relu -> convolution (1x1 filter) -> relu  Additionally, instead of using fully connected layers, the depth of the last conv layer is the same as number of classes, which they average over x,y position. This generates a vector of per-class scores.   There's a number of internal inconsistencies and omitted details which make reproducing the results impossible. Those issues must be fixed to be considered for acceptance. They do bring up one intriguing idea which gives a novel approach to localization.  - Authors would be better off using standard terminology, like I did above, that makes reading the paper easier.  - Some space is unnecessarily taken up by issues that are irrelevant/speculative, like discussing that this architecture allows for feature filters that are 'universal function approximators.' Do we have any evidence this is actually needed for performance?  - In section 3.2 they say that last averaged layer is fed into softmax, but this contradicts Figure 4 where it seems that last layer features actually correspond to classes, and no softmax is needed. I assumed the later was the intention.  - Following sentence is unclear, consider expanding or removing:  'A vectorized view of the global average pooling is that the output of the last mlpconv layer is forced into orthogonal subspaces for different categories of inputs'  - Most serious shortcoming of this paper is lack of detailed explanation of architecture. All I have to go on is the picture in Figure 2, which looks like 3 spatial pooling layers and 6 convolutional layers. Authors need to provide following information for each layer -- filter size/pooling size/stride/number of features. Ideally it would be in a succinct format like Table 2 of 'OverFeat' paper (1312.6229). We have implemented NIN idea on our own network we used for SVHN and got worse results. Since detailed architecture spec is missing, I can't tell if it's the problem of the idea or the particulars of the network we used.  One intriguing/promising idea they bring up is the idea of using averaging instead of fully connected layers. I expected this to allow one to be able to localize the object by looking at the outputs of the last conv layer before averaging, which indeed seems like the case from Figure 4.",1,5131
"Summary of contributions: Proposes a specific means to achieve Bengio's goal of 'conditional computation'. The specific mechanism is to use the sign of approximate activations to predict which activations are zero, so that the full-rank values of rectified linear units don't need to be computed for units that are probably 0.   Novelty: moderate Quality: low  Pros: -Demonstrates that the sign approximation method slightly improves test error. Cons: -The main purpose of the work is to reduce computational cost, but computational cost is never quantified -It's not obvious that predicting which units are zero will lead to the ability to reduce computation cost -If computational cost can in fact be reduced, it's not clear that the cost of the predictor is less than the cost that its use could remove -The baselines that are improved upon are not competitive -It's not clear that the hyper parameters were chosen in a way that makes the comparisons fair -Some of the commentary is misleading or questionable  Detailed comments:  pg 1 I don't think you can necessarily conclude that the computational requirements to train a large net decreased between the publication of [12] and the publication of [4]. The purpose of [12] was largely to demonstrate how many machines their system could use. At the ICML poster session where [12] was first presented, I heard someone else ask Quoc Le if the number of machines was actually necessary to get the results they had, and he said the high number of machines was not necessary, they just wanted to claim credit for networking that many machines to train a neural net. It's likely that most of the savings in [4] relative to [12] are just due to the fact that most of the computation in [12] was wasted.  pg 2  Section 2.1  I don't think you can conclude that redundancy of parameters implies redundancy of activations. For example, two units with the same weight vector have very redundant parameters, but changing just their bias unit can give them very different activations. Presumably nets would not benefit much from increased size if the units were truly redundant.  Simply being able to predict which units in a net are going to have zero activation doesn't necessarily mean that you will get computational savings. You need the cost of predicting which units are zero to be cheap enough that on average running both the predictor net and the predicted subset of the main net is cheaper than running just the main net. You also need some mechanism for cheaply running subsets of the main net. Doing sparse matrix operations with an arbitrary, dynamically updated sparsity mask is not significantly more efficient than just doing dense operations, to my knowledge.  I think you're missing the fundamental idea of conditional computation, at least as described in [2] and [3]. Bengio is not advocating figuring out cheaper ways of computing exactly the same function we already compute. He's advocating figuring out how to compute only some of the features that are most relevant to processing a particular input. What you're doing is more of a software engineering optimization where you figure out which features are zero. Bengio is advocating something more ambitious, figuring out which features are unnecessary even if they are nonzero. Another issue with your approach is that you're trying to individually predict which features are nonzero. This means you need to be able to have the predictor make an explicit decision for each of them individually, so your runtime is going to have asymptotic cost which is at least linear in the number of units. Bengio is advocating making predictions about entire groups of units at the same time. If done right, this has the chance of having cost only logarithmic in the number of features in the model, or, to phrase it more excitingly, being able to run a model that has exponentially more features than runtime and memory requirements.  Section 2.2 I'm not sure sparsity of the representation is the correct explanation for the performance of the rectifier nets in [7]. The sparse rectifier nets in [7] have sparse activations, but they have equally sparse gradients. i.e., the gradient through a unit is zero for an example if and only if the activation is zero for that unit. This paper ( http://arxiv.org/pdf/1302.4389v4.pdf ) shows that similar nets can perform well if they have sparse gradients but non-sparse activations. See especially Fig 2. of that paper. So sparse gradients may be more important than sparse activations.   pg 3  Your presentation is confusing because you refer the reader to fig 3.1 before defining S, but S is used in the caption of this figure.  Usually one does not use italic letters when denoting the 'sgn' function, since italic letters are used for algebraic variables.  pg 4  Section 3.3  I don't understand the paragraph on dropout and sparsity. The most problematic sentence is 'During training, the sparsity of the network is likely less than p for each minibatch.' Does 'more sparsity' mean fewer active units? So less sparsity means more active units? If we set p to zero, this implies that every unit is likely active. But that is the default SGD training algorithm, in which we certainly know that not all the units are active. Also, are you counting units zeroed by dropout as contributing to the sparsity? What about units that are zeroed by their input being negative? Why should their be any straightforward and simple relation between the dropout p and the number of units zeroed by their inputs being negative?   Section 3.4  1 is probably a pretty high value for the initial biases. Was dropout applied to the input as well as the hidden layers? If so, p = 0.5 is probably too high for use on the input.   pg 5  Table 4.1: Where did these hyper parameters come from? Why should the same hyper parameters be used for all conditions? How can we know that your 'control' curve in Fig 4.1 is a good baseline to improve over? If you hand-tuned your hyper parameters to work well with the S predictor net active, then it's not very surprising the control would perform worse. I'm not saying you intentionally did this, but without a description of your methodology for obtaining the hyperparameters, it seems likely that you implicitly did this while trying to get your new method to work.  It seems like your experiments are bypassing the most important part of the paper, which is to demonstrate a speedup. I don't see anything about timing experiments here. I suspect you can't actually get a speedup by this approach due to the difficulty of leveraging a speedup from sparsity if the sparsity pattern is dynamic rather than fixed, also due to the difficulty of leveraging sparsity on GPU if there is not a regular structure to the sparsity pattern. Moreover, as far as I can tell, you don't quantify the cost of frequently running SVD during training.  Are your nets actually large enough to be interesting? The bigger of the two has about three million parameters and doesn't seem to be able to fit the SVHN dataset all that well. Conditional computation is mostly interesting for its ability to push the frontier of network capacity, so I'm not sure the scenario you've experimented on is really interesting from the perspective of evaluating a conditional computation method.  pg 6  Similar criticism for MNIST, except here I'm confident that your baseline control system is not good. With dropout you should be able to get down to < 1% test error.The authors investigate a proposed method of speeding up computation in a feed-forward ReLU network by predicting the sign of the presynaptic activations with a low-rank predictor obtained by SVD on the weights.  Novelty: medium Quality: low  Pros: - Investigates a problem of substantial interest to the community, that of scaling up neural nets through what Yoshua Bengio has dubbed “conditional computation” - Experimental procedures are well documented, software cited  Cons: - misinterprets the goals articulated by Bengio on conditional computation: namely, to explicitly learn which computations to do/which parts of an architecture to activate, rather than simply identify ways of speeding up existing architectures through predictive computation. In that respect it could still be an interesting line of inquiry but not really “conditional computation” related. - Provides no empirical benchmarking: I strongly suspect an empirical investigation of the proposed speedup would reveal that the cache unfriendliness of the non-uniform memory access would result in a significant slowdown even for large, relatively sparse network. - The commentary contains several instances of speculation that is not qualified as such (see below) - The experimental baselines are questionable, uninteresting, and applying the same sort of fully connected architecture to only two tasks, one of which has not been widely studied with fully-connected networks and thus hard to judge. Especially in the first layer it seems like the efficacy of the low rank predictor may depend heavily on the input distribution.  Detailed comments:  Sec 2.1: - The speculation that parameter redundancy leads to activation redundancy is very questionable. Consider two filters consisting of oriented, localized edge filters, one appearing exclusively in the upper left quadrant of the receptive field and the other appearing in the lower right, with 3 of 4 quadrants having zero weights in each filter. The two have extremely redundant parameters (only a few bits are needed to describe one given knowledge of the other), but their activations are not redundant in the least.  Sec 2.2: - Both of the datasets on which you run experiments have had extremely competitive (state of the art, in the case of SVHN) performance documented with maxout activations, where activities are completely non-sparse. The importance of representational sparsity is thus far less clear than you seem to suggest.  Sec 3.1: - Notational comment: sigma() is typically reserved for the logistic function, or at least a sigmoidal function such as tanh. Its use to represent the ReLU activation is somewhat jarring.  - You can probably assume that a reader of this paper understands how to perform matrix multiplication. The exposition in terms of dot products is unnecessary. - Regarding considerable speed gains, I find this dubious given the degree to which optimized matrix-matrix multiplication libraries (the BLAS, etc.) leverage the properties of today’s CPU and GPU hardware. In sparse linear algebra applications where sheer memory requirements are not the limiting factor (i.e. making explicitly representing the sparse matrix infeasible), sparsity well in excess of 99% is typically necessary for specialized sparse matrix multiplication routines to beat BLAS on a sparse problem represented densely. While you may be able to claim an asymptotic speedup, it’s unclear whether this means anything in practice for a wide class of problems of interest. Even if things are sparse enough for there to be a savings, it’s unclear whether this would be negated by the necessity of performing the low-rank computation as well. - It would be interesting and comforting to see some analysis (theoretical or empirical) of the probability of the low rank approximation making a mistake (and each kind of mistake, as mistakenly flagging a filter response as having a positive value results in no actual error being made in the final analysis, as it will be computed and then presumably thresholded to zero, whereas not computing one that should’ve been positive will affect the representation passed up to the next layer). If you assume bounded norms of the weight vectors I’m fairly sure you could say something theoretically as a function of the rank of the approximation. - I’d also be interested in how the activation estimator’s efficacy differs for the first layer and subsequent layers, given that the latter have a more sparse input distribution, and whether preprocessing helps or not. Similarly, how does error accumulate through this process when you use it at multiple layers?  Sec 3.4: - multiplying an asymptotic O(...) expression by a constant makes no sense. O() notation is explicitly defined to absorb all multiplicative constants. What you should do instead is either a) make use of a numerical methods textbook pseudocode implementation of SVD to bound the number of theoretical flops, or b) drop the O(...), add a multiplicative constant (which you could bound) and add O(...)’s for the lower-order terms.  Sec 4.1 & 4.2 - Generally, the lack of comparison to the literature for your baselines is troubling. I don’t know what the state of the art is for a permutation-invariant method (such as a fully connected network) on SVHN, but your nets seem critically underresourced. I know that layer sizes for an MNIST network achieving close to 1% typically features layer sizes at least double those of what you report, and it seems like you are underfitting significantly on SVHN. The whole point of conditional computation is to save computation time in very large models, but you restrict your investigation to extremely impoverished architectures. - Your MNIST results are significantly worse with what is the permutation-invariant state of the art standard nowadays and there’s really no excuse for it, an error of 1.05-1.06% is easily reproducible. At the very least it would frame the error rates in terms of results with which people are familiar. - The architectures of the activation estimators seem arbitrary and it is not clear how they were chosen, or why a wider variety were not explored. - In addition to the error rates for various activation estimator architectures, you should make clear the theoretical speedup provided by this architecture, and the measured empirical speedup.  Section 5: - The extension to the convolutional setting is not obvious to me at all. Sure, you can write out the convolution as a matrix multiply with a structured sparse matrix but the SVD of that is going to be dense and gigantic. - There’s no mention of the possibility of learning the low-rank approximation, which seems like the natural thing to do, especially as concerns test-time speed-up rather than train-time speedup. I also consider it critical to compare against the case where there is no full rank weight matrix, just this low rank version being used as the actual weight matrix, to show that maintaining and carefully multiplying by the full rank weight matrix actually is in some respect a necessary ingredient. If you can get away with simply learning factored weight matrices (i.e. introducing bottleneck linear layers) then this scheme is needlessly complicated.",1,5132
"This paper proposes an estimator for the log-likelihood of intractable models with latent variables. The approach is simple in that it has no free parameters, doesn’t require an explicit likelihood, and only needs samples from the model. The approach is most useful for model comparison since the estimate is conservative rather than optimistic.  I enjoyed reading this paper. The proposed method is quite novel and elegant and has the potential to be a useful tool for model comparison. One issue is that the estimator seems to require a large number of samples in order to converge, and this potentially exacerbated by increasing model size. As stated in the paper, this is likely to do with the convergence of MCMC within the model itself. One empirical test of this would be to compare the efficiency of the estimator with exact samples vs MCMC in e.g., a small RBM.  The biased CSL is also novel, but seems to be even more optimistic than AIS. The argument of the paper is based on the idea that we would prefer conservative estimates to optimistic estimates for model comparison. When would the authors expect the biased CSL method to be useful in practice? How many steps would be required before biased CSL matches AIS?  Minor thoughts and some found typos below.  1. In table 1 the AIS and CSL estimates are vastly different. One is optimistic and one is conservative - which one is closer to the truth? Is there a reason that GSN is so much better? Obviously the truth is impossible to determine, but it is clear that more samples are needed before the estimate converges. 2. The RBM used in table 2 is quite small, using only 5 hidden units. 20 hidden units is slightly larger but still tractable. It would be good to see how the efficiency of the estimator is affected by model size. 3. An RBM trained with PCD is thought to have much better likelihood than an RBM trained with CD. Is this reflected in CSL estimates?  formulat -> formulate (section 1) collecte -> collect (section 2) in -> in (Monte-Carlo estimator in section 4) 30 steps -> 300 steps (or the the legend in Figure 1 has a typo, section 6) mode -> model (section 7)The paper proposes a method for estimating the log probability of any probabilistic model that can generate samples.  The method builds a local density estimator around the samples using the model's conditional probability,  which is used to evaluate the log probability of a test set.  An important selling point of the method is that it evaluates the probabilistic model and the  sampling procedure jointly, and that it is asymptotically consistent, in the sense that the estimates converge to the true likelihood as the number of samples approaches infinity.  This work is quite novel, and it places the idea of used by Breuleux et al. in a  rigorous framework.  Empirically, the method works well on small models, although it exhibits very substantial divergence from AIS on larger models, as shown in Table 1.  Perhaps the greatest weakness of this method, which is worth discussing, is that the number of samples that's needed in order to accurate compute a log probability grows exponentially with the entropy of the distribution.  For an example, consider the dataset consisting the concatenations of of 10 randomly chosen MNIST digits.  It is  fairly clear any sample set <<< 10^10 will vastly underestimate the log probability of a perfectly good sample. That is unfortunate, because it means that the method will not work well on complicated models of high entropy distributions, such as images or speech.   This weakness notwithstanding, the method is very adequate for model comparison.    To summarize:  Pros:  interesting method for obtaining conservative underestimates of the log probability, works with nearly any model.  Cons:  method's complexity is exponential in the distribution's entropy;  the proposed fix is no longer  conservative.In this paper, the authors propose a new way to estimate the probability of data under a probabilistic model from which sampling is hard but for which an efficient Markov chain procedure exists. They first present an asymptotically unbiased estimator, then a more efficient biased estimator.  The idea is undeniably interesting. Some of the most used generative models satisfy these constraints and being able to calculate the probability of data under these models is crucial to comparing them. However, the results presented in this paper are underwhelming. For models where AIS was usable (the DBN, the DBM and the RBM), the CSL results wildly differ from the AIS ones. Since the results on the small RBM (Table 2) give a clear advantage to AIS, I am inclined to believe these results more.  Another caveat, unfortunately extremely difficult to avoid, is that the effectiveness of these methods can only be empirically proven on tiny models where mixing problems do not occur. I really do not blame the authors for that but this really limits the potential impact of the method.  Experiment in Figure 1 is also very light to conclude on the effectiveness of Biased CSL. Binary MNIST is a very particular dataset and this experiment does not convince me that it is actually usable to compare models, especially of different types.  Conclusion: this paper does not prove the effectiveness of the proposed method. The propositions are not worth publication by themselves.  Other comments: - CSL is only a lower bound on the true log probability of the data in expectation. This should be made clearer in the paper. - The pseudo code should either be commented or removed entirely. As it is, it is only useful to people who already understood the algorithm. - Could you give more details on the parameters for AIS? How many chains? How many intermediate temperatures? How does the computation time compare to CSL?",1,5133
"The paper describes a heuristic algorithm for learning representation features from labeled and unlabeled data. These features are then used as inputs to a classifier trained on labeled data. The main motivation of the algorithm is semi-supervised learning when unlabeled data can be used for estimating and selecting the representation features. Properties of these new features are analyzed under different hypothesis and in particular, a bound is provided for selecting the new features among a set of candidates. Experiments are then performed on a large dataset of Medline abstract and the proposed approach is compared to supervised and semi-supervised classifiers. The paper introduces new ideas for learning intermediate representations.  They appear to be quite effective in the experimental comparison and the proposed method performs better than alternative semi-supervised baselines. The author also develops a series of properties of the learned features. On the other hand, the paper presents different weaknesses. The development of these features is neither intuitive nor theoretically founded. The final classifier performs a linear combination of the initial data features, and it is hard to understand why it should perform better than state of the art linear methods. The proposed heuristic could be interpreted as a prior on classifier weights, but again in the context of large training sets like the ones used in the experiments, this should not be a real advantage. Some of the experimental results are surprising. It is said that logistic regression and linear SVMs cannot be trained on large amounts of data and the author does not present results with these classifiers beyond 200 k training data. This should be reconsidered.  Results of semi-supervised classifiers are also doubtful. The baselines used for comparison are not able to leverage the use of unlabeled data when usually for text classification, semi-supervised learning is quite effective. The form of the paper should also be improved.  In particular the experimental illustration of the distance and bound statistics is not clearly explained. Globally, there are interesting ideas in this method, but also conceptual and experimental weaknesses that should be improved upon.The paper proposes a feature selection algorithm and learn a new representation based on the selected features.  The new representation is then used for constructing classifiers.   This is a very difficult paper to read, due to the exposition style. As far as I can gather, the feature selection algorithm just selects features are (on average) as correlated to other features as possible, cf. step 1 in section 2.2. The new representation is then weighted combination of the old features.   I have difficulty in understanding either contributions.  There is little motivation on using a reference feature --- which is not available in practice.  Moreover, how Theorem 3's condition is generally true?This paper proposes an interesting approach: identify features which are likely to correlate with a class predictor on a small set and learn to predict the presence of this binary features on a large unlabeled dataset. The paper is however not very clear and its experimental section is far from thorough.   The paper is dense but misses important points. In particular,   - what are the modeling assumptions of RDEs?   - what types of data are likely to succeed?  It could be more fluent and better structured. In particular, the theorems follow each other without much transitions. The reader could be guided more telling what you are going to show and why, on results are going to be linked toward the final goal. I also feel that the results of supervised RDE on their own seems very good, better than SVM and logistic regression. It might be good to invest more work to propose RDEs in a supervised setting with extensive empirical comparison on reference dataset, before then extending them to semi-supervised learning. A paper with a single message is likely to be clearer.   The quality of its experimental section could greatly be improved. The experiments are performed on a single dataset. Reporting results on standard text classification datasets like RCV1 and other types of data would greatly improve the paper. I feel this is necessary since your results indicate that SVM and logistic regression, which have been state-of-the-art text classification algorithms for many years seem significantly outperformed by the simple supervised RDE. If this is confirmed over benchmark dataset this could be a game changer. However, this single result in non-standard setting is likely to rise doubts. I would also appreciate an analysis of the different step of the algorithm:  - is the reference feature selection step important (e.g. you could compare with other feature selection strategies like information gain) ? - are the individual RDE good predictors of their reference features ? is that important or it does not matter for the final task? - what is the impact of k on the generalization performance?  Also, the baseline algorithm SVM and logistic regression are not trained on the full dataset (this is easily feasible with an efficient SGD implementation like http://leon.bottou.org/projects/sgd). Moreover, I do not understand why no learning curve is reported for semi-supervised RDEs (only the point at 5K labeled samples is reported). Finally, you mention that the hyperparameters of the baselines have been tuned on the test set, which is usually discouraged as it could a very optimistic estimate of the generalization performance. It also raises the question of the methodology used for tuning of k, t and the regularization parameter 'ridge' for your technique. Could you detail your procedure and provide more detail on hyper-parameter sensitivity?",1,5134
"This paper rederives the fast dropout method which is a deterministic alternative to the originally proposed stochastic dropout regularization for neural networks. Then the authors apply fast dropout to recurrent neural networks (RNN) on two different datasets.   The main focus of the paper is the derivation of the fast dropout training with almost no link to RNNs until the experiment sections, which makes the paper consists of two disconnected components. The authors could have presented experiments on feedforward NN without affecting the smoothness of the paper at all.  The results are expected. Dropout, on small datasets, does better than baseline systems which doesn’t use any stochastic regularization and is as good as ones that does (Graves 2013). In table 2, it is mentioned that assumptions made in RNN-NADE system is also applicable to FD. The authors are encouraged to add these assumptions to get better results.  The results sections (3.1.2 and 3.2.2) still need more work. There are no analysis or discussion about the achieved results. Analysis of the effects of network structure, dropout rate, and other hyper-parameters are missing. The tables' order is reversed (table 3 is referenced first then table 2 and table 1).  For the results in 3.1.2, please clarify if there is a test data on which you measure NLL or you measure performance on the training data only (there is no training/test division menitoned for the music datasets).The paper applies the fast dropout of Wang and Manning to RNNs, obtaining excellent results on two standard RNN datasets.  This work also provieds an interesting analysis of fast droupt, presenting it as an explicit regularization.  Pros:  The results are good.    The paper convincingly shows that fast dropout is effective for training RNNs.   I like the equation E[dJ/da * da/dw_i] = dJ/dE[a] * dE[a]/dw_i + dJ/dV[a] * dV[a]/dw_i. It helps reason about stochastic systems such as this.    Cons:  The novelty lies almost entirely in the new analysis of dropout.   I also wish the analysis could tell us more about dropout. The analysis establishes that the variance may increase or decrease in response to delta_a. But what is it?  That's the most interesting part.  We need to know what happens to delta_a in order to be able to say nontrivial things about dropout.  Right now, the analysis says, it all boils down to delta_a.  But how  can we relate delta_a to situations that occur in practice?  I.e., can you say, delta_a is positive in situtaion a, and negative in situations b?   Perhaps the analysis already allows for such conclusions, but I don't see them, and so it would be nice to inculde something like that.    Also, it is worth stating that a global regularizer may not exist, and the contribution of dropout to the update may not be the derivative of any gradient.  There are a few typos: Page 6, sampling:   hat a = E[a] + s * sqrt{V[a]}, and not E[a] + s * V[a] as written in the text.  I worry that this may have caused an error in the remanider of the sampling section, but I am not sure because I didn't understand it fully.  Likewies, Eq. 1 should be E[a], not V[a].This paper is a rather straightforward extension of fast dropout to standard RNNs. Maybe too straightforward, it is not very groundbreakingly novel. However, I like the results, and the application of the method to RNNs is useful to know about for other researchers. The authors remark that a comparison for standard RNNs (rather than LSTMs) with adaptive weight noise is lacking -- why have the authors not tried this comparison I wonder, as it seems highly appropriate in this context and doable in their setup. There are a few points in the text that should be improved. For example, calling Graves' adaptive weight noise method MCMC is questionable in my view. Also, there are quite a few typos: abstract: absence -> absence first paragraph: sensitive initialisations -> sensible initialisations first paragraph: remove 'so-called' second paragraph: Contrary -> In contrast last par of sec 1: reason -> discuss first par sec 2.2.2: functionc ->function. sec 2.2.2: 'an subsequently' -> 'and subsequently' page six, line 11: 'were *** is defined as' -> 'where *** is defined as'",1,5135
"The paper describes a sparse coding model with complex valued basis functions. For training, it proposes to minimize reconstruction error plus penalty terms that encourage the amplitudes and phases of the basis functions to be smooth.  At first sight the model seems reminiscent of Cadieu, Olshausen (2012) [4]. But in that work, it is the coefficients that are penalized to have smooth amplitudes over time, whereas here, it is the basis functions themselves that are penalized to be smooth.  The model is applied to time-domain speech signals (one-dimensional data). The paper compares the results of complex valued sparse coding with smoothness penalties versus complex valued sparse coding without. The comparison shows that with penalties, basis functions seem to be more localized and filters within a pair tend to have quadrature relations.  Without penalties they do not seem to. I find this somewhat surprising because I would have thought that minimizing reconstruction error (plus orthonormalizing filters within each pair as suggested) would already achieve this, like it does in the case of images. The paper does suggest that sound data is fundamentally different from image data. I am curious what it is about sound data that causes it to require this extra machinery for learning complex basis functions. It would be very good to have actual results on images as a control.  This would also help disentangle two topics that are hard to separate in the paper, which are 1) fundamental  differences in sound data versus image data, and 2) learning complex bases with and without smoothness penalties.  I am wondering in what way the smoothness penalties are related to weight decay, or in what way they may just help find better local optima. It seems like this would be easy to check by initializing model A (no penalties) with model B (with penalties).  The title says 'natural sounds' but as far as I can tell, all experiments were done on a speech dataset. I'm not sure I completely agree with the statement that speech is a good enough proxy for natural sounds in general.  There are a lot of typos (e.g., 'Gramm-Schmidt', 'strucutre', 'analyzis').A sparse coding model of natural sounds (speech) is proposed. The signal is represented by a complex sparse coding problem with smoothness priors on both amplitude and phase. Learning and inference proceeds as in standard sparse coding. The method is analyzed in terms of statistics of complex pairs filters as well as denoising.  The method is not very novel. Complex sparse coding was already introduced in the past and the sparsity priors on the amplitudes and coefficients are a straightforward extension (or simplification compared to the work by Cadieu et al.).  Pros: - interesting application - fairly clear written paper  Cons: - insights may be good but I probably did not fully understood them. Why are sounds inherently different from images? Is it an artifact of how the experimental set up? Without sparsity/smoothness constraints, the problem is clearly underdetermined and therefore filters do not necessarily converge to quadrature pairs. - what is the contribution of this work compared to Cadieu et al? They had an extra layer, but the basic idea of smoothness of phase and amplitude is present also in that work. - empirical validation is not sufficient because:   - more quantitative results would be beneficial to assess the benefits of this model. For instance, the authors may want to compare and cite: Y. Karklin, C. Ekanadham, and E. P. Simoncelli, Hierarchical spike coding of sound, Adv in Neural Information Processing Systems (NIPS), 2012 - some parts need clarification   - eq. 9: why is this a good choice? wouldn’t it be better to have it bounded below?    - sec. 2.2 why rescaling the gradients when there are beta and gamma?    - in the compression experiment, shouldn’t the reconstruction error be taken into account?  Overall, this is interesting work. However, several clarifications are required in order to better assess novelty and to understand the method. Also, the empirical validation should be strengthened.This paper shows that imposing a prior over the basis functions in a complex representation of sound results in bases that are closer to hilbert pairs, with smooth amplitude envelope and linear phase precession.    It is not clear why imposing the prior directly on the basis functions is necessary.  If you  think of the complex pair as a phase-shiftable basis function, then it would make sense for the real and imaginary parts to be related by hilbert transform.  It makes me wonder whether the optimization was done correctly in inferring the sparse amplitudes - I.e., the phase must be allowed to steer to the optimal position, yielding a sparse representation.  It appears the gradients were computed with respect to the real and imaginary parts of the coefficients, rather than the amplitude and phase, which may be why the phase is not being properly  inferred.  The slowness prior on the phase doesn't make sense  - this would bias the bases toward low frequencies, no?   Some comment seems warranted.  The learned tiling in time-frequency doesn't make much sense.  What is causing the arching pattern?  It's not clear.  Most of all, it's not clear what we gain from this representation beyond previous attempts to learn a sparse representation of sound (Smith & Lewicki).  It would have been nice to compare coding efficiency and so forth against a purely real (vs. complex) representation.",1,5136
"One might think that everything has been said about motion estimation and motion energies, though refreshing ideas are always welcome even in subjects with thousands (ten of thousands?) papers.   The paper overall presentation and discussion are very clear and friendly.  When discussing multiplicative (2.3), isn't this saying we should be working with additive but in the log space? log space is very frequently used in image analysis.  The locality property is very interesting and as the authors claim, very powerful for computation. I believe is worth investigating for other applications and models.  Not all the mentioned results are the state-of-the-art for the used datasets, but this is not critical.  Does it make sense to report results for optical flow standard sets?  While I don't believe the paper is revolutionary, it is nice to read and has some interesting insights.The paper introduces a variation of common mathematical forms for encoding motion.  The basic approach is to encode first-order motion information through a multiplication of two filter outputs.  This approach is closely related to the motion-energy model and the cross-correlation model.  There is a lot of math leading up to a rather simple transform (after learning).  I would explain the resulting model as simply 'squared spatio-temporal Gabor filters followed by k-means pooling.'  The squared outputs are the components of the energy-based model (quadrature pair squared spatio-temporal Gabor filters are added in the energy based model) and k-means is used to pool first-order motion selective responses.  The more general problem of motion encoding needs to address the goal of motion selectivity and form or pattern invariance/tolerance.  As the authors point out, their squared outputs do not solve the motion encoding problem and the following pooling layer is intended to provide the solution.  The simplest next step would be to combine (add) two outputs to increase form invariance (this is the motion energy model).  Slightly more complex would be to group multiple squared outputs (these are the ISA models applied to spatio-temporal input).  The authors propose to use k-means for the pooling operation.  The results are validated on common action recognition computer vision databases.  I do not find the results surprising.  Given the very close similarities of the proposed algorithm to the Le et al. ISA algorithm it is not at all surprising that the results are nearly identical.  The training time improvements are also not surprising given the results from Coates et al. 2011.  This paper should probably be cited in regards to the learned filters: Olshausen 2003, Learning Sparse, Overcomplete Representations of Time-Varying Natural Images There are other results in the ICA community that should be cited, as they give similar results.  I am confused about some details on the model architecture used for SAE and SK-means.  The beginning of section 4 appears to only describe the SAE architecture.  What about the Sk-means architecture?  and how does the k-means in section 3.4 relate to the models evaluated in section 4?  My apologies if this is discussed somewhere in the text, I just can't find it in the places I expect.  Here are some suggestions/comments: The exposition is useful for bringing together many of motion models in the literature.  You argue for even-symmetric non-linearities, but couldn't a rectified linear unit also produce the desired effect? ; however it would not associate the contrast reversal, which may be the better inductive bias.  In the current form I remain uncertain what the benefit is of the proposed approach over the motion energy approach or ISA.  K-means does pool over more than 2 features, but this is to be expected given the architecture.  Can you show that 2-d subspaces are sub-optimal in terms of performance?  You do show that the covAE underperforms, why?  Is the Le et al. 2011 pipeline giving the major benefit?  In my opinion, the most novel contribution of the paper is the 'synchrony k-means' algorithm.  As far as I am aware this is a nice extension of the fast convergence results shown by Coates et al..  This general framework appears promising for the related problems of 'relating images.'  Therefore it could be the focus of the paper, with the SAE algorithm used merely as a control or motivation.  I am also skeptical about performance measurements of activity recognition as proxy for motion encoding.  These datasets likely suffer from various confounds not related to motion encoding.  Maybe a simpler test would be beneficial, scientifically?The paper introduces an algorithm to learn to detect motion from video data using coincidence of features between consecutive frames. While there are novel elements, the basic ideas are similar to papers of the same authors and the algorithm by other authors referenced in this paper.   Details: - The formula (20) looks remarkably similar the paper you reference (and that beats your performance) - sec 3.1 of 'Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis'. The only difference is mainly that you use sigmoid nonlinearity and they use square root.   - 4.2 - which auto encoder you are comparing to - there is a large number of them. In particular what if you just used k-means on concatenated frames? That would also detect 'coincidence' between frames. It would also train extremely quickly (and then use smoother version during inference as in Coates et al).  - In table 1 - is this on sequence of frames or pairs? (3.1,3.2 vs 3.3?)",1,5137
"Summary of contributions: 	Proposes to regularize auto encoders so that the encoded dataset has a similar nearest neighbor graph structure to the raw pixels. This method is advocated specifically for images.   Novelty: moderate (note: the authors seem to believe they are introducing the use of multi-layer auto encoders on images, but they are not) Quality of results: low - moderate Quality of presentation: low  Pros: 	Demonstrates improvements in clustering and semi-supervised learning performance Cons: 	Presentation is confusing and in many cases factually incorrect 	Presentation lacks motivation and reasoning about why the method should work 	Quantitative results are on small and obscure datasets, and improvements are relative to baselines of unclear value  Detailed comments:  Abstract: 	The abstract is ramble and not very focused. This is a conference on representation learning, we don't need you to explain representation learning and deep nets in the abstract. Focus on how you've changed the auto encoder. 	 	'we introduce the multiple- layer auto-encoder into image representation': definitely not true, you even cite papers from 5 years ago that use multi-layer auto encoders on images.  	The abstract should say something about what your new method actually is / does and why you think it is a good idea. I can't tell from the abstract what your method is except that you've changed auto-encoders in some way.  	'Extensive experiments on image clustering show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-word cases.' 	Be up-front about what your results are. What does encouraging mean?   Introduction 	Par 1 	f(H)? H hasn't been introduced yet. Do you mean f(X)? 	What do you mean by well approximate X? If H is meant to be similar to X, what's the point of switching to it? Do you mean it should preserve info about X?  	Par 2 	You cite a purely supervised method (Krivhevsky et al's ImageNet classifier) and then say 'Those methods normally need to use the auto-encoders to pre-train…' Not true.  	'It has been generally accepted as the consensus that the pre-trained network does provide a better representation for the original data.' Definitely not true! See for example Charle's Cadieu's work presented at ICLR last year.  	Par 3 	Typo: Locally Linear Embedding is LLE, not LEE 	 	Par 4 	What does 'weighted connected' mean? 	The wording of this paragraph is not especially clear, but I take it to mean you want f(x_1) to be near f(x_2) if x_2 is a nearest neighbor of x_1. Why do you think this is a desirable property? It's well known that Euclidean distances in images are not very meaningful. That's the whole reason we want to use representation learning on them.  Section 3 Graph Regularized Auto-Encoder 	Par 1 	This paragraph seems incredibly dismissive of the body of work that develops our understanding of auto encoders as learning manifolds, and how these manifolds relate to classification problems. I would say previous work such as the manifold tangent classifier definitely explores ideas related to the 'geometrical and discriminating structure of the data'  	Par 2 	This paragraph consists of nothing but the letter 'f'  Section 3.1  	Equations 4 and 5: 'sigmoid' should not be in all caps, that makes it looks like the product between variables s, i, g, etc.    (This comment applies throughout the paper, not just these equations) 	 	Equation 7: when you say V is 'the weight matrix' do you mean it is a weighted adjacency matrix describing which examples should be near each other? Usually in auto encoder literature people use 'the weight matrix' to refer to W_H or W_Q. If V is indeed this adjacency matrix you should describe how it is computed and what the weights mean. Even just putting in a forward reference to section 3.3 can help the reader be less confused.  Section 3.2  	You really do not need to spend so much space describing the extremely well-known concept of greedy layer wise pre training  Section 3.3 	OK, so V is the graph encoding matrix.  	3.3.1: Could you please explain the motivation for each of these? i.e., what effect you are hoping their use will have on the learning algorithm?  Section 4 	What is ORL? You should cite the publication that introduced it. Is this ORL? http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html It looks like the name has changed.  	You might want to get the datasets you work on added here: http://rodrigob.github.io/are_we_there_yet/build/ 	This will make it easier for reviewers to understand your work in context.  	In general I do not find these experiments very compelling because they are mostly done on small and obscure datasets. It's also not clear to me which of the baselines you ran yourself and which if any are taken from the literature. Baselines that you ran yourself are less compelling because you may not have the same familiarity with pre-existing methods as the inventors of those methods, and you certainly have less incentive to make them perform well.  	At a minimum, I'd like to see some more explanation of why the baselines you're improving upon are impressive. What would be better is to demonstrate good results on datasets that are used more frequently by people in the deep learning community. You are introducing a new kind of auto encoder so you should compare it to pre-existing auto encoders on datasets where auto encoders are frequently used, such as MNIST or Cover Type.  Section 4.1 	I notice you specify the hyper parameters for GAE and GNMF are optimized by grid search, but you have no mention of this for the SAE. Did you also optimize the SAE hyper parameters by grid search?  	You say that the GAE has 2 coefficients to be set by grid search, k and lambda, but it seems like there must be a whole lot of other values to set, such as the dimensionality of H. What did you do about these?  Section 4.2 	Footnote 3: why is a single sample per class 'meaningless'? I agree it's really hard to do well in this case, but why is 1 sample totally worthless and 2 acceptable? If you've added more labels, isn't your work no longer comparable to previous work on the same data set?A new auto-encoder method is proposed. Features are learned by minimizing the sum of the square reconstruction error and a penalty on feature dissimilarity weighted according to a variety of criteria. The basic idea is to weight more strongly the features that are in the neighborhood of the training sample or based on the class labels. A multi-layer version of the autoencoder is proposed by following the layer-wise training protocol. The method is tested using several metric on a few small datasets.  Strengths The problem is relevant and interesting. The method is technically sound.  Weaknesses The paper lacks clarity. It does not read well and the language is often vague (what does it mean “represent well” or “positive impact” or “hurt numerical optimization”?). The paper is also rather incremental. The idea is similar to [6] but also related to methods for dimensionality reduction like deep parametric t-sne (see R. Min, L.J.P. van der Maaten, Z. Yuan, A. Bonner, and Z. Zhang. Deep Supervised t-Distributed Embedding. In Proceedings of the International Conference on Machine Learning (ICML), pages 791-798, 2010 ) and methods like H. Mobahi, R. Collobert, J. Weston. Deep Learning from Temporal Coherence in Video. ICML 2009. In this light, it would be valuable to add a discussion of the advantages of this method versus [6], for instance. The major difference is that the reconstruction error replaces the “pull apart” term in the loss function. What’s the advantage of having an explicit decoder? Doesn’t it introduce even more parameters in the model? Finally, the empirical validation could be more convincing if the authors used larger datasets where many other authors already benchmarked (e.g., cifar, mnist, timit, svhn, to mention a few).An algorithm is presented to generate a representation for unsupervised and partially labelled data  with graph-based regularization to keep nearest-neighbors close. English is not clear in some places.   I found the whole paper hard work to follow.   Results tables don't give column headings. It wasn't clear what the variable was - the dimension?  It would be clearer to break out the 'error measure' into a separate column.  The two metrics used weren't very clear to me - the mapping from unsupervised k-means clusters to class labels  How does it compare to a simple 1-NN hinge-loss technique similar to those used by Weston e.g. for training joint embeddings?  http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf  What parameters eta, lambda $k$ (for the graph construction) were chosen? How did this $k$ affect the results?  How expensive is the proposed technique compared to the alternative?",1,5138
"This paper proposes to use deep convolutional neural network (DCNN)combined with ranking training criteria to attack the multi-label image annotation problem. DCNN is now widely used in image classification (annotation) problems. Applying it to multi-label image annotation problem is a natural extension of prior arts. The combination of DCNN with the ranking training criteria to solve the multi-label annotation problem, however, is new and is the main contribution of the paper.  The authors evaluated the proposed approach on the NUS-WIDE dataset, which is considered the largest multi-label image dataset available. They compared the proposed approach with baselines that use manually designed image features and showed that the proposed approach outperforms the baseline by 10%. They claim that this is mostly due to the features learned from DCNN. They also compared several different ranking criteria and demonstrated that the weighted Approximate Ranking (WARP) criterion performs the best.  While their results are not surprising given the recent success of DCNN on image classification tasks, this paper does show a novel usage of the DCNN on the multi-label image annotation problem.   If the paper is to be improved, I would suggest to include published results on the same task as part of the baselines. This allows readers to understand better the position of the proposed approach.* A brief summary of the paper's contributions, in the context of prior work. Paper considers several loss functions for multiclass label annotation.  * An assessment of novelty and quality.  They have done good job, and ran experiments on the proper, large scale, however work is not very novel (or creative).  * A list of pros and cons (reasons to accept/reject). pros: - Gives reasonable advice, which loss function use for multi class image annotation.  cons:*Summary* This paper proposes to use convolutional networks for image annotation. Great care is given to the selection of an appropriate loss function as well as the comparison with reasonable baselines over the NUS/Flickr dataset. The paper reads well, gives enough context and references to related work. It reports improvement with respect to the state of the art. In my opinion, this is a good paper, with the only drawback that the evaluation is conducted over a single dataset, with a vocabulary of only 80  tags, which is small compared to realistic application.   *Detailed review*  I would like to clarify only two points regarding (i) small tag vocabulary in NUS, (ii) relationship with imageNET classification.    (i) the vocabulary of NUS/Flickr is only 80 different tags. This is very  small compare to web annotation or even personal photo gallery annotation. In particular, the fact that your network has 80 outputs make the evaluation of the output score of every tag for every forward/backward step very  inexpensive (compared to evaluating the rest of the network). This is very different from the initial conditions in which the WARP loss was introduced. Loss functions which does not rely on sampling can perfectly be used and might be better. I notably think at  T. Joachims, A Support Vector Method  for Multivariate Performance Measures, Proceedings of the International Conference on Machine Learning (ICML), 2005 or Ranking with ordered weighted pairwise classification from Usunier et al 2009. More fundamentally, I feel that focussing on 80 tags hides most of the interesting challenges in real tasks: reasonable 10k vocabularies implies greater perplexity and therefore require greater performance for the CNN. They also suggest giving greater importance on tag coocurences and language modeling to understand unlikely predictions like ocean and lake tag in the same image from your example.   (ii) I appreciate that you highlight the difference between annotation and classification, and that you want a model trained from scratch for fair comparisons (Section 2.1). However, the CNN trained over ImageNET of [20] or subsequent work has spurred hopes for a universal vision machine. If large  CNNs trained over 1k and 20k imagenet were available to you, it might be interesting to evaluate how a NUS model initialized from those would perform. This would be an additional result which would not replace the network trained from scratch but rather analyze the reusability of the  imageNET network and give perspective on the importance of the imageNET breakthrough.   *Comments along the text*  Page 2. 'parametric model might not be sufficient to capture the complex distribution of the data' this sentence should be removed. Parametric model can model complex distribution for non linear problems. Use a different wording to introduce that nearest neighbor approaches are competitive.  Page 3. 'staircase weight-decay' I am not familiar with this name, which is rather explicit though. You might want to sprincke references over neural net specific terms to allow other ML and core vision people to read your paper. E.g. references after momentum, asynchronous SGD, staircase weight  decay might help.   Page 3 'posterior probability of an image x_i and class j' the wording is wrong, it should read posterior of class j given image x_i.  Page 5 'weight kNN' should read 'weighted kNN'",1,5139
"Summary of contributions: Studies the effect of # of parameters, # of layers, and # of units on convolutional net performance. Uses recurrence to run nets that have e.g. more layers but not more parameters in order to distinguish the effects of these three properties.  Novelty: moderate Quality: low  Pros: 	-Nice empirical demonstration that more parameters helps. Cons: 	-Does not study dropout. I think dropout is really important for this kind of paper, because dropout has a strong effect on the optimal model size. Also, dropout is crucial part of the state of the art system on both CIFAR-10 and SVHN, so it seems out of place for a paper on how to set hyperparameters to good performance out of a neural net to disregard one of the most important techniques for getting good performance. 	-Insufficient tuning of hyperparameters. 	-Support for the claims in the abstract seems weak, with many experiments going against the claims 	-The stated goal is to provide guidance for how to set hyperparameters so that practitioners don’t have to resort to trial and error. But I don’t really see anything here that prevents that. For example, Fig 4a shows standard U-shaped curves for the # of layers hyperparameter. The paper says “adding more layers tends to increase performance” but this is only true on the left side of the U! The whole point of trial and error is to figure out where the bottom of the U is, and this paper completely ignores that. 	-The kind of parameter tying considered in this paper is not one that is typically used in practice, at least not for this kind of problem. The conclusions are therefore not all that helpful. i.e., the authors introduce a new form of parameter tying, and then show it isn’t useful. We don’t need to publish that conclusion, because no one is using this useless form of parameter tying anyway. 	-The authors don’t investigate the effect of the tiling range of tiled convolution, which is a form of control on the degree of parameter sharing that people actually use. It’d be much more interesting to study this form of parameter sharing. (This paper feels a bit like it started off as a “new methods” paper advocating convolutional recurrence, and then when the new method didn’t perform well, the authors tried to salvage it as an “empirical investigation” paper, but the empirical investigation part isn’t really targeted at the methods that would be most useful to study)  Detailed comments:  1.1 Related work:  	You should also mention “Multi-Prediction Deep Boltzmann Machines”, Goodfellow et al 2013. This paper uses recurrent networks on the image datasets MNIST and NORB. Like DrSAE, it is discriminative. It may be interpreted as a form of autoencoder, like the methods you mention in the second paragraph.  	 2 Approach: 	Your approach is definitely not the first to use recurrence and convolution in the same model. It’s probably worth discussing similarities and differences to Honglak Lee’s convolutional DBN. He describes performing mean field inference in this model. The mean field computations are essentially forward prop in a convolutional recurrent architecture, but the connectivity is different than in yours, since each update reads from two layers, and some of the weight matrices are constrained to be the transpose of each other rather than being constrained to be equal to each other.  	It’s also probably worth discussing how you handle the boundaries of the image, since this has a strong effect on the performance of a convolutional net. Since you say all the layers have the same size, I’m assuming you implicitly pad the hidden layer with zeros when running convolution so that the output of the discrete convolution operation has the same size as the input.  	 2.1 Instantiation on CIFAR-10 and SVHN  	I don’t know what it means to put the word “same” in quotes. I’m assuming this refers to the zero padding that I described above, but it’s worth clarifying.  2.2 	I think it’s fairly disappointing that you don’t train with dropout. 	How did you choose this one fixed learning rate and momentum value? How do you know it doesn’t bias the results? For example, if you find that deeper models are better, are you really finding that deeper models are better in general, or are you just finding that deeper models are more compatible with this specific learning rate and momentum setting? 	It seems especially important to tune the learning rate in this work because varying the amount of parameter sharing implies varying the number of gradient terms that affect each parameter. The speed at which the parameters move is probably much higher for the nets with many recurrent steps than it is for the nets with no recurrence.  3.1 	“That we were able to train networks at these large depths is due to the fact that we initialize all W to the identity” -> it’s not obvious to me that it should be hard to train convolutional rectifier networks at most of these depths. For example, Google’s house number transcription paper submitted to this conference at the same time trains a 12 layer mostly convolutional network with no mention of network depth posing a challenge or requiring special initialization. The maxout paper reports difficulty training a 7 layer rectifier net on MNIST, but that was densely connected, not convolutional. Was it only difficult to train the recurrent nets, or also the untied ones? This is important to explain, since if the recurrent nets are significantly harder to optimize, that affects the interpretation of your results. 	Are the higher layer weights for all of the networks initialized to the identity, or only the ones with tied parameters? Is it literally identity or identity times some scalar? If it’s literally identity rather than identity times some scalar, it might be too hard for SGD to shrink the initial weights and learn a different more interesting function. Have you tried other initializations that don’t impose such a strong hand-designed constraint, such as James Martens’ sparse initialization, where each hidden units gets exactly k randomly chosen non-zero incoming weights? This initialization scheme is also described as making it easier to train deep or recurrent nets, and it seems to me like it doesn’t trap the recurrent layer as being a fairly useless memory layer that mostly functions to echo its input.  	“Likewise, for any given combination of feature maps and layers, the untied model outperforms the tied one, since it has more parameters.” I don’t agree with the claim that the untied model performs better because it has more parameters. This would make sense if the tied model was in the underfitting regime. But you have already said in the same paragraph that many of the tied models are in the overfitting regime. If you look at fig 2. there are several points where both the tied and untied model have 0 training error and the tied model has higher validation set error. If the correct story here is overfitting due to too many parameters, then the untied model should do worse. I suspect what’s going on here is something like the identity initialization being a bad prior, so that you fit the training set in a way that doesn’t generalize well, or maybe just your choice of a single momentum and learning rate setting for all experiments ended up benefiting the untied model somehow. For example, as I said above, the recurrent nets will generally have larger gradients on each parameter, so maybe the high learning rate makes the recurrent net adapt too much to the first few minibatches it sees.   	Fig 2 		In the abstract you say “for a given parameter budget, deeper models are preferred over shallow ones.” It would be nice if on the plot on the left you evaluted points along the parameter budget contour lines instead of points on a grid, since the grid points don’t always hit the contour lines. As is, it’s hard to evaluate the claim from the abstract. However, I don’t see a lot of support for it. The best test error you get is toward the bottom right: 0.160 at the rightmost point in the second row from the bottom. Of course, this is the only point on that parameter budget contour, so it may just be winning because of its cost. However, if I look for the point with the most depth, I see one with 0.240 near the 2^18 contour line. At the bottom right of this contour line, the shallow but wide model gets 0.205. 		Overall, here is my summary of all your contour lines: 			2^16: only one point on it 			2^17: Contradicts claim 			2^18: Contradicts claim 			2^19: Contradicts claim 			2^20: Supports claim (sort of, points aren’t that close to contour line) 			2^21: Supports claim (sort of, points aren’t that close to contour line) 			2^22: Supports claim (sort of, points aren’t that close to contour line) 		So it seems to me that this plot contradicts the claim from the abstract at least as much as it supports it.  	Right figure: 		This supports the claim in your abstract.  	Table 1: While it does make sense to compare *these* experiments against methods that don’t use dropout or data augmentation, I don’t think it makes sense for these to be your only experiments. I think the case for excluding data augmentation from consideration is getting very weak. There is now a lot of commercial interest in using neural nets on very large datasets. Augmentation of small datasets provides a nice low-cost proxy for exploring this regime. As far as I know, the main reasons for not considering data augmentation are 1) data augmentation requires knowledge of the data domain, in this case that the input is an image and the output is invariant to shifts in the input. But you are already exploiting exactly that same knowledge by using a convolutional net and spatial pooling. 2) gaining improvements in performance by improving data augmentation techniques distracts attention from improving machine learning methods and focuses it on these more basic engineering tricks. But I’m not asking you to engineer new data augmentation methods here; you can just use exactly the same augmentation as many previous authors have already used on CIFAR-10 and SVHN. I don’t think there is any valid case at all for excluding stochastic regularization from consideration. It doesn’t require any knowledge of the data domain and it is absolutely a machine learning technique rather than just an engineering trick. Moreover, it is computationally very cheap, and state of the art across the board. By refusing to study stochastic regularization you are essentially insisting on studying obsolete methods. The only regime in which stochastic regularization is not universally superior to deterministic backprop is in the extremely large data domain, which as academics you probably don’t have access to and you are also actively avoiding by not using data augmentation.  	Fig 2 and 3 in general: I understand it’s too expensive to extensively cross-validate every point on these plots, but I think it’d be good to pick maybe 4 points for each plot (maybe the upper-left and lower-right of two different contour lines) and run around 10 experiments each for those 4 points. Overall that is 80 training runs, which I think is totally reasonable. The current plots are somewhat interesting but it’s hard to have much confidence that the trends they indicate are real. Obtaining higher confidence estimates of the real value of a small number of points would help a lot to confirm that the trends are actually caused by the # of feature maps and depth rather than compatibility with a fixed optimization scheme.  Section 4: 	I don’t think the “received wisdom” is that more depth is always better, just that the optimal depth is usually greater than 1. 	You say your experiments show that more depth is better for a fixed parameter budget, but doesn’t Fig 2. (right) contradict this?An analysis of deep networks is presented that tries to determine the effect of various convnet parameters on final classification performance.  A novel convnet architecture is proposed that ties weights across layers.  This enables clever types of analysis not possible with other architectures.  In particular, the number of layers can be increased without increasing the number of parameters, thus allowing the authors to determine whether the number of layers is important or whether the number of parameters is important independently.  (Normally, these are confounding factors that are hard to judge separately.)  Several experiments are proposed that independently vary the number of maps, number of parameters, and number of layers.  It is reported that while the number of maps appears to be irrelevant in this setup, the number of layers and number of parameters are very important [more is better!]  This is a pretty unorthodox but interesting form of analysis.  I think it is worth highlighting that “number of layers” experiment, since I didn’t see immediately how you could do this with another tying strategy.  The results confirm our intuitions about important parameters, but also suggest that perhaps weight-tying spatially is one place for improvement.  I am a bit worried that there are caveats to this analysis that could be better analyzed.  For example, section 3.2.2 shows the “untied” system working better than “tied” in 3.2.2, but could this have more to do with the finicky nature of recurrent models (e.g., failure to find a good minimum) than the number of parameters?  Section 3.2.3 might implicitly address this:  for fixed number of layers and parameters, the tied model performs about the same.  If it had performed worse again, we might have been tricked into thinking that the number of maps mattered, when it could have implied that the tied model itself was a worse performer.  A bit more clarity about the potential caveats of this analysis and the implications of each experiment would help.  One experiment I was surprised not to see:  holding M and P fixed, compare a tied model with L layers to an untied model with fewer layers [to keep P constant].  This is apparently not what is done in 3.2.1, but might help address my concern above.  Pros:  Clever, novel analysis of interplay of deep network characteristics and their effect on classification performance.  Useful rules of thumb that may benefit future work.  Appears to confirm widely-held intuitions about depth and scale of models.  Cons:  The analysis method might be introducing effects that are not clear (e.g., the effect of using recurrence on the optimization problem).  Hard to know how these results will transfer to more typical convnets that use max pooling, LCN, etc.  Other:  In much of the analysis I thought it may be more useful to consider training error as the main metric for comparing networks.  At this point, being able to achieve low training error is the main goal of fiddling with the model size, etc. and testing error/generalization is governed by a different bag of tricks [synthetic data, Mechanical Turk, unsupervised learning, extensive cross-validation, bagging, etc.]This paper analyzes the effect of different hyper-parameters for a convolutional neural network: the number of convolutional layers (L), the number of feature maps (M), the total number of free parameters in the model (P).  The main challenge is the tight relation between these three hyper-parameters. To study the effect of each factor independently, a recurrent architecture is proposed where weights are tied between different convolutional layers so that the number of layers can be varied without changing the total number of parameters. Pooling is only applied to the very first layer but is not applied in any of the tied layers on top.  While It is important to see experimental papers that offers analysis of the effect of different design parameters for neural networks like this one, weight tying across different convolutional layers is a bit artificial for this task (you scan the whole image at once in each layer).   The main take home message of this paper is that varying the number of feature maps is not important given that the number of free parameters and model depth are held constant. It is important to add experiments/arguments that show if the same conclusion holds when pooling is used for example.   General comments: - What are the kind of features learned by the tied network vs the untied (normal) one? Does the network work around tying by dedicating some features to be used most of the time per specific hidden layer but not by the others? (as if it is working in an untied regime but with smaller number of feature maps per layer).  - Don’t think table 1 is needed because the paper is not aiming at achieving the best ever accuracy but rather exploring different factors affecting performance.  - Regarding writing, the paper gets repetitive at points, for example, the information in table 2 is stated in a paragraph in page 7. The same conclusions are stated in the same way multiple times.  - In page 8 “We then compared their classification performance after training as described in Section 2.2.” Is there something specific you mean by “after training”?",1,5140
"* A brief summary of the paper's contributions, in the context of prior work. Papers suggests to stack multiple machine learning modules on top of each other. Applies it to SVMs.  * An assessment of novelty and quality.          Not novel, quality is low.  * A list of pros and cons (reasons to accept/reject). pros : Exploration of non-standard deep architectures. Good direction of research.  cons : Paper suggests randomly establish group of features to find correlated groups. This task might be extremely expensive and infeasible.   Very poor experiments. One experiment on synthetic data, which is trivial to learn (sum xi) ^ 2, and others on unknown datasets.  It is unclear where is non-linearity coming from. If this are just SVMs stacked on top of each other, and there is no non-linearity in between ? Then entire procedure is just a linear classifier with regularization.   Paper doesn’t state what is optimization objective of the entire system. It just brings an algorithm.  Dimensionality of data is extremely small ~25 dims.This paper presents a tree-structured architecture whose leafs are SVMs on subsets of attributes and whose internal nodes are SVM taking as input predictions computed by the children.  The generality of the method is unclear: you seem to implicitly assume regression problems but this is never explicitly mentioned. Could the method work for classification?  The significance seems modest to me. First, it is unclear in what sense the algorithm performs feature learning as the intermediate layers contain in facts predictions. Additionally, all the experiments use a linear kernel and thus from what I understand from Section 3 the tree in Figure 1 computes a linear function of its input. Clearly I must be missing something otherwise this would not be a deep learning system at all. But the presentation should be improved to clarify this. The notation is also confusing, for example does overline{y} denote a mean? Over what quantities exactly (targets or predictions)? Pseudo-code 2 seem to contradict Figure 1 as each SVM is trained on inputs x (again probably a notational issue).  The experiments are only preliminary and based on small data sets. The reported R and hat{R} seem to be unnormalized, why?The authors propose a hierarchical SVM approach for learning. The method uses the prediction of lower layer SVMs as input to the higher-layer SVMs. Adaptive methods are proposed where lower nodes are updated incrementally in order to improve the overall accuracy.  I could not find what the proposed algorithm exactly does and what is the exact function implemented by the feature graph architecture. I assume that the authors use linear SVMs for regression. Then, should not a combination of these linear SVMs also be linear? Or is a nonlinear kernel being used? The overall lack of specification of the proposed approach, makes it difficult to compare with existing alternatives (hierarchical kernels, boosting, neural networks).  The paper is rich content-wise, including generalization bounds and stability analysis. Authors derive a generalization bound for the feature graph architecture, showing that surplus error grows linearly with the number of modified nodes in the graph. The generalization bound does not seem to be very tight, as authors show empirically that generalization is on par with basic SVMs.  The authors propose maximizing feature correlation in the first layer as a heuristic to construct the feature hierarchy. This is generally a good idea, but I am wondering whether having only one output per group of correlated features is sufficient in order to learn complex models.",1,5141
"An architecture for labeling neural components in electron microscopy images is presented based on a combination of unsupervised feature learning and supervised classification.  The proposed system learns a basic feature representation using vector quantization applied to small patches.  Several strategies for combining the extracted features into a hidden representation are proposed, and these features are passed to a supervised learning stage to predict an “affinity graph” which encodes the segmentation of image components.  Subsequent stages of features are generated by computing additional features from the predicted affinity graph and the original input image in the hope of learning features that better represent the output domain.  Several modifications are proposed to improve performance, including a “hard example mining” strategy to up-weight regions of input images that are difficult to segment properly.  Results are presented on a test set, with validation numbers provided for numerous variations of the proposed architecture.  It is shown that the best system widely outperforms a baseline conv-net on many quality metrics.   This paper covers an interesting application domain that requires a reasonably scalable learning approach as well as some novel components to properly predict the structures desired.  In that direction, it may be interesting to other experts wanting to achieve maximum performance on this application.    Other than the CNN baseline, it is hard to know how difficult this problem is, since the dataset is apparently novel.  On the other hand, the paper takes a “kitchen sink” approach to selecting an appropriate algorithm (as evidenced by the complex pipeline and extensive appendix with additional variations).   It is, thus, difficult to know whether the improvements are coming from these particular innovations or from some other source.  For example, in some cases it is clear that the addition of multi-scale or more layers of features reduces the training error significantly and it could be that other more orthodox architectures could achieve similar results.   What insight should a reader interested in other domains distill from these results?   Pros:  Interesting application involving a structured output, though similar to image segmentation. Authors cover a wide search through a novel type of architecture to yield a successful labeler.  May be valuable to domain experts.  Cons:  A complex pipeline that combines many components [from prior art], making it hard to see what is contributed outside of this particular application. Difficult for non-expert to judge quality of results.   Other comments: The addition of dropout appears to enhance generalization (along with several of the other modifications).  This might help the CN baseline as well.  It may be useful / interesting to understand what explains the improved generalization.  (E.g., is the CN overfitting, and this avoided by the use of unsupervised training?)This paper presents the application of a composite classification system to the classification of 3D microscopy scans.  The problem is formulated as predicting an affinity graph (binary classification 'pixels belong to same foreground object' vs other cases). The classifier is a composite construction from supervised and unsupervised methods: vector quantization, pooling, subsampling, whitening and a 1-hidden-layer neural network.   Alternative classifier architectures are explored (using multiscale or single scale VQ,   These classifiers are stacked ('recursively') with the resulting classifier performing better, though it is not clear where the improvement comes: increased depth or increased field of view.  How does this work if you make the classification MLP deeper?   I would like to see comparisons of the different techniques in terms of number of parameters.   Describes an interesting composite architecture on a specialist problem, with 3D data. It is interesting to see the comparison of this composite classifier approach and the effectiveness of stacking.   Pros:  Interesting comparison of a more hand-built approach to a CNN. Interesting to see application to 3D data Interesting to see the effectiveness of stacking.   Cons:  A somewhat specialist problem (and proprietary data) limit the audience and applicability of the results.This work presents an approach for 3d imaging analysis that tries to improve on previous work by addressing the limiting aspects of the existing methods. The paper presents 3 main contributions: a fast-training wide network, a recursive approach, and a weighting scheme to pull the focus of the training towards harder cases.  This paper is well written, well structured and flows well. The results show that the methods proposed  improve performance on the 3d image annotation task. The 3 contributions are tested and stacked one-by-one and each is shown to improve performance.  Genereal comment: Can the improvements proposed in this paper be applied to 2d images as well? It would have been interesting to see an application to a benchmark dataset to have a comparison with current state-of-the-art methods.  Pros:  - Well justified improvements that show clear improvements.  - Open source software is presented.  Cons:  - Many metrics are presented, though, as a non-expert, it is hard to judge which metrics are more relevant.",0,5142
"This paper studies the variance in the results of neural network training on a wide variety of conﬁgurations in automatic speech recognition. It raises the question on how to compare two deep learning models when it's difficult and sometimes impossible to run many experiments.  This variance problem is well known. The main contribution of this work is running many empirical experiments to demonstrate the problem in ASR and alerting readers on the right way to compare two different models. I think it can bring some values to the community. However, if a practical formula for comparing different models can be provided the significance of the paper would be greatly improved. This paper shows experimentally that error bars for the exactly same NN models trained with exactly same data but with different initial seeds of pseudo-random number generators (initial weights, data shuffling, etc.) can be really significant. The range between best and worst models often exceeds rather genuine and incremental improvements reported in ASR field in recent years (which itself are rather inflated, since if we were improving by ~10\% relative each year, ASR problem would be solved long time ago - this is just a digression, not a complaint regarding this particular paper of course). The findings are not new and well known to many who trained NNs but I think worth reminding, and a workshop is a very good venue for it.  I know you reference papers for your model configurations (sorry, my fault I didn't had a chance to check if they contain this information) but it would be a good idea to put more details on how exactly you initialise the models - what range of initial parameters, is pre-training used, etc. -- since this paper is mostly about this aspect, it would be nice to have them in-place.  One criticism I have is the experiments investigate only sigmoid models, which are known to be particularly sensitive to weights initialisation, which can heavily affect training dynamics in deep models. It would be really nice if you could try if (and to what extent) this issue persist with piece-wise linear units.  Also, since you already have this, could you plot or write somewhere a short comment whether and to what extent training objective is correlated with the obtained WERs, is it at least monotnous? Otherwise, even if one was able to derive some uncertainty bounds on the NN outputs, this still would be an unsatisfactory predictor of WERs, at least with CE criterion.  Regarding this sentence ""Interestingly, the starting point seems to be much more important than the network quality used to generate the lattices"" is not that surprising to me. Any denominator lattices will do, given the right paths (or right kind of mistakes) are in them, and those models are likely to make similar ones anyway.   It is also interesting and somehow a counter example of claims (of, by the way excellent paper, of Choromanska et al.,2014) that for sufficiently large models, it is rather hard to end up in poor or saddle point minima, and that most local minima will do a good job. It's subjective, but perhaps 15.5% for SWBD (the worst minima you report) isn't that bad at all.  Fig. 1 typo in the word cross-entropy *loos*This paper presents a detailed analysis of the random effects of the training sample order and weight initialisation on the WER of two ASR tasks.  The interesting finding is that this randomisation can in fact lead to substantial changes in performance -- such that similar magnitudes of variation would often be taken as significant differences between algorithms.  This is an important, and perhaps shocking finding.  The results are given added credibility by the use of state-of-the-art ASR systems in all work.  The paper is accompanied by an thorough and interesting analysis.  Particularly interesting is the comparison of the random effects of training data ordering vs. network initialisation.  Of course, one negative aspect, as the authors note themselves, is that this study is extremely computationally expensive, and for real practical benefit to come from it, more efficient methods would need to be found.  However, this in no way diminishes the interest of this timely work.  ",1,5143
"This paper introduces a new form of Normalizing Flows, the Inverse Autoregressive Flow,  that is constructed by inverting an autoregressive map. The resulting maps are easier to scale compared to previously proposed normalizing flows and autoregressive posterior networks. These flexible maps are used to equip VAEs with better inference networks.  The originality of this paper lies in the key observation that, while being as flexible as Autoregressive maps, Inverse Autoregressive maps do not require sequential computation and can be easily parallelized. Furthermore, as for Normalizing Flows, it is cheap to compute the log-det-Jacobian terms for Inverse Autoregressive Flows. These terms are necessary to compute the likelihood of transformed samples.  The paper is well written and clearly explained. Perhaps the order of its narrative could be improved a bit by first introducing the challenges of  flow-based inference models (e.g. sequentiality in autoregressive models and the usual O(d^3) cost in computing log-det-Jacobian terms). Once these challenges are explained, the authors could then explain how the Inverse Autoregressive Flows address them and only then show that they can be interpreted as the inverse of an autoregressive map.  The authors show that training convolutional VAEs with the Inverse Autoregressive Flows as inference network results in model with better log-likelihoods.  Instead of reporting the bound and the estimated likelihoods in Table 1, it would be good to show the estimated likelihoods and the KL-divergence between the true posterior and the variational posterior (difference between the variational bound and the estimated log-likelihoods). In this way, the readers could see more easily that the KLD between the true posterior and the variational posterior is actually smaller.  The idea introduced in this paper lives in the intersection of previous work such as Normalizing Flows, NICE and MADE. But the particular instantiation of the model introduced in this paper constitutes a significant improvement of previous work in terms of scalability and general applicability to amortized inference.This paper proposes to use the Gaussianization transform of an autoregressive model as an efficient way to implement the encoder of a variational autoencoder.  The proposed approach makes a lot of sense to me. Both the inverse / Gaussianization transform of a well-working autoregressive model and the posterior of a good variational autoencoder with Gaussian prior should on average map to an approximately Gaussian random variable, so it is reasonable to assume that the former is a useful building block for the latter.  The comparisons seem a bit limited but appropriate for a workshop paper.Pros: Authors extend normalizing flows to a more powerful family of function - autoregressive functions. Importantly despite autoregressive structure the computations are all parallel which is obtained by running the autoregressive transformation in the opposite direction. Second authors did a large amount experimentation with variational auto encoder architectures and obtain a very good performance in cifar dataset both with and without the inverse auto-regressive flow.   Cons: While they did explain the network details in the text, it is hard to parse it accurately enough to be able to reproduce. It would be good if they wrote a table or diagram with all the transformations and all the details. They can also publish the code.   Additional Comments:  It would be good to emphasize which z is fed into the decoder z_K or z_0 (I assume z_K).   About the equivalence in section 4.4: The autoregressive prior model is a different model then the model of this paper. One can indeed change coordinates of the model of this paper so that the prior is autoregressive and approximate posterior is diagonal, but then one also has to change the model so that it undoes the autoregressive prior - defeating the purpose of autoregressive prior - making cheap deep generative model (deep “along the layer”)",1,5144
"This is a short paper that studies density ridges in manifolds using the framework proposed by Ozertem and Erdogmus: it preprocesses the data using PCA, identifies density ridges by following the principal eigenvector of the local manifold Hessian, and projects the data onto the ridge estimates using an approach proposed in prior work by Dollar et al. Embeddings and interpolation results are shown on the MNIST and Frey faces dataset.  Although I may have misunderstood parts of the proposed approach due to the brevity of the submission (even in the short workshop format, I believe it possible to provide a bit more details), the novelty of the paper appears limited: it is a straightforward combination of prior work by Ozertem and Erdogmus and by Dollar et al. The paper presents no comparisons with prior work (neither experimental nor conceptual), which makes it difficult to gauge the contribution of the paper. In particular, it remains unclear what the goal of this line of work is. Is it to learn better feature representations from data? In that case, the study should present experiments aimed at evaluating the quality of the learned representation; the visualizations in Figure 1a, 2a, and 3a do not achieve this goal (in particular, since it is known that non-parametric techniques such as t-SNE can produce scatter-plot visualizations of much higher quality than PCA). Or is it to learn better models for image generation / interpolation? In that case, the study should develop methods to evaluate the quality of generated images, and perform comparisons with techniques that try to achieve the same (GPLVMs, mixtures of Bernoulli models, fields of experts, generative-adversarial networks, etc.).  Overall, I believe this paper is of insufficient novelty and quality to be accepted at ICLR.    Minor comment: ""As long as the embedding space is of higher dimension than the manifold a linear method causes no harm."" -> If by the dimensionality of the manifold the authors mean its intrinsic dimensionality, then this statement is incorrect. For instance, consider a manifold that is one-dimensional space-filling curve living in a 10-dimensional space. The dimensionality of the manifold is one, but a linear method needs to preserve all 10 dimensions in the data to prevent distant parts of the manifold from collapsing.The paper proposes to perform image synthesis or reconstruction with help of a manifold that capture the image shape variations. A manifold is estimated from the data, and then synthesis is performed. Manifold models are reasonable in some image reconstruction problems, and often provide elegant solutions.  The ideas and results in this short paper are correct. However, the paper does not present any novelty unfortunately: the problem, the framework are not new. And the tools used for manifold learning, or for image reconstruction are classical too.   Due to the very limited novelty, this paper is unfortunately below the threshold of acceptance for ICLR. ",0,5145
"This paper presents an intriguing idea for training MRFs (or CRFs) together with an inference network by training the inference net to minimize a variational upper bound on the partition function. Unlike standard variational inference, this bound is in the right direction for learning. Compared with methods like tree-reweighted BP, this method potentially allows for more accurate bounds, as the approximating distribution can be made arbitrarily close to the true one.  I'm uncomfortable with referring to (1) as a ""variational upper bound"" on the partition function, since the stochastic estimate is an upper bound only in expectation, and may underestimate the true value with overwhelming probability. (E.g., consider the case where q is uniform and p is peaked.) Similarly, (2) is technically a lower bound on the likelihood, but when estimated with samples from q, it may overestimate the true value with overwhelming probability. I'm not sure what the proper wording would be, but I think the abstract over-promises as currently written.  As the authors point out, Monte Carlo estimates of (1) and (2) could have very high variance, just like importance sampling based estimates of Z. I suspect this problem would be very hard to overcome on full-size models.   Still, the idea is quite neat, and could be the basis of future work on training MRFs/CRFs. I would certainly recommend acceptance to the workshop track.  This paper presents a simple and intuitive approach to variational inference for undirected graphical models. The paper is concise, and the bound is derived quite simply based on the variance of an importance sampler that estimates the marginal likelihood. (This unbiased estimate brings to mind pseudomarginal samplers.) The linearity bound on the log expectation also makes sense in order to derive a lower bound which produces unbiased stochastic gradients during optimization.  I am concerned however with the scalability of the approach to both larger data and higher dimensions (the preliminary experiment is very small in both respects). The approach seems to share the downfalls of importance sampling in general. It is unclear the extent to which recent adaptive importance sampling techniques applied to variational inference are practical (unlike Burda et al. (2016) for example, there is not a baseline to say that at the least, it does not produce a worse bound than the typical KL(p ||q)).  Nevertheless, this is an interesting direction worth exploring for undirected graphical models, and the proposed directions using expressive variational models and recent importance sampling techniques make sense.This paper proposes to estimate the log-partition function of Markov random fields by optimizing a variational bound over a model of proposal distributions. For the proposal distributions the authors consider uniform mixtures of exponential families.   The paper is concise and presents interesting ideas. A central idea of the paper is to define a proposal model for which the variational bound is convex.  Unfortunately the arguments seem to be flawed or to require modifications.   COMMENTS In Section 3.2. ``One can easily check that a non-negative concave function is also ... are in the exponential family, it follows that $\sum_k \pi_k q_{\phi_k}(x)$ is log-concave, and hence the above expression is convex.''   This appears faulty.  For an exponential family $q_{\phi_k}(x)$ is log-concave in the natural parameters.  However, a sum of log-concave functions is not necessarily log-concave.  In fact, one finds examples of sums of exponential families that are not entry-wise log-concave in the natural parameters.  Kindly verify / explain / improve this.   OTHER COMMENTS  It may be worthwhile to consider the convexity problem in relation to ``convex exponential families'' and ``mixtures of exponential families with disjoint supports''. For these kinds of models, the maximizers of the likelihood function can be expressed in closed form or in terms of those of the individual mixture components.   MINOR COMMENTS  * It would be good to mention whether x is discrete or continuous, scalar or vector. Also whether $\theta$ is finite dimensional.  * In Section 2 ``closed-form expression'' is confusing. Given that $I$ is intractable, it seems that $I^2$ is also intractable.  * In Section 2 variance of the estimate $\hat I$, a $1/n$ factor seems to be missing.  * In Section 2 the variance vanishes when $p=q$ can be inferred from the fact that $w(x)=I$ is constant, without using Jensen's inequality.  * In Section 3 ``natural algorithm for computing'' should be ``estimating''.  * In Section 3 ``as minimizing a tight upper bound'' should not say ``tight''.  * In Section 3 ``This approach is complicated by the fact that unlike earlier methods that parametrized conditional distributions $q(z|x)$ over hidden variables $z$, our setting does not admit a natural input/output to a neural network.'' Why do you need an input/output in order to use a neural network? Maybe I just don't understand this sentence.  * In Section 3.2 $\pi_k$ has not been introduced. I suspect this is just $1/K$?   ",0,5146
"Pros:  This empirical results look quite promising.  Cons:  1) As the author mentioned in introduction that the proposed problem is able to take care of 'large p small N', it is worth to compare the proposed method to sparse learning / compressed sensing methods, for example, L1 norm regularized algorithm.  2) The experiment setting needs further clarification. In Figure 1.b)  it says that 'The error bars and shaded areas indicate standard deviation of CI over 10 cross validation sets'  and in section 3.3 it says that 'we sampled %70 of the data set 10times without replacement to have 10 different training sets.'  3) There is a typo in section 3.1 'Figure ??'.This abstract describes an interesting use of a neural network: discovering an appropriate set of features for a Cox proportional hazards model, with applications to cancer genomics.  The idea is to estimate survival of a cancer patient from genetic data.  The challenge for this is that many of the data are effectively censored -- we do not know how long currently living patients will survive.  This is a well-studied likelihood and the essential proposal is to use the last layer of the neural network to form the linear features used for survival prediction.  This last statement is a bit more of an inference than I would like, as the abstract is light on details.  There are various architectural choices that seem sensibly handled, and I commend the authors for systematically tuning hyperparameters with Bayesian optimization.  I was slightly surprised to see layerwise pre-training used on this model; this approach has gone somewhat out of fashion and it would be interesting if this offered improvements over directly training a multi-layer perceptron.  Overall, I think this abstract is well over the bar for acceptance.The authors describe a straightforward application of modern neural network methods to survival prediction: they replace the linear effects component of a Cox proportional hazards model with a feedforward neural net; or alternatively, the stick a Cox partial likelihood loss function on top of a standard feedforward neural net. The authors demonstrate that they have a mature understanding of modern techniques, which is not always the case among folks pushing deep learning into new application areas (demonstrated most by their use of Bayesian optimization, well done!).  The ReLU neural net clobbers both a classic Cox proportional hazards model with a Elastic Net-style regularization AND the survival analysis analogue of a random forest (often the most daunting foe of neural nets in empirical comparisons). The results are especially remarkable since they were obtained on a small data set (only 658 samples total, with 183 inputs). The deep learning community should celebrate empirical victories of this type (in new domains and problems, done by people who are applying the methods properly).  Some questions and comments:    * The figure reference at the bottom of Page 2 is broken.    * The ""%NN"" format for numerical percents is non-standard. I suggest the authors change those to ""NN%.""    * Some discussion of how performance varied with model architecture would be useful. Did 2-layers beat 1-layer and 3-layers?    * Unsupervised pretraining isn't typically used these days, especially for shallow neural nets. Did it, in fact, help?    * The authors might want to mention and cite which framework (if any) they used to implement their neural nets. ",0,5147
"This is an important topic: models with linear pathways have been a major part of recent successes in deep learning. The factors identified sound interesting. I'd be curious to learn more; it sounds like a good workshop poster.The paper proposes that deep neural network architectures that work well in practice, mainly do so because of their volume preserving properties. Inspired by this, the authors propose a neural network architecture that is volume preserving by design. According to their hypothesis, this type of network can still be trained successfully after stacking hundreds of layers.  I found the paper pleasant to read and very clear. The idea is relatively simple, but from a practical point of view, this can actually be considered a positive attribute.  I found the theoretical motivations for the importance of volume preservation unconvincing. A mapping that preserves volume, does not necessarily preserve the norm of the vectors it acts upon. The matrix diag(5, 0.2) has a Jacobian with determinant 1, but repeatedly applying it to a vector of ones would still let the first element grow and the second element vanish at an exponential rate.  Another shortcoming of the paper is the absence of comparisons with the other architectures that are discussed. The empirical evidence for the volume preservation hypothesis would have been much stronger if the amount of success of the very deep networks could directly be related to the extent to which the volume preserving property holds for them. The highway networks and residual networks are only approximately volume preserving, so it would be very interesting to see if exactly volume preserving nets can be used to train even deeper networks.    All in all, I still find the results and ideas interesting enough for a workshop presentation.Based on older work by Deco & Brauer (1995), in this workshop submission the authors introduce Deep Volume Conserving Neural Networks, which can be trained with many layers thanks to triagonal weight matrices. The authors offer a perspective on recent related work (Highway Nets, Residual Nets).   Pros: interesting topic, clearly written.  Cons: some concerns over the lack of content.  As stated by the authors, the underlying idea is not necessarily new (there was also other recent work at this venue exploring similar ideas: NICE: Non-linear Independent Components Estimation, Dinh et al, 2015). The question thus is whether the presented architecture works well in practice, which is not explored much.  The authors present an experiment on a variant of MNIST. They provide evidence that many layers can be trained, but report that the nets overfit to the training data. Rather than restricting the connectivity of the networks further to reduce their complexity, as suggested by the authors, I would rather recommend to tackle more challenging datasets; after all, the advantage of many layers in a feedforward net should be more expressive power. ",0,5148
"This paper builds further upon the line of research that tries to represent neural network weights and outputs with lower bit-depths. This way, NN weights will take less memory/space and can speed up implementations of NNs (on GPUs or more specialized hardware).  In this paper they present a new heuristic for choosing the bit-depth of every layer differently, so as to trade-off speed, accuracy and model size more optimally then when using an equal bit depth for every layer.  Pro: The paper is well written and the results are interesting and new. Cons: More experiments would be helpful. E.g., numbers on ImageNet, better comparison with previous art where they focus on fixed point networks during training.The authors describe an optimization problem for determining the bit-width for different layers of DCNs for reducing model size and required computation.  The described method is interesting and seems easy to implement. Experimental results on CIFAR-10 illustrate the benefits of approach with modest reduction in model size.  However, more experiments and details should be given. What about testing networks with just fully-connected layers? Is it the case that the ImageNet issue holds for CIFAR-100 as well? Details of how they fine tune after quantization? What is the previous state-of-the-art classification error on CIFAR-10 with fixed point? Plot w/ FLOPS required after quantization? These would help give better understanding of the significance of the work.This paper proposes a layers wise adaptive depth quantization of DCNs, giving an better tradeoff of error rate/ memory requirement than the fixed bit width across layers. Some points are not clear in the paper: - how do you fine tune after quantization? The statement ""... 6.78% with floating point weights and 8-bit fixed point activations "" is not clear. -Seems that the convolutional layers are the only one quantized and they have less parameters than the  the fully connected layers, how does the number of parameters growing up impact the performance? - How do you choose kappa?   ",0,5149
"The paper uses t-SNE for embedding non-stationary bird-song data, by computing   embedding of overlapping time windows of a time-series. Smoothness across embedding is  achieved by seeding each embedding with the embedding of the previous time window.   The problem of learning dense representations of time series is important and interesting,  and the authors are experts in the field of development of songbird vocalization.  The presented approach successfully visualizes interesting data, revealing the formation  of songs in developing zebra finch.   However, this paper is more about applying a well-established method in a standard way,  than about introducing a novel method. There are no quantitative evaluations with competing  approaches. As a result, it is not clear what the paper teaches us about methods for learning  representations at this point.         This abstract describes the use of t-SNE to cluster syllables of bird song.  The clustering is performed on overlapping windows of the signal, thus connecting them over time and allowing them to evolve as the young bird develops.  The abstract is well written and quite clear.  The originality of the method lies more in its use of a new method to analyze birdsong than in its development of a new method for clustering temporal sequences in general.  It is not clear that t-SNE is really required here, perhaps other clustering techniques performed over such sliding windows would work just as well on this task.  Pros: important problem examined by domain experts, well executed, interesting scientific results  Cons: no comparison with other approaches, straightforward extension of existing representation learning techniques",0,5150
"The paper presents a method for binding information to spatial locations in frequency space based on Holographic Reduced Representations, allowing set of (data, location) tuples to be encoded in a single complex vector of fixed size. The operation is differentiable, and can be inserted into neural networks to endow them with the ability to perform spatial reasoning. The approach is validated by experiments on two toy problems: identifying MNIST digits based on spatial queries, and performing value iteration for path planning.  Over the past year we have seen many approaches for inserting spatial attention into neural network models such as soft and hard attention over discrete locations, and differentiable forms of spatial attention as seen for instance in DRAW (Gregor et al, 2015) and Spatial Transformer Networks (Jaderberg et al, 2015). In light of this prior work, I believe that the frequency-domain based approach for spatial reasoning put forth by this paper is indeed novel and interesting; however the paper in its current form does not do a good job of explaining its relationship and significance with respect to these existing methods.  My major complaints about the paper revolve around the quality of its experiments and in the clarity of exposition describing them. For the first experiment: what, concretely, are the inputs and outputs of the network? What is the network architecture? How exactly are the spatial queries encoded and passed as input to the network? How does the network specify its glimpse locations, and in what format does it receive information about its glimpses? How is it trained? How well do baseline methods perform on this task and dataset? As written, the experiments do not contain enough information to highlight the strength of the proposed method.  The details of the second experiment are mostly left as an exercise to the reader, and the only experimental result is a single qualitative example.  PROS - The underlying idea has is interesting and has merit  CONS - Relationship to prior work is not clearly discussed - Numerous details are missing from experiments - The proposed approach is not tested against any baseline methods The paper presents an interesting application of HRRs to spatial location encoding.  This approach appears novel and possibly a fruitful direction.  The current work cursorily presents the idea, and two demonstrations.  In its current form, the work lacks sufficient detail to justify the direction over other approaches and/or sufficient quantitative findings to justify the direction over other approaches.  As a workshop submission, I believe that preliminary ideas are valid contributions.  However, this submission could use a bit more development to communicate the idea and the demonstrations presented.  Some questions/comments: I could use some additional explanation as to why location and identity should be combined through complex multiplication, while multiple glances should be combined through complex addition.  Why in your formulation is ""c"" complex, instead of real?  Why is the dimensionality of c the same as r?  Is this a limitation of your approach?  Are there more general formulations that would decouple the representational power of these two factors?  Pros: + Novel idea + Interesting, unsolved problem Cons: - preliminary experiments without detail or comparisons - needs more exposition to convey model set up and choices This paper considers the problem of encoding spatial relationship in a fixed-size representation, and leverage the idea of HRR. The paper gives two application use-cases: MNIST and path planning.   Hereafter I give how I see the pros and cons of this paper.   + The idea of using HRR is interesting  + I like the path planning application.  + spatial relationship that can be integrated in neural nets is certainly useful  - The novelty does not look very high. Considering the prior works on HRR and spatial relationships, this looks like a straightforward application paper.  - The literature review is minimalist. In particular, many works in computer vision have addressed the problem of encode spatial relationship. This has also been done with complex vectors, for instance the recent paper by Bursuc and Tolias in the journal Computer Vision and Image Understanding: ``Rotation and translation covariant match kernels for image retrieval'', which also exploits the convolution theorem (as in HRR anyway). As a result, it is not clear to me if there is anything novel in this submission. The worse is probably that CNN or its spatial extensions (like transformers networks) are not even mentioned, see my last point below.  - Poor experiments compared to what is normally done in papers dealing with images. There is not evaluation nor any comparison to any reasonable concurrent method, like a CNN architecture: CNN representations are state of the art, and thanks to convolution they also implicitly encode the translation invariance. They are now routinely used in computer vision system. It is very surprising that these papers are not cited/mentioned given the massive impact they had in the previous years.  - In my opinion details are missing for the idea to be reproducible. ",1,5151
" Summary:  This paper presents several qualitative experiments aimed at understanding the path taken by SGD through weight space.  Major comments:  “Flat region of weight space”: At several points it is claimed that models don’t arrive at a critical point but instead a flat region of weight space. Yet a truly flat region is, of course, a critical point or manifold of critical points; and nearly flat regions of weight space are common next to saddle points (see, eg, Saxe et al. ICLR2014). Hence if solutions do enter a flat region, this is not evidence for or against the local minima or saddle point hypotheses.  The observation that different networks can be led to very different solutions by reordering input examples is not surprising given the analyses in eg, Baldi & Hornik, 1989 or Saxe et al., 2014. The main point is that the many symmetries in a deep network lead to a manifold of global minima. This is an infinite set of critical points which all attain equal error. Hence, due to noise (such as reordering input samples), solutions can wander along this manifold—all global minima are equally good—and there is no pressure to settle on one particular optimal solution over another. As a simple example, take a deep linear network with just one neuron per layer, y = a*b*x, where a and b are scalar weights, and with just one input example {y=1, x=1}. The manifold of global minima is the hyperbola on which a*b=1. The euclidean distances between different solutions can thus clearly be arbitrarily large (one solution is a=1/10, b = 10; another is a=10, b=1/10), and as large as the euclidean distance to the origin.  Some results in this paper are already known. For instance, Saxe et al. ICLR 2014 showed that gradient descent learning trajectories are nonlinear. Indeed, Goodfellow et al. ICLR2014 also found that gradient descent does not take the straight line path from initialization to eventual solution.  Additional experimental details are necessary to evaluate the paper. What loss function is optimized? What exact initialization is used? These are centrally important to evaluating the results here. In particular, a small random initialization will perform differently from a large-norm initialization.  There are some conceptual confusions. At several points it is claimed that “after symmetry is broken” the error surface is still nonconvex. Finding a particular initial condition for which symmetry is broken does not make a minimization problem convex. The non convexity of a minimization problem can influence gradient dynamics even away from critical points, for example, it can induce long nearly flat plateaus.  ""...descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it's possible that neither interpretation is accurate."" Indeed those descriptions are not accurate, but this is already well-known and described in the Deep Learning textbook: http://www.deeplearningbook.org/version-2016-02-17/contents/optimization.html Fig 8.1 caption says ""Gradient descent often deos not arrive at a critical point of any kind."" and uses the norm of the gradient to show it.  ""Weights do not converge to critical points, instead traveling large (euclidean) distances through flat basins in weight space."" This is already shown in the 3D visualizations in the Goodfellow et al paper.  ""While a straight line in weight-space from initialization to solution may correspond to monotonically decreasing loss, the path actually taken by gradient descent seems far from straight."" This is already shown in the Goodfellow et al paper, both via the 3-D visualizations and via direct measurement of the distance from the path.  ""Even once symmetry is broken, neural network error surfaces are neither convex nor quasiconvex but continue to diverge towards many different low error basins. Starting from the same initialization, but then feeding each network examples in shuffled order is sufficient to diverge each network along a different path. This suggests that the error surface is not only globally non-convex, but also locally non-convex even for a partially trained net."" This is partially new, but there is already a lot of existing work on the effect of the order of presentation of examples. The references in Sec 8.7.6 discuss some of the existing findings: http://www.deeplearningbook.org/version-2016-02-17/contents/optimization.html   The paper is too misleading and takes too much credit for previously known ideas for me to endorse as it is. The following ideas from the paper are new findings and I encourage you to develop them further for a future submission: ""A small number of principal components explains most of the variance along a training trajectory."" ""All pairs of solutions after a fixed number of epochs appear to be roughly the same euclidean distance from the origin and from each other. This is true even with identical initializations, and pretraining before cloning"" From the experiments in this paper, it's hard to tell whether this effect is real, or if it is an artifact of the simplicity of the MNIST dataset. The submission presents an experimental setup for analyzing the successful gradient-based optimization and performance of networks with large numbers of parameters. They propose to train a convolutional network on MNIST and analyze the gradient descent paths through weight space. The trajectories are compared and evaluated using PCA.   This is very similar to the approach taken by Goodfellow et al, and it is difficult to see any new contributions of this submission. The results are mostly well-known at this point, although there is certainly room for further research in this area. The demonstration of divergence during training because of shuffled inputs is interesting but not surprising. There are no new visualizations or qualitative results, and the quantitative results are limited to 2 numbers (the variance explained by the top 2 and top 10 principal components) which are meaningless without more extensive comparison and analysis.  The submission could be a white paper to justify some further research, but it does not have enough substance or novelty to be in the ICLR workshop.",0,5152
"The description of the authors approach is not clear and would need to be re-writen. As far as I understand, they generate pixel probability maps to belong to the foreground/background that they give as input of the CNN of Ciceran et al. 2012 instead of the image patches, and iteratively use the obtained output to feed again Ciseran et al's CNN.  However, I am not sure I fully understand what the authors really did because I don't see why they write “It is also important to point out that training with the ground-truth map directly provided no benefit in improving the segmentation quality” The authors evaluate the results of their approach on the neuronal image dataset from the ISBI 2012 challenge.  The novely of the approach is not high but sufficient for a workshop submission and results are convincing, the major remaining problem remains the clarity of the paper.  Missing related work: Turaga et al, Convolutional networks can learn to generate affinity graphs for image segmentation, 2010.  Minor: abstract: no space before a dot by training -> train introduction of I-CNN: we don't understand in the first reading that it refers to the authors method proposition  In Jurrus et al. (2010); Pinheiro & Collobert (2013); Lee et al. (2015); Tu  (2008); Tu & Bai (2010), they applied… -> Jurrus et al. (2010); Pinheiro & Collobert (2013); Lee et al. (2015); Tu (2008); Tu & Bai (2010) applied …  Figure1 -> Figure 1 3 SYSTEM DESCRIPTION AND RESULT -> 3 SYSTEM DESCRIPTION  Reference list: please remove the reference that are not cited in the text or add citations in the text.  hao -> Hao water-shed -> watershed  in the Conclusion: “the new algorithm” -> the new procedureThis paper presents an iterative method to progressively clean up segmentation maps, by repeatedly applying a CNN to the output of the previous iteration. The novelty seems to lie in the application domain rather than in the network/model/training regime. The paper is quite unclearly written: I was unsure what the architecture of the network is, and how the iteration is applied. For the results, only 1 example is provided, which is insufficient, even for a workshop paper. The paper describes a method to clean up and improve boundary maps obtained with CNNs by processing them with additional CNNs. The reported results seem impressive (although, I have never worked on this application). Unfortunately, the description of the method is essentially lacking. From what I understood, the method is very similar to several previous works including:  Volodymyr Mnih, Geoffrey E. Hinton: Learning to Detect Roads in High-Resolution Aerial Images. ECCV (6) 2010 (not cited)  a seminal paper from the pre-deep learning era: Zhuowen Tu: Auto-context and its application to high-level vision tasks. CVPR 2008 (cited)  Perhaps, the detailization of the algorithm and the amount of novelty are not sufficient for acceptance to ICLR that focuses on new approaches and algorithms for learning representations. However I can imagine that venues/conferences/workshops for people working on membrane segmentation/connectomics would be interested.",1,5153
"The submission investigates an interesting variant of neural networks, referred to as `input-convex.' Hereby, the composite function of a standard neural network, or more generally a neural network for structured output spaces, is restricted to be convex in the output space, i.e., the variable of interest for prediction, and optionally the input space. This translates into non-negativity constraints on some trainable parameters, as well as a convexity and non-decreasing assumption on the employed activation functions.  Summary: -------- Clarity: The paper is well written and the idea is easy to follow. Quality: The idea is generally well demonstrated, but some experiments are missing in order to judge the efficacy of the proposed modifications, e.g., providing inference and training time on MNIST as well as adding some more baselines. Originality: The investigated variant is new but related to recent work which should be reviewed more adequately. See comments below for details. Significance: Due to some missing important experiments (timing), it's hard to judge the significance of this work at the moment.  Pros: Convexity in the output space recovers some guarantees for inference. Cons: Need for guarantees during inference hasn't been demonstrated and time for both inference and learning might be prohibitively expensive at the moment.  Comments: --------- 1. Why did the authors choose the name `input-convex' if convexity in the output space is the most desirable property? I think the title might be slightly confusing.  2. Using the rectified linear unit as the activation function allows to rephrase inference as a large linear program. Did the authors investigate non-linear activation functions where inference amounts to solving constrained optimization problems?  3. The non-negativity constraint on the parameters \theta is missing in Eq. 5.  4. For a reader it is desirable to get to know the difference in training and inference time between standard neural networks and the proposed `input-convex networks.' Hence, providing error over time in addition to Fig. 3 as well as a small table containing inference times seems worthwhile. I suspect inference and hence training to be time consuming for larger models, but an investigation is missing at the moment.  5. Admittedly, 4 pages are rather constraining, but I think there is significant amount of very related work that should therefore be mentioned. E.g., work by D. Belanger and A. McCallum, `Structured Prediction Energy Networks,' and also recent work combining structured prediction with deep learning, e.g., by M. Jaderberg et al. (Deep Structured Output Learning for Unconstrained Text Recognition), S. Zheng et al. (Conditional Random Fields as Recurrent Neural Networks), L.-C. Chen et al. (Learning Deep Structured Models), A. Schwing and R. Urtasun (Fully Connected Deep Structured Networks) and references therein.  Minor comments: --------------- - Aside from one note, the output space (\cal Y) is never defined formally. It might be worthwhile to at least specify it explicitly for the experiments. - I wasn't able to open the document in Acrobat. The authors may want to check.Comment: Summary: The paper presents a novel variant to the standard neural network architecture. Under certain constrains the proposed architecture is convex in their input space. This convexity property facilitates fast inference over a subset of input variables making them highly suitable for structured prediction problems. The authors report rather simplistic experiments to back their claim.   Novelty: The ideas proposed in the paper are fairly novel and very well motivated.  Clarity: The paper is very well written and easy to read.  Significance: While the ideas proposed in the paper sound quite impactful especially in the structured prediction setting, I have some serious reservations with respect to their actual utility. For starters, the constraints specified with the model are rather too restrictive. There are a large number of non-linear activation and pooling functions which one will not be able to use. In addition the non-negativity constraint on the parameters is even more restrictive. As a result the usefulness of the proposed model is not very convincing.  Quality: While the paper is well written, the experimental section is extremely weak: almost non-existent. It is a bit strange that the authors motivate their proposed model by listing its extreme usefulness for structure prediction problems. However they validate their claim on two rather simplistic dataset: a toy dataset, and mnist. I wonder why.   Pros: The paper presents a novel model which could potentially be used for fast structure prediction using deep networks. The paper is very well written and easy to read.  Cons: While the model is interesting, it has some rather significant drawbacks. The constraints are quite limiting to make the model of any use for a real problem. In addition the experimental section of the paper is almost non-existent: the authors train and test their model on a synthetic data set and an mnist dataset. First, the authors do not compare their model against any other baseline. Second, the model was motivated to be useful for structure prediction problems, however it is tested on something completely different. ",0,5154
"The objective of this paper is to improve the saliency map generation approach of Simonyan et al. 2014.  Strengths: + The output look significantly better than Simonyan et al. 2014 + The submission is well written and easy to follow  Weaknesses: - The paper contains no evidence or argument for why the proposed method produces maps that cover salient objects and not non-salient objects - Even though some saliency benchmarks exist, the submission does not provide any numerical results  The submission proposes a method that significantly improves the saliency maps produced by Simonyan et al. 2014. This reviewer, however, does not think that progress in this direction is generally useful. The task of image saliency is not well-defined, in general or in this paper. It's not clear that the proposed method captures ""saliency"" even if one tries to define it: it is not demonstrated that the method highlights objects that are considered salient while leaving objects that are not considered salient dark. There do exist image saliency benchmarks (e.g., salicon), but no numerical results are presents so it's not clear that the method improves anything. Pros:  Novelty&Significance: I think this paper is interesting in the sense of combining the semantic information into traditional saliency problems under the weakly supervised scenario. It computes local contrast with CNN feature and then output a semantic mask of the targeting object. Actually, I think it is a good complementary to current strong supervised saliency detection such as bounding box salient object detection, salient object segmentation.   Clarity: The paper is clearly descriptive and easy to understand.   Cons:      It lacks of numerical comparison and evaluation, with some standard criteria.       Currently, rather than saliency,  it is more close to weakly supervised segmentation. So maybe the authors need to refer to some weakly supervised segmentation works using CNN, e.g. Chen et.al ICCV 2015.       For general saliency. I am not clear whether it can transfer to unknown object domain, which could also be interesting.    ",0,5155
"The idea is excellent, although somewhat obvious. The paper describes training character-based text models directly on SMILES files that encode chemical graphs as strings and then making predictions about the molecules.  Reasons to accept:  - good idea that works at least somewhat  Reasons to reject:  - limited empirical evaluation  - Need to explain the SMILES format well enough that the data augmentation procedure is clear. How is the walk determined normally?the analogy between CI and sentiment analysis is intriguing and potentially fruitful.  the community will definitely appreciate this work.  hopefully authors can increase data resources in follow-up work to further improve performance over classifiers with engineered features.This works shows that SMILES (walks across a graph of atomic connections) allows the advancements in text classification to be brought to cheminformatics with good results compared to using the latest hand-tuned features. The idea is not particularly novel but the results show how an unrelated area can fairly easily benefit from advancements in DL NLP. The intuition that localised features determine molecular binding seems like a great fit for sentiment analysis techniques.  It would be nice to show the molecule lengths and sizes of dataset in Table 1, I don't think these are mentioned anywhere. It would also be nice to try newer sequence prediction techniques such as LSTMs (possibly with pretraining).",0,5156
"In recent years, many generative models have been proposed to learn distributed representations automatically from data. One criticism of these models are that they produce representations that are ""entangled"": no single component of the representation vector has meaning on its own. This paper proposes a novel neural architecture and associated learning algorithm for learning disentangled representations. The paper demonstrates the network learning visual concepts on pairs of frames from Atari Games and rendered faces.  Novelty - The proposed architecture uses a gating mechanism to select an index to hidden elements that store the ""unpredictable"" parts of the frame into a single component. The architecture bears some similarity to other ""gated"" architectures, e.g. relational autoencoders, three-way RBMs, etc. in that it models input-output pairs and encodes transformations. However, these other architectures do not use an explicit mechanism to make the network model ""differences"". This is novel. The paper claims that the objective function is novel: ""given the previous frame x_{t-1} of a video and the current frame x_t, reconstruct the current frame x_t. This is essentially the same objective as relational autoencoders (Memisevic) and similar to gated and conditional RBMs which have been used to model pairs of frames. Therefore I would recommend de-emphasizingthe novelty of the objective.  Clarity - The paper is well written and clear.  Significance - This paper opens up many possibilities for explicit mechanisms of ""relative"" encodings to produce symbolic representations. There isn't much detail in the results (it's an extended abstract!) but I think the work is exciting and I'm looking forward to reading a follow up paper.  Quality - Based on the above, I would say that this is a high-quality workshop paper in terms of the ideas and definitely of interest to the ICLR audience.  Pros  - Attacks a major problem of current generative models (entanglement)  - Proposes a simple yet novel solution  - Results show visually that the technique seems to work on two non-trivial datasets  Cons  - Experiments are really preliminary - no quantitative results  - Doesn't mention mechanisms like dropout which attempt to prevent co-adaptation of features  quality: overall the paper is clearly written and the toy experiments demonstrate the claims. The topic is very relevant to this venue.  clarity: good.  originality: medium. This work is related to Memisevic's work on learning relations between pairs of images. The mechanism to compute the transformation is different, but the paper should comment and discuss this.  significance of this work: although premature as a publication, this work can already bring good discussion on how to learn factorial representations.  pros: + very relevant topic + somewhat novel model  cons: - missing references - toy nature of the experiments",1,5157
"Up till now, all work on GANs has had no proper metric for quality, and model selection has been via human eyeballing. The paper proposes a simple metric to compare the performance of GANs. Pairs of GANs are trained independently, and then at test time are pitted against one another (G of GAN1 against D of GAN2 and vice versa). In this validation process, we can look at the ratio of discriminator successes to rank models.  The idea is simple and good. It has been explained clearly. It is very significant for all research in the GAN framework, which has seen a lot of growth.  The only concern I have for the area chair is that there's a chance of double-publishing the same idea -- the full paper is already out, which has been submitted to ICML. The full paper was pushed to arxiv before the deadline of the ICLR workshop http://arxiv.org/abs/1602.05110 . This paper is a section copied from the authors' full paper http://arxiv.org/pdf/1602.05110v2.pdf. The authors don't change a word.   Summary:   This paper described generative adversary metric, which is defining score on test and score on sample from two discriminators then choose winner. The score is error of discriminator.   Limitation:   In partially compare, eg compare to VAE, it requires to obtain a good D for GANs first, it may need to do evolution training to find a good baseline model. Also as D has seen G's sample during GAN training, but has not seen VAE's example. Direct using D's score to compare GAN against other models is unfair.   Reject reason: Nothing different to full paper's section.This paper introduces a new method to compare generative models especially those trained with GAN framework. The idea is that after training each model independently, we compute the error rate achieved by the discriminator of one model against the generator of the other model, and use this error rate to compare the models.  Pros: ===== 1) This paper tries to solve the important problem of evaluating generative models.  2) I think the idea of the paper is interesting for comparing different models trained with GAN framework.   Cons: ===== 1) I don't think this idea is applicable to any other generative model such as VAE. The discriminator of GAN typically is good at detecting sharp edges and local structure, which is why the generator of GAN always learn to generate sharp images. However, as opposed to GAN, the generated images of VAE are often blurry images which contain some global structure. So I think it would be very hard for the VAE generator to fool a GAN discriminator into thinking that the image is real. So it would not be fair to use this evaluation method to compare a GAN model with a VAE one.  2) This paper seems to be a copied section of an ICML submission by the same authors with no additional contribution.",0,5158
"The authors claim that under certain assumptions on the variance of pre-nonlinear output and the gradient, the initialization method from [Glorot&Bengio,2010] can be recovered when the derivative of the activation equals to one. Via Taylor expansion of the activation functions they show that sigmoid violates this condition.   I do not understand the statement ""Clearly, when x is around 0 Sigmoid will make gradient vanishing if we use same learning rate in each layer."" I believe the derivative of sigmoid is non-zero especially around zero and close to zero else where. I believe gradient vanishing occurs when the activation function is pushed into saturated regime.   The proposed version of sigmoid (sigmoid*) can be seen as a version of tanh function with larger non-saturated regime and steeper gradient. Similar activations were proposed in past (sorry for not being able to provide a reference at this moment). I encourage authors to look at the hidden responses of the network trained using sigmoid*, they might be operating in non-saturated regime. I also don't see how the proposed activation is equivalent to using different learning rates for different layers?, unless combined with some specific initialization scheme.  RectifiedTanh function, which is a specific case of proposed leaky tanh, was shown to perform as good as Relu in http://arxiv.org/pdf/1506.08700v1.pdf. I believe that the leaky nature of proposed activation helps, but the main jump in performance might be due to the rectification nature of the function.The authors study weaknesses of the saturated activation functions (TanH and Sigmoid) and propose ways of improving it.  In first part of paper authors propose to fix difficulties of training sigmoid networks by one of two ways:  a) Simply apply separate learning rates to the different layers: starting with 1 for linear classifier, multiply each previous lr by 4 and correct the weight matrices the opposite way. This could be seen as analogy of (He et al., 2015) correction of (Glorot et.al, 2010) formula but for Sigmoid networks instead of ReLU. The idea is simple - to compensate vanishing gradient by proportionally increasing learning rate. As far as I checked on 4-layer network, it works, but seems rather impractical for deep network, i.e. correction coefficient for 11-layer network would be 4^11 = 4194304. However, the idea of layer-wise adjustment of learning rate potentially could help is other situations, not necessary with sigmoid.   b)rescale and shift sigmoid to so called sigmoid*(x) = 4*sigmoid(x) - 2. This solution is much simpler, but I believe, such function cannot be called sigmoid, rather tanh. Reason:  sigmoid*(x) = 4*sigmoid(x) - 2 = 4/(e^-x +1) - 2 = (4 - 2e^(-x) - 2 ) / (e^-x + 1)= 2 (1 - e^-x)/(e^-x + 1) = 2*tanh(x/2).  In the second part of the paper authors propose ""leaky"" version of TanH, showing that it could compete with ReLU activation. It could be interesting to see how other ReLU-family inspired variants of TanH would perform: a)rectified TanH = 0, if x < 0 b)Randomized Leaky TanH in RReLU fashion c)Parametric Leaky TanH.  I believe that paper with such evaluation will help future research in saturated activation functions. This is a very badly written paper. There are several typos and mistakes. I will not list them, I recommend the authors to proof-read the paper a few more times. On the other hand, those mistakes make the paper difficult to follow, although the ideas presented there are not very complicated. I think this paper requires a better write-up.  Section 2 is very confusing and ambiguous. Where does w^{(l)} come from? It is not define in the text.  I really can not see how you go from Eqn 3 to Eqn 4.  Taylor approximations in Eqns 7,8 and 9 are done around 0. You should explicitly state that in the text. Eqn 7 is wrong. Eqn 10 is basically 2*tanh(x). There is a discontinuity between the ideas presented in Section 2 and Section 3.   Why the results of Sigmoid on Table 1 are N/A. If you are not going to put the results there, why did you put this there? It seems like still leaky-tanh performs slightly worse than leaky-ReLU. Why would one prefer leaky-tanh over leaky-ReLU for feedforward networks? The conclusion provided in the paper is obvious and really not surprising. These ideas could be better analyzed and investigated.",1,5160
"This paper proposes a neural network attention model that learns the tiling of the units in the retina.  The model is a relatively minor variation on the attention mechanism from the DRAW model of Gregor et al. so the main novelty is in the experiments.  The result showing that the model learns a layout with a high resolution fovea and a low resolution peripheral region only if it is not allowed to zoom in and out is very interesting.  Minor comments: - Paragraph 2 on page 1 refers to Figure 2 instead of Figure 1. - Equations 4-10 would be clearer if they showed dependence on both m and n.The authors propose to learn a grid of filters with learnable mean and variance. They provide preliminary experiments on the translated cluttered MNIST dataset. They motivate their approach with an analogy to tiling in the human retina. Empirically, this approach is motivated by recent successes  using attention in deep learning.  While the approach described in this paper is sensible, there is a major problem in that it is missing basic evaluation on their MNIST task.  Even if the authors do not compare to previous methods, they should report some basic heldout validation/test performance. On the more qualitative side, the example given seems quite compelling, but the reader is left to guess if only a few examples look like this (a basic overlap metric would help). The analogy to the human retina is certainly interesting, but it is unclear how this somewhat toyish model could ""discover the optimal tiling of retina ..  in a data driven manner"".   Other: It would be sensible to cite the following papers for attention     Gregor et al. DRAW: A Recurrent Neural Network For Image Generation, 2015     Ba et al. Multiple Object Recognition with Visual Attention, 2015  Some of the equations (4)-(10) seem to be missing temporal indicies.This paper proposes a visual attention model for images. Attention is modeled as a grid of gaussian filters as in the DRAW model. In addition to the gaussian parameters, the authors also propose to learn the grid layout by assigning learnable offset parameters to the filters.  While the proposed contribution is interesting, its empirical evaluation is rather weak. Authors should report validation/test errors in addition to the training error to see how their approach generalizes to unseen examples. In addition, authors should compare their contribution with a proper baseline such as DRAW (that uses an uniform tiling of the input) in order to assess the benefit of learning the grid layout. They should also compare previous attention approaches such as Recurrent Models of Visual Attention (Mnih et al.) 2014 or Dynamic Capacity Network (Almahairi et al.) 2016. Finally, it would be interesting to try the approach on a more realistic dataset such as SVHN.",0,5161
"The paper applies autoencoders in place of matrix factorization to induce latent representations of learners and problems in the KDD educational data mining challenge. Results are negative, underperforming a very weak ""average performance"" baseline.   This may be an interesting domain for representation learning, but the paper does not yet make a clear contribution. The problem formulation was already given in the KDD cup. The proposed approach is not original, nor is it customized to the problem in any way -- it's just a drop-in replacement for matrix factorization. The results are negative, failing to outperform a baseline that does not adapt to the student, while matrix factorization does yield a slight improvement. This suggests that the autoencoder may not be correctly applied.The paper applies representation learning techniques, i.e., autoencoders, to learn hidden representations on KDDCup 2010 educational data. The reported results are not encouraging though.   It's an interesting new domain to apply representation learning techniques, but the technical contribution of the paper is very limited. The authors applied autoencoders out-of-box from some existing library, and reported numbers. Adding these latent representation did not bring any add-on value to the baseline suggests that it was probably not correctly applied. The paper is too short for readers to get much details on what have been tried, and what worked or didn't, and why. ",1,5162
"This paper proposes to use a multiple convolutional layers neural network with highway MLP to classify genomic sequences on the transcription factor binding site task, and also proposed to visualize the motif by utilizing the method proposed by Simonyan et al. (2013) to have a better interpretation of what the model learns. This paper moves a small step from Alipanahi et al. (2015) which uses a single convolutional layer for the TFBS classification task. This paper proposes to use a multiple convolutional layers neural network with highway MLP to classify genomic sequences on the transcription factor binding site task, and also proposed to visualize the motif by utilizing the method proposed by Simonyan et al. (2013) to have a better interpretation of what the model learns. This paper moves a small step from Alipanahi et al. (2015) which uses a single convolutional layer for the TFBS classification task.   The main novelty of this paper is to apply the recent progress of deep learning to biomedical tasks, such as CNN and Highway MLP, and achieves very good results on TFBS task. It is appreciated that the authors show that deep learning are able to provide good results in biomedical literature. However, the drawbacks of this paper are as follows:  1.  The novelty of the paper is limited. Extending the model of Alipanahi et al. (2015) to a deep model and utilizing the Highway MLP is good. However, there is no key improvement over Alipanahi et al. (2015). Hence, the novelty is not very significant.   2.  The description of the model proposed in this paper is not sufficient. As the paper focuses on the medical literature, it would be much better to provide more details of the methods, such as what is TFBS classification, motif, and the details of Equation (1). What is more, the paper did not explain what is the meaning of Figure 2(b), but only a ""A comparison of motifs can be seen in figure 2."". What does Figure 2(b) compare? And what does the y-axis of Figure (2) mean? This might make the reader confuse.   In summary, I think the pros of the paper is that it applies some successful approaches in DL on biomedical tasks and achieve competitive results. However, the cons is that the novelty of the paper is not significant and clarity of the paper also needs to be improved.    This paper proposes to use deep neural networks to improve the classification performance for predicting transcription factor binding sites (TFBs). Compared to DeepBind (Alipanahi et al., 2015), the authors use a deeper network (combination of deeper convolutional layers and highway MLP layers) and show that this model improves the classification performance (measured in AUC). The paper also presents a visualization method similar to Simonyan et al. (2013). The main contribution of the paper can be summarized as: (1) improved classification performance of TFBs using deeper CNNs, and (2) a visualization method of motifs from the learned CNN model.   Novelty: The technical novelty of the method is quite incremental. Compared to DeepBind, the main novelty is the use of deeper CNN layers and highway MLPs; the improvement of the AUC is ~1% (0.946 vs 0.931) which is moderate. In terms of visualization, the main difference is to optimize the input to maximize the output of the network via backprop (Equation 1) instead of using actual samples.   Clarity and presentation:  The paper is quite clearly written (given page limit), but more details would be helpful for clarification.  Significance: Although the technical novelty is incremental, there is a reasonable (although not big) improvement in performance. The proposed visualization method (which is not technically novel) can turn out to be useful in practice. This paper could contribute to more investigation and application of deep learning in the computational biology domain, which is under-explored compared to other areas (vision, speech, etc.).   Overall, the paper would fall on the borderline; however, given the relative paucity of deep learning papers in computational biology, the paper may be worthy of presentation at the ICLR workshop. ",0,5163
"This is an interesting paper, offering a novel perspective on training directed graphical models.  It is known that learning in directed generative models using gradient ascent on the marginal log-likelihood requires one to obtain samples from the posterior probability distribution. The paper suggests getting those samples by running an MCMC chain that leaves the posterior probability invariant. The MCMC chain used is based on either independent proposals from an auxiliary distribution Q, and an MH accept-reject step, or on multiple-trial Metropolis independence sampler based on the same auxiliary distribution Q. The auxiliary distribution Q is also learnt by gradient descent on the KL divergence between the posterior and Q, where samples from the posterior are obtained in the same way as before.  This method of training is novel - previous methods used either MCMC chains based on Gibbs sampler (Neal, 1992 - unfortunately not cited in the article), or used optimization of a lower bound on log-likelihood, or biased estimates of the gradient of the log-likelihood. The method is most directly comparable to the Reweighted Wake Sleep method, because ultimately the updates to the parameters follow the same equations every time the proposed transition is accepted (but reuse previous samples when the transition is rejected, which is an important difference from the RWS algorithm).  One drawback of the proposed method is that it requires to store a state of the MCMC chain, one state of latent variables configuration per datapoint in the dataset. It might not be too restrictive for smaller datasets, like MNIST, but is prohibitively expensive for larger datasets.  The experiments use a published implementation of RWS as a baseline. This is unfortunately not the best practice, as the implementation of the proposed algorithm might use slightly different initialization, hyperparameters, or length of training, which makes the contribution of the algorithm itself harder to separate. This is exacerbated by the fact that the difference in log-likelihoods of trained models is fairly small (although significant). It would be better to use exactly the same initialization and hyperparameters for the RWS implementation and for the proposed algorithm.  Another comparison is of the proposed algorithm (MIS version) to (non-reweighted) Wake-Sleep. In this comparison the proposed algorithm converges to significantly better performing models, indicating that storing the previous states of the MCMC chain, and following the proper Metropolis accept-reject step does provide a significant advantage.  The paper has multiple typos and grammar issues, and would benefit from additional editing.The authors present a new method to perform maximum likelihood training for Helmholtz machines. This paper follows up on recent work that jointly train a directed generative model p(h)p(x|h) and an approximate inference model q(h|x). The authors provide a concise summary of previous work and their mutual differences (e.g. Table 1).   Their new method maintains a (persistent) MCMC chain of latent configurations per training datapoint and it uses q(h|x) as a proposal distribution in a Metropolis Hastings style sampling algorithm. The proposed algorithm looks promising although the authors do not provide any in-depth analysis that highlights the potential strengths and weaknesses of the algorithm. For example: It seems plausible that the persistent Markov chain could deal with more complex posterior distributions p(h|x) than RWS or NVIL because these have to find high probability configurations p(h|x) by drawing only a few samples from (a typically factorial) q(h|x). It would therefore be interesting to measure the distance between the intractable p(h|x) and the approximate inference distribution q(h|x) by estimating KL(q|p) or by estimating the effective sampling size for samples h ~ q(h|x)  or by showing the final testset NLL estimates over the number of samples h from q (compared to other methods). It would also be interesting to see how this method compares to the others when deeper models are trained.  In summary: I think the paper presents an interesting method and provides sufficient experimental results for a workshop contribution. For a full conference or journal publication it would need to be extended.   I also found some grammatical issues and I would recommend additional proofreading. ",1,5164
"This paper explores two augmentations to simple character RNN language models. Closing the performance gap between character and word based language models is an obvious and important research goal. While the aims of this paper interesting, at this point it looks more like a work in progress with a number of significant gaps, particularly in the evaluation.  General Points: - Given that simple RNNs struggle to capture long range dependencies, would it not be better to do this study with LSTMs? The assumption that the conclusions drawn for simple RNNs will hold for LSTMs seems flawed. - Equation 3 is not a correct likelihood function if the vocabularies of the two mixture components are not equal, which they are not in this instance. Section 5 implies that this function is used to calculate the BPC evaluation metric. This would result in overly optimistic BPC scores for the Mixed model. - The conditional output RNN is an interesting way to get direct ngram context into the model. It might be worth noting the similarity to the Multiplicative RNN of Sutskever 2011, where that model uses a conditional tensor contraction for the transition weights. There are many other ways to incorporate ngram conditioning, for example using direct ngram features on the input or feeding more than one character to the input layer. It would be informative to see how these other, easy to implement, approaches compare to the proposed approach. - The experimental evaluation is let down by poor choices of data sets. Firstly the datasets are all rather small compared to those usually used for language model evaluation. The processing of the PTB data set is particularly quirky, i.e. lowercased, punctuation removed, and <UNK> symbols for rare words (I assume predicted as < U N K > !). It is not clear why this processing is a good choice for a data set for evaluating a character LM. While the Europarl data set (at least v7, it looks like you used v1?) is a good choice for a multilingual corpus, training on just 60k sentences seems a bit unambitious. Also, if the sentences were randomly selected for test this could mean that test sentences came from the same documents as training sentences? This is not desirable for a LM evaluation. - Where did the estimate of '4 times the average entropy for OOVs' come from? This seems a bit random. It would be easy to build a character ngram/RNN estimate for singletons. This practice used to be standard for open vocabulary language models. - When reporting BPC metrics on text it is useful to report the performance of standard compression algorithms such as PAQ. These almost always significantly beat ngram and RNN models.This paper introduces two model extensions to improve character level recurrent neural network language models. The authors evaluate their approaches on a multilingual language modeling benchmark along with the standard Penn Tree Bank Corpus. Evaluation uses only entropy rather than including the language model in a downstream task but that's okay for a paper of this scope. The paper is clearly written and definitely a sufficient contribution for the workshop track it would be really nice to see how well these methods can improve and more sophisticated recurrent architecture like gru or lstm units. On the PTB Corpus it would be nice to include a state-of-the-art or standard n-gram model to use as a reference point for the reported results.  The conditioning on words model is an interesting approach. It's unfortunate that such a small word level vocabulary is used with this model. It seems like the small vocabulary restriction is due to the fact that the word level model is jointly trained along with the character models. An alternative approach might be to use as input features the hidden representations from a word level recurrent model already trained when building the Character level language model. I don't have a good sense for how much joint training of both models matters.   When conditioning on recent history the authors might think about the NLP context trick of conditioning on a bag of words or bag of characters instead of considering only 10 grams. This would allow for a broader context coverage without expanding the feature dimension too much",1,5165
"This paper presents a combination of the inception architecture with residual networks. This is done by adding a shortcut connection to each inception module. This can alternatively be seen as a resnet where the 2 conv layers are replaced by a (slightly modified) inception module. The paper (claims to) provide results against the hypothesis that adding residual connections improves training, rather increasing the model size is what makes the difference.  Pros: First off, a combination of inception & resnets is kind of incontournable. This work is presenting an interesting combination of two strong and impactful models, and is in that sense quite significant and (moderately) novel. The ""fair"" comparison in terms of computational budget is insightful and I tend to agree with the author's claims. However see the first 2 cons below.  My remarks (count them as cons or suggestions for improvement): + The resnet-151 is (probably?) still quite a lot deeper than inception-resnet-v2,    so it feels too early to conclude that res connetions wouldnt give an edge    when making these inception-resnets even deeper. + I find Figure 3 hard to believe / worthy of more dicussion:    in inception-v3 vs inception-resenet-v1 the shortcut connections    make a massive difference in terms of convergence time.    However for the scaled-up version there's almost no gain from residual    connections! That just looks so *very* strange. + No explanation how inception-resnet-v2 looks like, how big is it exactly? + A bit more argumentation for the inception-resnet design would be good.   To me it seems like several inception blocks for one shortcut would also be   an option. + Not very well-structured, ""Model"" contains arguments that might be better    in ""Results"" or ""Discussion"" sections. + Paper appears to be hastily written, the citation style is confusing, the    section ""4. Results"" seems to be written on a phone?     ""Firs we compare"", ""Figure 3 This graph"", ""newline introduced"", + All combined, I'd say that the paper is worthy of publication but it is a bit   premature and looks a bit like ""marking territory"". At the other hand,   this is a workshop paper so this might be fine.The paper shows good results and has interesting insights. It is a bit raw in presentation. Finally, and most importantly, we've moved away from feature engineering with deep learning, but are not doing model engineering. For example: 1) formulated principles of construction nets    - avoid bottlenecks on early layers,     - spatial aggregation (convolutions) can be done over lower dimensional embedding because adjacent units highly correlated => why bottlenecks inside inceptions.  2) factorization   - 5x5 factorized with two 3x3, 3x3 everywhere   - 3x3 factorized with 1x3 and 3x1 in the middle of network   - added inception with conv 7x7 and factorized to series of 1x7 and 7x1. It behaves well only for low-dim grids in the end of network.  Ideally, we would have algorithms that help design models for a specific problem and make these decisions from data. Furthermore, inception was engineered for ImageNet, and is likely starting to overfit. It is not clear if all of the design decisions at this stage are actually generalizable to other problems. ",0,5166
"This paper extends the work of Ba and Caruana, Do deep nets really need to be deep? by asking the same question about deep _convolutional_ nets, and reaches the opposite conclusion in this new context.  I don't think the conclusion (that deep convents work better than not-deep-convnets on images) is going to surprise anyone; but, the architecture search is quite extensive, so at least this paper provides some circumstantial evidence to support the commonly held intuition.  I do wish the authors had been similarly meticulous when writing the paper as they were when running experiments.  There are a lot of moving parts involved here and I would have really appreciated if some effort was made to synthesize the results in a comprehensible way, rather than simply dumping the all the details into paragraphs of latex and expecting the reader to untangle them.  For example, understanding what is ""Teacher CNN 1"" on page 3 requires digging into Section 4.7 in the appendix, finding the paragraph talking about 129 CIFAR models trained that talks about the performance of the first and _fifth_ best model, to finally discover that this is the top three performing models from the ""Super Teacher"" ensemble (which then requires a bit more digging to verify that this is the same thing as the ""Ensemble of 16 CNNs"" from the table on page 3).  I am recommending accepting this paper because the experimentation is quite thorough and I think the ICLR workshop is the right venue to to present something like this, but I strongly encourage the authors to spend some effort making tables and diagrams and organizing the presentation of their hyperparamter search in a way that is comprehensible.  I appreciate the level of detail, especially in a paper supporting a negative result with experiments, but the presentation needs serious work.The paper confirms the importance of being deep and convolutional empirically by showing that the shallow models cannot achieve as high accuracy as deep and convolutional counterparts. The experiments has been held extensively and the conclusion made in the paper sounds quite convincing even though it is based only on empirical results.  Overall, the claim of the paper is not surprising, but many details in the paper such as architecture selection or the effectiveness of distillation would be good to be presented. Nevertheless, it would be great if authors can provide more analysis why and when the shallow network fails to be as good as deep network than simply presenting the numbers.   It'll be good to provide training loss for student model and compare with the teacher model to show less overfitting.  The paper is interesting.  Bayesian optimization (BO) is used to support the claim that there are no shallow models that are as good as the best deep models for CIFAR-10. Rationale being if BO couldn't find hyperparameters and a learning algorithm to train such a good shallow model, then there is no such shallow model to be found. This evidence is strongest when you search a large set of shallow models and training algorithms, the BO search appears to have converged, and BO search has found models at least as good as the best ones known to be in the search space.  Re: searching a large-enough set of shallow models & algorithms, it would help to show that the best models were not discovered near the boundary of the search space. If they were, the search space should perhaps be bigger to find still-better models.  Re: convergence of BO A plot of error over time of the BO searches would increase the strength of the case. Such a curve is not proof that a huge space has been searched but if the best models were all found near the end of the search, then it would undermine the conclusion.  Re: finding the best known models at each depth This appears to be true but a resume of recent high-scores' citations would be appropriate. ",1,5167
"The main idea seems sensible, and fairly well-explained.  Having a differentiable generalization of both addition and multiplication seems like a useful tool to have in general.  My main fear is that the idea isn't novel, but I haven't seen it presented in an ML framework before.  Problems: 1) It's still not clear to me how to compute the proposed function - I assume it's done iteratively? 2) Part of the motivation is training speed, but the authors didn't measure wallclock time.  This should have been included. 3) The use of \psi and something like \varpsi or \varphi is confusing.  Please use distinct letters for which we have names. 4) Is the domain restricted to the positive reals only when n = -1 (and we recover log(x)), or is it restricted whenever n < 0?   The paper suggests using a differentiable function which can smoothly interpolate between multiplicative and additive gates in neural networks.  It is an intriguing idea and the paper is well written.  The mathematical ideas introduced are perhaps not novel (a cursory search seems to indicate that Abel's functional equation with f=exp is called the tetration equation and its solution called the iterated logarithm), but their use in machine learning seem to be.  The experiment section is weaker - the problem seems somewhat contrived and with few datapoints. exp^{(n)} get the best test loss but the worst training loss - are the baselines simply overfitting? (it appears so when looking at the tanh testing curve). ",1,5168
"This paper proposed a novel graphical model that jointly reasons models in a multi-task setting. The proposed work extended Cai et al. 2011 under multiple context.  Theoretical convergence guarantee is given (proof is not provided). Experimental results over synthetic data shows the effectiveness and efficiency.   Overall the paper is technically sound (correctness is not checked carefully). However, lack of proof and experiments on real data make it less convincing. This paper provides an extension to graphical lasso to handle the case where there is one graph per data source, plus a shared sparse graph. (Rather than maximize likelihood subject to a sparsity constraint, they minimize density subject to a likelihood constraint, but this seems like a minor technical detail.) The objective is convex, they solve it with linear programming. They show a toy experiment where they generate two GGMs, and sample data from them, and then try to recover the structure.  Their method outperforms (in terms of the ROC curve for edge recovery) various other methods.  Although the objective function is elegant and possibly novel, the contribution is still very small: there is no algorithmic novelty (standard LP solvers are used, no discussion of scalability), and only toy experiments on synthetic data are presented. Finally, I don't think this topic fits well with ICLR. (They have also submitted a version to ECML-PKDD 2016 journal-track, which may be a better fit.)  This paper provides an extension to graphical lasso to handle the case where there is one graph per data source, plus a shared sparse graph. (Rather than maximize likelihood subject to a sparsity constraint, they minimize density subject to a likelihood constraint, but this seems like a minor technical detail.) The objective is convex, they solve it with linear programming. They show a toy experiment where they generate two GGMs, and sample data from them, and then try to recover the structure.  Their method outperforms (in terms of the ROC curve for edge recovery) various other methods.  Although the objective function is elegant and possibly novel, the contribution is still very small: there is no algorithmic novelty (standard LP solvers are used, no discussion of scalability), and only toy experiments on synthetic data are presented. Finally, I don't think this topic fits well with ICLR. (They have also submitted a version to ECML-PKDD 2016 journal-track, which may be a better fit.) ",1,5169
"The paper proposes an extension of the GP-LVM model for the problem of unsupervised learning with unbalanced categories. The aims is to avoid the dominance of the main category when modeling the data. The idea of the article is to decompose the learned latent representation in two pieces: one latent space is shared across all the data while another latent space is used to model each category. Such assumption is obtained by defining a new kernel function which is a sum of a kernel on the first latent space plus a category dependent kernel on the second space. Experiments are made on patches extracted from biological images. The article proposes a visualization of the latent spaces, examples of generated patches but also classification performances by using a (weighted) SVM on the latent space. The model is compared to other techniques, showing the effectiveness of the approach  The underlying idea is simple, intuitive and interesting. It is a significant contribution for a workshop paper and I like this approach. The two first pages of the paper are very clear, but after that the writing can be improved. Particularly, equations 4 to 8 are difficult to understand without reading the original GP-LVM paper since some notations are not defined : K_{u,u} for example. Moreover, I don't really understand how kernel k' is used in equations 6 to 8 since the category information (equations 2 and 3) is missing. Some additional informations could be provided by the authors on this part. Concerning the experimental part, the results are interesting and made on different setups (generation + classification). My only concern is the way the training set is obtained: the paper focuses on unbalanced categories which is clearly the case for the original dataset composed of 146,562 patches where only 550 are positive. So it seems to be a nice use-case... But the authors decided to only keep 5000 negative images creating a more balanced dataset. What are the results of the different methods on the original (really unbalanced) dataset ? The new one is not so ""difficult"".The paper proposes a latent variable model for generatively modeling imbalanced data. The data considered are of the form {(y_i, c_i)}_{i=1}^N where, for example, each y_i \in R^D represents a feature vector to be generatively modeled and each c_i \in {1, 2, ..., C} represents a categorical label. The data are imbalanced when, for example, C=2 and the number of observations in the first class is much greater than the number in the second, e.g. writing N_i \triangleq #{k : c_k = i} we have N_1 >> N_2. Modeling imbalanced data can be challenging because the latent features often end up modeling only the dominating class. This paper adapts the GP-LVM machinery to include both shared and private latent factors, thus allowing the private latent factors of each class to model class-specific details.  The construction is very natural. Indeed, it would not be surprising if there are some related ideas even in classical statistics, particularly in terms of factor analysis, and so for a full conference paper a related works section would be nice. However, the use of more general GP-LVM machinery is almost certainly new and has much greater reach, especially in the context of machine learning.  I was slightly confused by the `unsupervised learning' terminology, especially since the class labels are used in the definition of the kernel (Eqs. (2) and (3)). While it's probably accurate enough to include `unsupervised' in the title, it would be nice to clarify the use of the labels in other parts of the text and notation; in particular, you could write that you aim to build a conditional probabilistic generative model p(Y | c), and that while the labels are used to structure that model, the emphasis is latent variable representation learning with a generative objective on the data Y.  Overall, the paper is a great workshop paper and people will be interested in reading and discussing it.",0,5170
"This work aims to segment human body parts in depth images in real-time, using the approach of training a convolutional network on synthetic data.  This is a nice, simple approach and obtains what looks like promising initial results.  Significant gaps remain in order to turn this in a working system, most notably variation in body shape; however, the authors acknowledge this and it looks good for a preliminary result.  The paper seems a bit on the light side, even for an extended abstract, and I think it could be fleshed out a some more.  One possible suggestion, what do the results look like using the current system for a downstream joint or skeleton estimation?  Also, while accuracy numbers are reported, it is hard to place them in context without some baseline comparisons (maybe Shotton et al.?)  Overall, this is a simple system with what looks like good preliminary results, but I think would be stronger if the presentation in the paper were a little further developed.   A few additional comments:  - ""final layer of this network was shaped such that it had the same height and width as the input"":  Do the max-pooling layers downsample the resolution of the output?  Fig 2 shows that the network output is upsampled -- is the upsampled result what is reshaped?  How much spatial downsampling is there within the network?  Additional details of the network structure and sizes would be good to include.  - ""structure of the network was similar to Long ... skip layers were also not inculded"":  The skip-layers combining scales by adding is, I think, actually the largest defining feature of Long et al., so I'm not sure I'd call this a ""similar"" network.  It is a ConvNet that is simple and geared to the task, though.  - It would be good to show comparisons with other systems, at least Shotton et al.  - Relevant reference re: pose estimation with convnets:  Tompson et al., ""Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation"", NIPS 2014. This works at segmenting body parts from depth images using a convolutional network. The authors propose a modification of an existing architecture designed to fit on a ""consumer grade GPU.""  The authors train this network on synthetically generate data and applied on images captured by the Kinect camera.  Most of the evaluation was done on synthetic data, and no effort was made to establish the quality of the segmentation of real images except for visually inspecting the results on a few such images.  On the plus side, I think the results seem very reasonable on the synthetic dataset. On the down-side, I am very worried about the transfer capabilities which were not evaluated except by eyeballing, and the quirks in the training dataset (i.e., a single 3D model was used, arbitrary transforms were performed on the limbs, which may or may not reflect plausible positions for humans).  I see a lot of potential in this work, but it simply seems much too early to publish even as a workshop paper. I would have hoped for a more detailed explanation of the network and maybe some example images where the network fails to segment correctly the limbs + additional commentary on why is this happening. In addition, it would have made a lot of sense to also discuss in slightly more detail of the choice to put the 3D subject at a fixed distance from the camera, and maybe also explain whether the focal length of the 3D sensor has any effect on the segmentations produced by this algorithm.The paper describes a method for segmenting body parts in depth images of humans in real time. It uses an existing neural network architecture and a simple loss that has been used before for segmentation tasks. The model is trained on synthetic data and applied to Kinect camera images.   The paper is quite straightforward as far as segmentation work goes. I believe none of the components are novel, the only potential novelty is the application domain (segmenting depth images rather than RGB images). More segmentation work on RGB images could be cited, for example:  Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla ""SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation."", which uses a more advanced network architecture for a seemingly harder task.   There are strong assumptions in the approach: a single body type was used for training and it's placed at a fixed position relative to the camera. At test time, it is assumed that it is possible to accurately remove the background in scenes, although I am not convinced that this is the case for cluttered scenes.   The experimental results are lacking numerical analysis on real data-- only a single result for eyeballing is presented. It would have been helpful to show more results along with the failures of the model in more detail.  On synthetic data, it would have been helpful to show how accuracy varies with an increasing amount of noise. Most body parts other than torso have accuracy in the 60-70% range which is not necessarily very high. Finally, if the goal is to estimate body skeletons, why not directly regress to the 3D joint locations, like ""DeepPose: Human Pose Estimation via Deep Neural Networks"" by Alexander Toshev, Christian Szegedy.   For an application paper, I would have expected a little more in depth experimental analysis. Right now it's a bit thin even for a workshop submission. ",0,5171
"This paper proposes deep auto-resolution networks using unsupervised context information. The model consists of two convolution towers, one with 17 convolution layers on high-resolution image patch and the other with 40 inception-style layers on low-resolution full image. The outputs of both towers are concatenated and fed into a classifier or a regressor. The classifier is used to predict whether the high-resolution image patch is in the image and the regressor is used to predict the location of the high-resolution image patch inside the image. No annotations are required to train this network and experimental results show that the 40 layer embedding tower outperforms a random initialized network on cifar10 dataset.   The strengths of this paper include: 1) Learning from unsupervised context info is a very interesting topic. It enables us to train very deep networks to learn representations without any annotations.  2) Experimental results show that the network can learn some representations and improve the classification task.   I have the following concerns about this paper: 1) There is no network architecture details in the paper. Without that information, it is hard for other people to reproduce the results.  2) Also, no results reported in terms of the accuracy or error for the classifier and regressor net. It would be great to know how accurate the network can predict the context info and how much training data required to learn it. 3) The experiments only compare random initialization and the pre-trained auto resolution network. It would be great to compare with other unsupervised network as pretrained networks.    In general, I think the idea of this paper is great but the presentation and experiments are not satisfactory. The paper proposes an unsupervised learning strategy that uses high-resolution and low-resolution image correspondence as a surrogate task to initialize networks for image classification.  Two towers are trained to produce a feature representation that allows a classifier to determine whether a cropped or blurred patch is a part of an original image.  Tests on CIFAR-10 show that the learned representation for the low-resolution representation is a useful initialization for classification.  Pros: Overall, this is an interesting unexplored criterion for pre-training a network (even though pre-training seems to have gone out of style). On the experiments presented, the pre-trained solution is at least better than starting from random.  Cons: 30% relative gain is a nice result, but this is against a non-pretrained-baseline.  How does this compare to stronger efforts that include some form of pretraining?  Considering the long list of methods that provide a gain for this benchmark, more comparisons seem important. This is a nice idea, but I am not sure if it is being used to do something really novel yet, in comparison with Siamese networks or convolutional autoencoders.  There seem to be two ways for the network to solve the high-res-patch/low-res-image matching problem. The lazy way - downscale the patch, and compare it to small regions of the image. The clever way - look at the patch, deduce what it is, i.e. dog fur, and then check if that matches the image, i.e. is it an image of a dog, or possibly a fur coat?  If the network is solving the problem the lazy way, then the results should be similar to what you would get using a convolutional autoencoder. If it is solving the problem the clever way, then this a great new way to unsupervised learning for images. The experimental evidence seems to be rather hastily put together; it is not clear yet that advantage is being taken of the difference in resolution patch-vs-image. ",1,5172
"Paper summary:  This paper proposed a generalization of Residual Network (ResNet)(, which turns out to be an ordinary convolutional network. Based on this idea, it proposed the convolutional network initialization method using ResNet, and the ResNet in ResNet architecture. In addition, it also explored the data-depend forgetting gate.   Pros:  1. The forgetting gate idea was interesting and substantially different from both the ResNet and ordinary convolutional network. It is worth further explorations.  2. ResNet in ResNet (RiR) was an interesting idea. However, since the ResNet Init architecture was questionable (see “Cons”), building RiR with real ResNet (not ResNet Init) would be more reasonable. 3. Performance on Cifar-10/100 were quite good.   Cons:  1. The proposed architecture was essentially an ordinary convolutional network. Although the insight was interesting, but no clear evidence was provided on why the ordinary network can be better than ResNet. After all, the improved performance of ResNet might be due to the constrained architecture. Making it more flexible as proposed in this paper might be harmful. The underperformance of ResNet Init over ResNet Table 1 demonstrated the concern.  2. Initializing ordinary neural networks with ResNet was interesting (Table 5 Left). But the performance for ResNet was not reported. If ResNet Init outperforms ResNet Init, ResNet Init will become not very useful.    3. The good performance on Cifar might attribute to the wide architecture not rather than the proposed method.   Overall:  This paper provided interesting ideas and insights for ResNet. Some experimental results were promising. But the flaws in the method were somehow significant to me, which hindered me from recommending it for acceptance. I think more exploration on the RiR and “forgetting” idea is more promising than selling the ResNet Init.   The authors propose an initialization scheme based on some comparisons to the ResNet architecture. They also replace CONV blocks with the proposed ResNetInit CONV blocks to obtained a Resnet in Resnet (RiR). These experiments are needed, the connections made between the models in the paper are interesting.  That being said, a few recommendations to the authors:  - The discussion of ResNet Init and RiR is not very clear and I did not understand Section 2 well on my first reading. Please also expand more clearly on the differences between the 4 models in terms of the runtime, or number of parameters, etc. How are the parameters W initialized? This is not mentioned in the paper as far as I can tell, which seems like a very important point - are they drawn from gaussian?  - I would encourage the authors to focus their contribution and not add orthogonal half-baked experiments. For example either develop the forget gate ideas and report on them properly or I recommend not including them at all.  - Are the authors certain that only 5 papers from the entire body of scientific literature is relevant to this work?  I am slightly leaning to accept this work for workshop submission, provided that the paper is clarified, the contribution focused, and that the differences between all the architectures are better compared (e.g. FLOPS? or Wall clock time? or Parameters?)The authors propose a new way to initialize the weights of a deep feedfoward network based on inspiration from residual networks, then apply it for initialization of layers in a residual network with improved results on CIFAR-10/100.  The basic motivation for this paper is interesting, but as of now there is a lot of missing information and the report feels rather rushed. In particular:  The abstract is inaccurate with respect to the experiments actually performed in the paper. An architecture with the ability to 'forget' is only mentioned without detail towards the end of the paper with a single experiment.   Introduction: - 'Residuals must be learned by fixed size shallow subnetworks, despite evidence that deeper networks are more expressive'.  The proposed RiR architecture can use a shallower subnetwork but not a deeper one compared to ResNet, so it doesn't fully fix this issue.  - ""even though some features learned at earlier layers of a deep network may no longer provide useful information in later layers. A prior of ...""  The mentioned highway networks and its variants (with/without gate coupling etc.) do have the ability to 'forget', and a stack of highway layers can learn subnetworks of various depths. Why is this not mentioned here (perhaps I misunderstood something)? Additionally the highway networks paper mentions successful training of 'unrolled LSTM' for very deep networks, which are also explicitly used by ""Grid-LSTM"". These have forget gates. Additionally, an unrolled/Grid LSTM also has two streams along depth similar to what is proposed in Section 2, so I'm not sure how original this basic motivation is. It's okay if it's not original, but connections to existing work must be clear.  - ""We propose a novel architecture..."" Inaccurate currently, similar to abstract.  Section 2: -From what I can tell, the proposed generalized 2 stream architecture is never actually used. Instead an initialization is used which lets a usual layer implementation behave like the proposed architecture at the beginning of training. This is an incomplete evaluation of the proposal, and makes hard to say how valuable it is.  Section 3/4: Overall, apart from relations to highway networks, unrolled LSTM and Grid LSTM, the presented results seem preliminary even for a workshop contribution. This is because while some consistent improvements are shown for the initialization (wide RiR vs. wide ResNet), it's unclear what the reason for this improvement is.  One would expect the motivation 'problems' mentioned in the Introduction to lead to difficulties in optimization (not necessarily generalization). But I doubt that the improved results obtained are due to better optimization, since it is likely that all networks were optimized well (these networks are not too deep). Is it just a purely empirical observation then, that this initialization appears to result in better generalization? If so, this should also be stated very clearly.",1,5173
"This submission describes an application of recurrent neural networks to sequence prediction in electronic health records.  The core ideas and techniques are not novel, but the applications work itself is compelling and very interesting.  This results are good, the baselines sensible, and the explanation clear.  I think this kind of work is an ideal submission to the ICLR workshop track.This paper tackles disease progression modeling by training an RNN on an Electronic Health Record (ERH) dataset to predict disease diagnosis and medication prescription along with their timing.  This is an instance of a multilabel marked point process modeling task. The current two main classes of techniques used to solve the task are continuous-time Markov chain based models and intensity based point process modeling. The work presented in this paper distinguishes itself from these approaches by proposing a solution that is straightforward to generalize to nonlinear and multilabel settings and does not make assumptions about the data generation process.  The paper makes three main contributions:  * It claims to obtain good performance for recall @10 and @30 on the task. * It proposes an efficient initialization scheme for RNNs using Skip-gram embedding, which improves accuracy and speed. (Note: it is unclear to me what speed means in this context; is it convergence speed?) * It shows that the features learned by the model are useful in a transfer learning context, where the trained model is used to initialize a model trained on a smaller dataset coming from a different health institution.  The task is clearly explained and contextualized, and the real-world benefits (facilitate patient-specific care and timely intervention, reduce healthcare cost) are well justified. The main contributions are well outlined.  The paper claims that the model performs with similar accuracy to physicians, but doesn't seem to present evidence to back it up. From an outsider's perspective, I wonder why recall is the only performance measure considered. Aren't we also interested in reducing false positives? Still from an outsider's perspective, it's hard to evaluate how high of a bar the other baselines represent. It doesn't seem like any comparison is made with current approaches to solving multilabel marked point process modeling tasks.  In summary:  + Well written + Concrete real-world applicability + Application of RNNs to a new class of problems + Task clearly explained and well contextualized - Claim that the model performs with similar accuracy to physicians does not appear to be backed up by evidence - Does not appear to compare against current approaches to solving the task - Unclear whether the performance results are for the training set or for a held-out test set  I think that the work presented in this paper is novel and interesting enough that it should be accepted, despite the concerns I have with the performance measures.This paper presents an applications of RNNs to predict ""clinical events"", such as disease diagnosis and medication prescription and their timing.  The paper proposes/suggests: 1. Applying an RNN to disease diagnosis, medication prescription and timing prediction.  2. ""Initializing"" the neural net with skipgrams instead of one-hot vectors. However, it seems from the description that the authors are not ""initializing"", rather just feeding a different feature vector into the RNN.  3. Initializing a model that is to be trained on a small corpus from a model trained on a large corpus works. Concludes: information can be transferred between models (read across hospitals).  Claims: 1. Better recall @10 and @30 on the task. 2. Improved speed due to the skipgram initialization (I'm assuming this is convergence speed)  What I like about this paper: 1. Well written. 2. It's a neat application. Practically minded and the conclusions would have practical consequences.  What I think can be improved: 1. More thorough experiments, comparisons to continuous time markov chain models and intensity point processes. 2. More detailed results: I would like to have a better view of the data, in particular, I would like an explanation for why ""most freq visits"" is performing so well. 3. A clearer description of the problem. For instance, the authors suggest that continuous time Markov models were used for similar tasks, but it seems like their task is discrete time. If not, how are precision and recall measured here. ",1,5174
"The authors present a framework that can quantize Caffe models into 8-bit and lower fixed-point precision models, which is useful for lowering memory and energy consumption on embedded devices. The compression is an iterative algorithm that determines data statistics to figure out activation and parameter ranges that can be compressed, and conditionally optimizes convolutional weights, fully connected weights and activations given the compression of the other parts. This work focuses on processing models already trained with high numerical precision (32 bits float) and compress them, as opposed to other work that tries to train directly with quantized operations.  Results seem good (trimming AlexNet model from 32-bit floating point to 8 bits with only .3% degradation), however I am not familiar enough with this domain to know how this compares to other quantization work and cannot comment on originality.  While this is a very useful tool to have for some people, it is not very significant from a scientific point of view.  Comment: - The tested networks do not have batch-normalization layers I assume, batchnorm is pretty standard nowadays and thus dynamic fixed point may be much less useful when things are normalized. The paper would be stronger if it showed results for more recent architectures as well and answer this point.   The paper proposes a framework for quantizing the weights & activations within models trained in the Caffe framework, dramatically reducing the memory and computation requirements for big convnets used for image recognition. This makes the models useful for deployment on mobile devices and other low-power platforms.   The system is well designed and achieves compelling results, across 3 different models (widely varying in size). The application is an important one.   Several points:  - The quantizations are performed independently for each layer. How can you be sure that the errors won’t compound up, making the final outputs inaccurate.  - Fine-tuning is an obvious improvement, so it great that this is a future refinement.   - It would be good to show results for the latest ResNet and VGG models which use 3x3 kernels. It isn’t clear how these will compress. Also some implementations use the Winograd transforms, but might also affect the precision that you can get away with.  In summary, it is an interesting piece of work and should be accepted to the workshop track.",0,5175
"This paper introduces the multiplicative (or tensor) recurrent neural networks for sequence modeling and does a preliminary evaluation on the task of language modeling.  Main issue of the paper is the core idea (tensor RNN) is not novel, and there are no citations to the papers employing similar ideas:  (1) Generating Text with Recurrent Neural Networks by Sutskever et al (ICML 2011) (2) Modeling Compositionality with Multiplicative Recurrent Neural Networks by Irsoy & Cardie (ICLR 2015)  Combination of bilinear product with the gated RNNs is, to my knowledge, novel, however the paper is not structured around that idea as its main contribution.  Another issue is the weakness of experimentation, as well as it being unfair in terms of parameter size (which is already addressed by the author). To my understanding, the author compares models with a single set of hyperparameters, therefore the results are also prone to the randomness due to this choice. There should be some degrees of freedom for hyperparameter tuning to reduce this randomness and make a fairer comparison.  Pros: (1) Gated tensor RNN idea is novel. Cons: (1) The main contribution (tensor RNN) idea is not novel. Lack of citations to relevant papers that used tensor RNNs. (2) Experimentation is weak and unfair in terms of sizes of the models being compared. Thus the results are not conclusive or convincing.Quick Googling of related methods pointed me to ""Modeling Compositionality with Multiplicative Recurrent Neural Networks"" by Irsoy & Cardie, which was published last year at ICLR. They also use bilinear tensor products in the context of recurrent neural networks. Authors of this paper extend the idea for LSTMs where each matrix product is replaced by bilinear tensor products.  The experiments were run on the w 100M bytes of English Wikipedia. The results seem to imply that the proposed method (called GRTN) is significantly better than regular LSTM. It appears, however, that the GRTN uses significantly more parameters than the other approaches which makes the comparisons not necessarily valid. What is more, the log likelihoods of all methods seem to be significantly worse than in ""Generating Text with Recurrent Neural Networks"" by Sutskever et al, which was published in 2011.   Pros: - I haven't seen other people trying to use LSTMs with bilinear tensor products, which might be an interesting extension Cons - the experimental section is very lacking - the paper misses references to important related work",0,5176
"The authors propose to blend any two architectural components as the time of optimisation progresses. As the time progresses, the initial approach, e.g. employed rectifier, is gradually switched off in place of another rectifier. The authors claim that this strategy is good for a fast convergence and they present some experimental results.  Pros: - recognition of the convergence problem e.g. with drop-out - idea of evolving objective  Cons: - as the network switches between two approaches, it is unclear what is the closed form loss that the network optimizes - not clear what are theoretical guarantees of such optimization or the landscape of the local minima - the results indeed show some improvement, however, is this amount of improvement statistically significant and justified at a cost of even more obscure optimisation process? - lack of clear timing analysis - as the authors propose an approach which supposedly helps fast convergence, why not provide detailed plots of objective/accuracy vs. epochs?A common setting in deep networks is to design the network first, ""freeze"" the network architecture, and then train the parameters. The paper pointed out a potential dilemma of that, in the sense that complex networks may have better representation power but may be hard to train. To address this issue the paper proposed to train the network in a hybrid fashion where simpler components and more complex components are combined via a weight average, and the weight is updated over the training procedure to introduce the more complex components, while utilizing the fast training capability of simpler ones.  The paper is mainly presented in an empirical way, showing the performance improvement one can obtain from that. The theory is a bit lacking: for example, a proper decay schedule between the simple and complex components may be critical for convergence, and right now it is mostly setting by hand via hyperparameter \tau. However, the paper does a proper claim of its contributions and does not exaggerate it.  I think this would be an interesting empirical paper to be presented as a workshop publication.",1,5177
"This extended abstract proposes pruning methodology for the weights of an already trained deep neural network for object detection. This particular method applies to R-CNN style object detection approach, where the same network is applied to a lot of proposals. The paper hypothesizes that if the post-classifier network yields zero values for some activations on the whole image, then the same unit will never give non-zero values when network is applied on any of the proposals. This suggests a recursive algorithm to prune the network weight matrices based on the activations of the network on the whole image. The paper presents two pruning strategies, the first one guarantees equivalent activation output on the whole image. The second one is an approximate version that might change the output of the network. The various pruning methodologies are then evaluated on the VOC detection benchmark and demonstrated to be able to prune up to 60% of the weight matrices without effecting the overall quality of the output on the proposals significantly.  The positive: - The idea is sound and is relatively easy to implement for the R-CNN setup.  The negative: - The idea is based on an assumption that is not justified theoretically. The practical evidence for the activation is not presented in the abstract, but assumed silently. - The traditional R-CNN method performs poorly already on the small objects. The expected failure mode of this method is also on the small objects, so the comparison graphs do not have the potenetial to measure this failure mode easily.  - The traditional R-CNN method of applying the post-classifier in separation has been obsoleted by applying SPP in the Faster R-CNN setup. The gains theoretically achievable by this algorithm are not very relevant in the big picture since SPP pools features from the globally applied network activations anyways. - The idea is very specific to a special type of (already obsolete) detection procedure and is not likely to generalize to settings other than this.The paper presents methods to reduce the number of parameters of network for proposal based object detector (e.g., R-CNN), which can potentially accelerate the inference. The proposed method prune the network based on the network activation of each image, and then a smaller network can be applied to all different object proposals in an image. It is based on the assumption that network units with zero activation on the whole image cannot have nonzero activation on any object proposal in the image. Backward and forward pruning methods are proposed to prune the unit with zero or near zero activation. Experiments are done on the PASCAL 2007 to show that the pruning does not degrade the performance significantly.  Pros: - Proposed methods are simple and well described.  Cons: - The key assumption does not have theoretical proof, or experimental support. - There is no baseline comparisons in the experiments. It is not clear if a random pruning will be as effective as the proposed methods. - The proposed methods are designed for detectors that evaluates each proposals independently, but it is based on the fast R-CNN, which obsoletes this routine (see the RoI pooling layer). The computation of all the convolutional layers are shared in fast R-CNN. This makes it less interesting to apply the proposed methods on the convolutional layers.  - Actually, only the experiments on the full connected layers are shown. The paper presents two methods of reducing the number of parameters in a ReLU-based convnet based on pruning weights that result in a high proportion of inactive (activation 0) units.  Pros: -The method is simple, well-motivated, and well-described -Computation is reduced significantly while sacrificing little to no accuracy -Method is applicable to any convnet with relu activations, and could be trivially generalized from fully-connected to convolutional layers  Cons: The experiments are somewhat limited in that the pruning trick is evaluated on just two layers of one network for one problem, and the more recent detection approaches (Fast(er) R-CNN) do not have the same degree of issues with evaluating many proposals that R-CNN did, due to the ROI Pooling layer (first proposed in SPP)  Though the evaluation is limited and addresses a problem that isn't as big of an issue now as it once was, the method is general enough to be worth readers' time for the short paper.  Furthermore, my expectations for evaluation aren't as high for a workshop paper than in other venues, so I don't see the evaluation as being too much of a drawback for this work",1,5178
"This work proposes an encoder-decoder convolutional networks for segmenting medical images. Images with multi-resolutions are encoded by a pretrained encoder, and two decoders are employed to perform segmentation and reconstruction, respectively. The model is trained on a synthesized dataset and one result of real image is shown.  The novelty of the work is quite limited (and lack of detailed analysis) since simply two extra features are added on top of the typical encoder-decoder convolutional networks, namely (1) multi-resolution images and (2) two decoders (one for segmentation and the other for reconstruction) as regularization during training. Feature (1) has been known to be helpful for segmentation as demonstrated by [1, 2, 3] and Chet et al. arXiv 2015. Employing multi-resolutions brings about 2% (in meanIOU), but the experimental details are missing, such as how many resolutions are employed (and some ablation analysis of experimenting with multi-resolutions). Feature (2) is claimed to be useful in the paper, while no analysis experiment at all to support the claim (e.g., what will happen in terms of training speed and performance accuracy experimentally if no regularization employed?). Besides, the encoder-decoder convolutional network framework is typical. A few more baselines, such as Segnet, in the experiment are needed for comparison.  Employing deep neural networks (especially convolutional networks) to segmentation is a hot topic, while the lack of cited references in the paper cannot reflect this. The authors should elaborate more about the related work section. For example, [4] is also relevant to the work from the aspect of convolution-deconvolution network for medical images.  The paper also lacks a few analysis experiments to demonstrate the effectiveness of the proposed model. Specifically, (1) the claim that the usage of nearest-neighbor upsampling is helpful for faster convergence during training, (2) the performance gain of the proposed model is unclear (good simulated dataset or model generalization), and (3) what is the reconstruction result (and how good it is), and is it possible to show its effect on preventing overfitting? Even though the dataset for medical images is relatively small, the authors could try to analyze the models on other larger segmentation datasets or could employ some data augmentation, since I think it is crucial to analyze the proposed model carefully. Furthermore, some details are missing, such as how to synthesize the dataset.   [1] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. PAMI 2013.  [2] G. Lin, C. Shen, I. Reid, et al. Efficient piecewise training of deep structured models for semantic segmentation, arXiv 2015.  [3] P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene parsing, ICML 2014.  [4] O. Ronneberger, P. Fischer, T. Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI 2015.This paper presents a convolutional encoder-decoder architecture for segmenting materials (carbon fiber). The idea is to use the method of Wang et al. (2015) with a couple of small modifications: (1) use nearest neighbor for upsampling at the decoding stage (2) feed in multiple scales of the input image (3) add a reconstructive layer that predicts the input image (in addition to the output layer that predicts the segmentation mask).  The paper trains this model on simulated data. The claims are that (1) on simulated test data it works well (2) the multiscale trick is beneficial (3) the additional reconstruction cost is useful to prevent over-fitting. A few comments:  - I think the ""snowball training method"" would need some clarification. - The usage of simulated data would need more analysis -- it is unclear how faithful the data statistics are to the real dataset, so some comparative images would be good. Describing how the data was generated would be helpful. - There are no comparisons with other methods so it is unclear how good the proposed model actually is in comparison with other alternatives. - Results with/without extra regularization would be helpful, as well as a deeper analysis (metrics such as training and validation errors over time so that we can see the effect of over-fitting etc). - How would the authors train if they had access to some labeled data (which I assume is also a realistic scenario)? There is an obvious way to mix the supervised and unsupervised objectives, but that's not the only way.  All in all, this paper applies some pretty standard tools on a potentially novel set of data, but otherwise falls short in terms of novelty and depth of analysis.   This paper proposes to use an encoder-decoder architecture to perform segmentation in order to automate nondestructive evaluation of materials. This is unfortunately not suitable as an ICLR workshop contributions for several reasons:  1- this is a narrow application with no learning or representation contribution nor much generalization potential 2- even within the scope of application, the architecture is proposed without any rationale as to the choices made, nor any baseline with simpler methods or architectures. 3- the results of the proposed methods are presented in a way that is very hard to interpret: ""qualitative"" results with just 5 images of segmentation and no quantitative evaluation, the quantitative evaluation is given only on simulated dataset. There are no comparisons across variants within the same system  to justify choices made or contributions of pieces of the architecture(e.g. one decoder instead of a pair of decoders, to justify the sentence ""We found this method [using 2 decoders, one for the image and one for the segmentation] of regularization crucial for training the encoder-decoder network with entire images as training without it would result in severe overfitting of the training set"".  ",0,5179
"I understand that most of this work was done before AlphaGo came out, but the submission deadline for the workshop was a few weeks later, so the new results need to be taken into account -- for example, the abstract can no longer read “attain competitive results against state of the art convolutional net-based Go-playing programs”.  Thompson sampling in the context of MCTS is not new (e.g. Bai et al NIPS’13), even if I have not seen it applied to Go: if this is a key innovation, the results should have provided a direct comparison with UCT (for the same nets/rollouts).  Heavier rollouts (with convnets) are plausible, but again, there is no direct comparison (using the same PPN and same computation budget) to the cheaper pattern-based rollouts.  In summary, the paper focuses on outdated performance metrics instead of scientific insight and clean comparisons of the proposed methods. This paper is directly related to several previously published papers that use deepnets+MCTS to play Go.   The new idea in this paper seems to be the use of Thompson sampling during exploration. But no quantitative comparisons are presented to really understand the effect of this choice.  Besides that, this idea has been proposed in previous papers -- ""Thompson Sampling Based Monte-Carlo Planning in POMDPs"" and ""Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search"". ",1,5180
"The authors consider the problem of dynamically adapting a neural network to new input data (e.g. new classes), for which the required features might not have been learned. A neurogenesis method is proposed, that progressively introduces new neurons to the model as more data is presented to the network.  The method is motivated from a cost perspective, by explaining that adapting an existing model to new data is cheaper than storing the previous data and training a new model as soon as new data is being observed. The method and motivations are clearly explained and summarized.  The considered MNIST deep autoencoder model with sequentially introduced digit classes is a reasonable choice for testing, although the retraining costs are probably becoming more important when considering more complex models such as large convnets.  A technique called ""intrinsic replay"" is applied in addition to the neurogenesis process. It seeks to generate examples that are similar to those observed previously, and feeding them to the neural network in addition to the newly observed data.  The properties of neural network learning dynamics with time-dependent data distributions is still an open question, that has not been very extensively studied, but a highly relevant one.Interesting idea about how to increase the representation power of a DNN as we receive new (different) data. The paper is brief and clear which is welcome. The weakness of the paper is the different heuristics (hyper parameters ) that use in order to decide when to increase the DNN and the ""lower learning rate"" to maintain stability in the old weights. The influence of these parameters has not been tested although. Moreover would be interesting to use a different dataset beyond the mnist that use to be use to ""sanity check"".  There are several sentences like ""This step relates to the notion of plasticity in biological NG"" that would need some reference.   This paper presents an algorithm for growing the representational capacity of an autoencoder (AE) network in a data-dependent way. The idea is interesting, but the presentation is rough (as might be expected for a workshop submission). For example, see the several below sections:  > DNNs are … [not] well suited for transfer learning (TL)  Actually, many of the results shown in hundreds of papers over the last few years have been built on the effectiveness of transfer learning using DNNs.  > Samples from old classes are generated via hippocampus- inspired ”intrinsic replay” (IR) by retrieving a high-level representation through sampling from the multivariate Normal and Cholesky decomposition of the top layer of the full encoder network and then leveraging the full decoder to reconstruct new data points from that previously trained class.  Please fill in additional details here. Is a multivariate Gaussian distribution being fit to the top layer code? If so, how is it updated to account for the new units, particularly if the old data is not used? Is there any reason to suspect a Gaussian distribution is remotely a good fit to the layer code?  > Control 1 (TL+IR) - an AE trained first on the subset digits 1 and 7 and then retrained with one new single digit at a time with standard TL,  What is “standard TL”?? There is not a single standard transfer learning setup. More details need to be provided about what is actually happening in the Control experiments; without them the results are hard or impossible to interpret. ",1,5181
"This paper uses an unsupervised generative model proposed at NIPS 2015 as the basis of a new biclustering algorithm that alleviates the scalability limitations of similar algorithms. The new algorithm demonstrates excellent performance in an experimental comparison against 13 other biclustering algorithms on 4 simulated data sets and 3 real data sets.  The paper is clear and of high quality. Algorithms like these are widely used in bioinformatics, so this novel work could have high impact if it becomes a standard tool for analyzing gene-expression data.  Pros: - Clear presentation. - Large, high-quality experimental comparison to 13 other methods, on both simulated and real data.  Cons: - NoneThis paper formulates the problem of biclustering as a sparse latent variable model where one entry of latent factors represents one bicluster, and solved it by using a recently proposed Rectified Factor Networks (RFNs). Experiments were carried out on two simulation setups and three real datasets with comparisons against 13 algorithms.   +A principled solution to biclustering. +Solid results with improvements over previous arts. +Well written and easy to read  -""Laplacian prior on the parameters"". It will be better to present more details and to discuss how this prior influences the results (including the dropout). -Some presentation issues. Is there a reference to QSTAR? What is full name of IBD? ""Du to"" --> ""Due to""?Bi-clustering algorithms are relevant in many real applications. In this work, the authors address the problem  by means of  Rectified Factor Networks, that they us to formulate the problem in terms of a sparse latent variable model.  The paper is well written, novel, and the experimental section is extensive and convincing. I think that this work should be presented in the workshop sessions on ICLR.",0,5182
"The authors propose a technique to improve video object detection by applying a post-processing technique to single frame detections. The method falls into the standard “tracking by detection” paradigm and achieves a boost over single frame detection on the video detection task. The authors achieved 3rd place in the ILSVRC video object detection challenge, scoring 48.7 mAP versus the winners who achieve 67.8 mAP. Tracking by detection is a well studied problem (and the authors give a few references). This approach falls squarely into that line of work but the core object detector is modernized to be a CNN. Novelty is low, it’s an application of a fairly standard framework to a modern object detector. While the method did ok in the video object detection challenge, it lost to the first place winner by a huge margin (~20 mAP absolute difference). While a few older “tracking by detection” papers are given a full submission would require a much more thorough comparison and literature review. For example see Burgos-Artizzu et al. in BMVC13 on “Merging Pose Estimates Across Space and Time” who also apply a generalized NMS to video. Of course there exists numerous other such papers. Regardless, as it stands, it is unclear if there is any novel algorithmic contribution. It may be that the post-processing works better than previous approaches, but there are no comparisons to show this is the case. This was definitely a reasonable contribution to the challenge but does not meet the bar for publication. The paper proposes a dynamic programming based inference scheme for non-max suppression of detections in a video. This allows high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip, to improve the final video object detection performance.  The proposed idea has already been used in past work (for example, almost exactly in Finding Action Tubes, Gkioxari et al. CVPR 2015, I am sure there are numerous other papers using the same idea). I don't find the ideas presented in the paper to be at all novel, and dont think the paper makes a contribution substantial enough for publication.This short paper offers a useful, and straightforward, modification to R-CNN style processing to improve video tracking / temporal detection.  The idea may have limited novelty, but is not likely to be of broad interest to the ICLR community, since the proposed method leverages rather classic dynamic programming schemes. It does not appear there is a novel approach, e.g., to learn the model end-to-end from data. Unfortunately I cannot recommend acceptance to the workshop program, given that it is rather selective this year.",0,5183
"This paper explores the use of an algorithm from the evolutionary optimization literature as an alternative approach to Bayesian optimization for hyperparameters.  In particular, the authors propose the use of CMA-ES for the parallelized hyperparameter optimization of a deep neural network.  On one problem, they demonstrate that CMA-ES appears to reach better validation performance than a popular Bayesian optimization method.  This is a well written paper that is easy to follow and offers an interesting datapoint for Bayesian optimization and hyperparameter optimization researchers.  One concern, however, is that the empirical evaluation is too light.  The authors run a single optimization on just one problem and the experimental setup may have some issues.  In particular, the comparison is likely conflated by discontinuities in the optimization surface.  It seems reasonable to compare to approaches that take this into account and for which implementations are provided in the same package as the authors ran (e.g. PESC).  Also, the reported results on the CIFAR-10 validation set seem too good to be true, which makes one worry about the experimental setup.  In Figure 1, it looks like the GP-based approaches (EI and PES) experience major model fitting issues.  This would be suggested by the observation that they don't seem to improve at all after the first few function evaluations.  One concern is discontinuities in the objective function, which could be caused by having the neural net being trained diverge.  Looking at the hyperparameter bounds, it seems reasonable to expect this to happen (e.g. high momentum and high learning rate).  Various papers (Gelbart et al., Gardner et al., PESC, Snoek, Gramacy) developed constraints to deal with this issue.  Did the model diverge during training and if so, did you consider using the constrained alternatives?  The CMA curve never seems to sample close to the optimum (i.e. the best values are always extreme outliers).  That seems strange.  Has it just not converged to the optimum?  Validation errors below 0.3% sounds extremely low for CIFAR-10.  Typical values currently reported (i.e. state-of-the-art) are around 6% to 8% depending on the type of data augmentation performed.  The suggestion of using priors over the search space within Bayesian optimization seems very sensible.  Note that, Scalable Bayesian Optimization using Deep Networks does exactly this (using a prior mean function as a quadratic bowl centered in the middle of the space).  That is in a way analagous to the setup for CMA-ES here (starting with a Gaussian spray of points centered in the middle of the space).  The initialization seems like a major possible source of bias.  One might worry that the bounds are setup with the optimum near the center, which would favor the approach that starts with random points at the center.  It would be useful to experimentally validate this by starting the Bayesian optimization approaches at the center as well.  Wow, the bounds for selection pressure seem very broad...  Does this hyperparameter really vary the objective in an interesting way over a range of 100 orders of magnitude?  One might imagine that this could really confound model-based optimization approaches, unless the objective varies smoothly accross this space.  In the introduction, I don't think 'perfect parallelization' seems like a fair statement at all.  Random search and grid search offer 'perfect parallelization' but that doesn't not imply that these are better approaches.  I highly doubt that CMA-ES uses the parallel experiments more efficiently than other approaches.  In fact, one might view (as I do) that the need to distribute a random sample of points in CMA-ES is a major disadvantage.  It *has* to parallelize, which seems terribly inefficient.  Overall, the idea of comparing to CMA-ES seems like a really great idea, since it is the champion algorithm from the evolutionary optimization field.  I think this is a good start, but I am concerned as an empirical study it needs more rigor before it should be accepted.  Perhaps the authors can address the above concerns in their next manuscript.Summary: This paper investigates the use of the CMA-ES algorithm for embarrassingly parallel hyperparameter optimization of continuous (or integer) hyperparameters in deep learning models, specifically convolutional networks. The experiments show that this method can potentially outperform GP-based hyperparameter optimization, although more experiments are needed to draw any solid conclusions. As one example, I think it would be worth investigating how well random search does on this problem as a baseline. For the smaller 5-minute problem at least, the methods should be run multiple times to get error bars.  As far as I know CMA-ES searches locally, which could be a cause for concern if the function is multi-modal. I think that running CMA-ES with a few different initial distributions would be helpful to show whether it is robust to this effect.  Novelty: The idea of applying CMA-ES for hyperparameter optimization is not necessarily novel (CMA-ES was used to tune speech recognition models in [1], for example), but the idea is simple enough and potentially practical enough that it is worth investigating for deep learning. A reference to [1] should be added.  Clarity: The paper is well written overall, but there is very little information on the CMA-ES algorithm used in the experiments. I recommend adding an algorithm box outlining the CMA-ES approach used in the paper. There are also some non-standard hyperparameters, such as the batch selection, that should be briefly explained.  I’m not sure if the claim that there is no way to truly parallelize SMBO is true. For example, there is the q-EI acquisition function in [2].  From the experiments, there is a table of transformations that were applied to the hyperparameters. Were these transformations used for all of the methods? Hopefully yes since that would otherwise have a drastic effect on the results.  It would also be really helpful to see what the best hyperparameters are, particularly if they are near the center of the search space.  Significance: The nice thing about this paper is that it could result in a very simple and practical methodology. At the moment there are still several open questions, but if the conclusions hold up to more intense scrutiny then it could be very significant.  Quality: This paper is a straightforward application of a simple algorithm to a difficult problem and is of sufficient quality for a workshop paper.   Pros: - Simple application of a well known algorithm to a practical problem - Results show a lot of promise and merit further investigation  Cons: - Needs more experiments before any conclusions can be drawn - The paper is light on details of the design choices in the experiments - CMA-ES should be more thoroughly described  References: [1] Watanabe, S. and Le Roux, J. Black box optimization for automatic speech recognition. MITSUBISHI ELECTRIC RESEARCH LABORATORIES TR2014-021, May 2014  [2] M. Schonlau. Computer Experiments and global optimization. PhD Thesis. University of Waterloo, 1997 This paper proposes using CMA-ES for hyperparameter optimization. An advantage of employing this model is its clear parallelism strategy, which is difficult to achieve in existing approaches. This is an interesting direction of research, as it introduces a new alternative to popular hyperparameter tuning techniques. I am not familiar with CMA-ES, so cannot comment on the novelty of this idea. However, additional experiments are required to validate its empirical success — especially once this contribution evolves into a conference track submission.  One aspect that is not clear to me is the tradeoff between ""perfect parallelism"" and observation efficiency. That is, random search also features perfect parallelism, but past observations don't meaningfully inform future evaluations.   CMA-ES is claimed to perform well for larger function budgets, but this seems to be in contrast to the usual (and necessary) assumption of expensive function evaluations. The experiments presented report results for evaluation times of 5-30 minutes, but this is one to two orders of magnitude less than realistic neural network training times.  Apart from a more comprehensive experiment coverage, all existing experiments require multiple evaluations and corresponding error bars. For example, the experiment on the bottom right seems misleading. The first evaluation by CMA-ES reports a lower error than other approaches are able to ever attain, or attain just prior to convergence. However, this evaluation was done completely at random, so it is not indicative of the performance of this method in general, just of the initialization strategy.  In addition, while we expect CMA-ES to perform well for a large number of observations, GP-based approaches cannot scale to this regime. As such, this approach must be compared to an appropriate baseline (for example, Snoek 2015).",1,5184
"A clear description of the so called ""convolutional lookahead"" for RNNs in order to incorporate small windows of future context information in a similar fashion to bidirectional RNN, but in a way amenable to streaming decode.  The primary drawback of this paper is the limited experimental section - it would have been great to see more comparison over various settings of `tau`, ideally showing convergence to the full bidirectional RNN solution with larger and larger settings - the authors mention other experiments (""increasing future context size did not close the gap"", conclusion), but fail to show them here. One other experiment of interest would be to see the performance limitations of only using the convolutional lookahead, either by making the network use a single recurrent layer (bidirectional vs lookahead) in a fashion similar to Deep Speech 1, or making *all* layers use convolutional lookahead. Also showing the experiments in which ""using a regular convolution layer with multiple filters resulted in poor performance"" due to overfitting would be useful - perhaps the gap between them is due to capacity limitations in the lookahead?  Additionally, the paper mentions ""We note that much better performance can be obtained for both datasets by using a more powerful language model or more training data. We have observed that in both cases the improvements from the lookahead convolution layer are consistent with the smaller scale experiments shown here."" - it would be good to actually *see* these experiments in a table or description, rather than an offhand comment.  More experiments are always interesting, and actually showing the experiments mentioned in passing in the text would be even better, but the paper as it stands is already a ""minimum viable paper"" for workshop purposes. It clearly displays a particular technique, its uses, and some drawbacks and performance issues in application. Some of the mentioned experiments above are also described as ""future work"", so it is clear the authors know these are interesting directions of exploration, and ideally a subset of those results can make the workshop paper.Instead of computing the output for a unidirectional RNN at time point t using only the hidden state of the layer below at the same time, the paper proposes to use point-wise multiplication and addition from future states of a unidirectional RNN to compute the hidden states at the next layer (this is akin to a separate convolution on each of the feature dimensions). This, the authors argue gets it closer to bidirectional RNNs.  The idea is simple, and the paper seems to show results that there are gains from using the approach, but important details are missing that make it hard to judge whether the gains come from the model or not.   Specifically, the authors say on page that ""The next five layers are either all unidirectional (forward) or all bidirectional recurrent layers"" From this I would assume that row 1 of the paper is all unidirectional, and row 3 is all bidirectional, while row 2 is all unidirectional, except for the last layer which is a ""look-ahead convolution"" If that's the case the results are good.   However, the next lines ""We also compare with two baselines constructed by replacing the second-to-last layer with either a unidirectional recurrent layer or bidirectional recurrent layer"", make we wonder if this is really the case; the statement leaves open the possibility that Row 1 is bidirectional all the way, and then unidirectional, followed by the softmax, while Row 2 is bidirectional all the way and then a look-ahead convolutional layer etc... this result would be less convincing since it does get bidirectional inputs to the top layer..  An obvious comparison would have been unidirectional all the way, look-ahead convolutional all the way and bidirectional all the way. I'm surprised this isn't the one that is offered. And if it is indeed the one that is offered, the paper should writh the model section in such a way that its clearer.",0,5185
"The paper proposes an approach to re-set convolutional filters that are apparently not being trained well (and randomly reinitialize them) It proposes a criterion that is based on the gradients propagated to these filters.   The approach is not entirely novel, since it is well known that very low gradients indicate no learning. Regardless, forming the idea to eliminate and reset the filters during the training and evaluating the contributions on the entire performance is the contribution of this work.  + The paper is well organized and written clearly. + The approach is motivated well.  + The overall approach makes sense. + The advantage is that one can potentially train more compact networks (fewer but more useful filters and less redundancy among them)   - The approach itself is a little bit heuristic; also, looking at figure 3, if you cross-validated on the East crater region but tested on the West, there are regions of the parameter space where the gains in one set may be misleading for the other.    Overall the proposed method seems to be useful for the purposes intended. The paper introduces a heuristic which aims to revive ""dead"" units in neural networks with the ReLU activation. In such networks, units that are less useful may be abandoned during training because they no longer receive any gradient. This wastes capacity. The proposed heuristic is to detect when this happens and to reinitialize the units in question, so they get another shot at learning something useful.  It's a simple idea that is definitely worth exploring, but I feel the paper needs some work. My main concern is with the experimental evaluation -- although I appreciate that this is a workshop submission about preliminary work, the choice of dataset and model reduce the relevance of the results. The paper would also benefit from proofreading, there were quite a few spelling and grammar mistakes. The second paragraph of section 2 in particular has a lot of redundancy.  + simple idea, reasonably clearly explained.  + results are reported across large ranges of the hyperparameters (Fig. 3).  - the ""two datasets"" used in the experiments actually seem to be two halves of a single dataset, which is a bit misleading. The authors claim this dataset is well-known, but I don't think it is (at least not in the ICLR community). It is also quite small. I think MNIST would have been a much better choice, since it is the accepted ""toy dataset"" for experiments like these nowadays. This would make the results much easier to interpret.  - the network architecture used for this problem is really small (only two convolutional layers with 20 filters each), and since the point of the heuristic is to reduce the required capacity to solve a given task, evaluating it only on such a tiny model reduces the relevance of the results. I think scaling behaviour is especially important here.   * I'm not entirely clear on how the validation was handled, as it is only mentioned that the datasets are split into equal parts for train and test (no separate validation set). I think this is not a huge issue here because results are reported across large ranges of values for the hyperparameters, but of course it is important to be careful with this. It would be useful if this could be clarified in the paper.",0,5186
"In this paper, the authors present a fact that neural networks may become less efficient when odd-sized convolution kernels (like 3x3, 5x5 kernels) are used. The main consideration is from the implementation of the inner-product operation in hardware.  Figure 1 is quite intuitive: one can catch the main idea by taking a glance at it.  Experimental results are acceptable: with smaller kernels, the recognition performance is comparable while the FLOPs are effectively reduced. It would be better if this idea is verified on some larger experiments such as SVHN and ImageNet.  Minor things. (1) The mathematical notations can be more formal: in representing the network structure (20Conv5 ...), please use \rm{Conv} or \mathrm{Conv}, also please replace all the 'x' to '\times' (in 'Conclusion'). (2) Please magnify the font size in both figures, until one can read it clearly on a printed version of the paper. (3) The fonts of digits in Figure 2(a) and Figure 2(b) are different, which is weird.  In conclusion, this is a good workshop paper that tells the community a simple yet useful fact.The authors bring attention to the fact that odd-number filter sizes waste computational resources; even-number filter sizes can maximize the efficacy of CNN accelerators. They are able to reduce the complexity of LeNet and VGG11-Nagadomi network with comparable performance in accuracy.  Figure 1 is very good to understand what the paper is about.   Figure 2, on the other hand, is hard to understand; caption doesn't provide enough information.  For Figure 2a, why are there two sizes for each test error and normalized complexity bars? If they are the size of the first and second layer filters, why do  8x8, 4x4 filters have less complexity compared to 4x4, 4x4 filters? In Figure 2b there are two bar sets for 2x2 filters, later in the text it appears that one uses more feature maps, this information should be at least in the caption if not in the chart.  Overall, the idea is useful and good to keep in mind. ",1,5187
"This paper proposes a way of initializing vectors of entities from their definitions present in different kinds of resources. For a given entity its averages the vectors of the words presents in the entity description and assigns it as the initialized entity description. This has been done several times in the past work, and does not present any interesting insight. Although, the authors present improvements on some task, still the work has no insight to contribute to the community.This paper proposes to sum the distributional vectors of the words that occur in the WordNet?FreeBase glosses of an entity to derive a new distributional representation of the entity. Entity representations derived in this way are then used as initialization vectors in the TransE model, that generalizes relationships between entities in a knowledge base. The resulting approach obtains competitive results, in particular on a WordNet-based benchmark.  The paper is generally clear, the approach is simple and elegant, and potentially very useful, since it might address the important issue of how to assign distributional representations to out-of-vocabulary words. At the same time, as the authors recognize, the approach is not novel. Indeed, they should cite references going much further back than Chen et al 2014--at the very least, Hinrich Schuetze's seminal 90s work.  Given that the crucial advantage of the proposed approach is to extend coverage of distributional models, I found it disappointing that the authors did not frame the evaluation in terms of coverage. They mention in the introduction that 50-80% of the entities in the benchmark are not in the word2vec and GloVe spaces. How does the proposed model perform on tuples involving these entities? How do they compare, with respect to these entities, to models that are initialized with plain word2vec/GloVe vectors, restricted to the in-vocabulary entities only? (with random initializations for the oov entities).  More generally, given that models directly initialized with corpus-trained w2v/GloVe vectors for the entities are the most obvious comparison points, results should have been reported for the latter also with GloVe, and also for WordNet. The latter comparison seems crucial, since WordNet is where the proposed model shines.  Minor points:  I would have liked a short description of TransD, to get a sense of how much simpler the proposed approach really is.  Also, the difference between the filtered and raw setups should be explained. ",1,5188
"This paper is investigating an interesting problem for deep learning, i.e. how to robustify a deep neural network. The paper is well written and easy to follow. The method proposed in this work is novel to me.   Pros * An interesting problem * A feasible and efficient solution  Cons * There is no justification why adding Gaussian noise into the input is a better choice, compared with other random distribution. I understand Gaussian distribution is easy for computation. But it is not so straightforward why adding Gaussian randomness could improve the robustness of the method.  * The experiments do not show benefits of introducing randomness into inputs, for standard training/test setting. Compared with baseline and standard training + stochastic FF, the performance actually drops by more than 1% using the stochastic FF on CIFAR 10. Although certain robustness to adversarial noise is demonstrated, I am more glad to see the proposed method can improve the performance of standard setting and realistic problem.This paper proposes a new method to make CNNs more robust to adversarial noise by adding Gaussian noise to the input images and CNN layers. The paper is unclear in many parts  making it difficult to determine the exact computation being proposed. The best best of my understanding, the authors are proposing adding Gaussian noise to the input images and then at each subsequent layer sampling from a Gaussian with the mean and variance computed from the mean and variance of that layers input. This is not clearly explained and it is also not clear how to train this model.   Pros: - The stochasticity results in a slight improvement of the same deterministic network.  Cons: - The title and introduction suggest the authors are proposing a method to make CNNs robust to adversarial noise. However, adversarial examples are never used in the method and it seems to me that the method being proposed is perhaps just a general regularizing technique not specifically related to adversarial examples. This would be a fine contribution, but the way the method is motivated is very misleading. - The proposed method performs significantly worse than Goodfellow et al.'s (2014) method of training with adversarial examples (when evaluated on adversarial examples, which is the setting in which the authors are claiming to address). Combing the proposed method with Goodfellow et al.'s (2014) method provides a very slight improvement however this could simply be due to the regularizing effects of adding stochasticity. - Comparisons with other methods of introducing stochasticity to a network are missing. Some questions remain such as: what happens for noise distributions other than Gaussian? How does this compare to the simpler method of just adding noise to the input (and/or hidden units) and forward propagating as normal during training?  Some additional minor comments: - typo: page one ""...during training but not has been applied..."" - page 2 you say X in R^3, when I think you mean X in R. If R^3 was in fact meant then the channel index should be dropped I think? This short paper studies how to improve the stability of CNN architectures to adversarial noise. For that purpose, it proposes a Gaussian additive noise model applied to each layer, and the result is combined with adversarial training on large scale classification.  Pros: - The model is simple  Cons: - Unfortunately, this paper suffers from lack of quality, clarity, originality and significance. - The paper does not clearly motivate what is stated in the title. My understanding after reading the paper several times is that the authors train using a noise model that is not the adversarial one, but a gaussian additive noise injected at each layer, but then evaluate the performance at test time using the adversarial noise. What is the motivation to use Gaussian noise then? Is there any principled reason to consider this stochastic model in order to approximate the adversarial noise?  - Section 2 presents a series of qualitative approximations to justify why one can use Gaussian noise after rectifications and poolings. The resulting noise model thus seems to be the superposition of gaussian noise injected at each layer. The statements involving the Central Limit theorem are too imprecise -- if one considers for example recent CNN architectures with 3x3 spatial kernels, it is unclear to me why the output of the ReLu is well approximated with a Gaussian, since what matters is not how many terms you average, but how many independent (or uncorrelated) terms you average. - Section 3 presents the numerical experiments, but we see no error bars in the results. How statistically significant are these numbers?  - Also, I do not really understand the Imagenet results. How come the accuracy using purely Gaussian noise is significantly better than the error using Gaussian noise + adversarial noise, given that the validation set is corrupted with adversarial noise? This behavior is strange, and does not appear to happen in the cifar experiments.  - How do you interpret that the gaussian regularization indeed slightly better (if we believe the differences are statistically significant) when test examples are corrupted by adversarial noise, but slightly worse when no noise is added?   Overall, my impression is that this paper does not give enough rigorous insights into the problem they are addressing, neither theoretical nor experimental. ",0,5189
"This paper proposes conditioning sequence to sequence models based on rules stored in an indexible differentiable memory (basically attention). The idea is interesting, although somewhat encompassed by the more general Neural Programmer-Interpreters of Reed and De Freitas (http://arxiv.org/abs/1511.06279), although that paper perhaps had not been accepted to ICLR at submission time for this one. The two main problems with this paper, which tamper my enthusiasm, are that the model is not as clearly defined as it could be in section 2. The equations suffer from not have symbols consistently defined in the text, and an example could be given (other than in the figures). Mapping the mathematical description of the model to the figures (esp. figure 1) is not evident.  Regarding the evaluation, the task is synthetic and fairly small scale, so I would tone down the claims to generality of the model on its basis made in the second paragraph of section 3. A normal seq2seq baseline would have been nice. The results for the baselines are somewhat surprising (0% for the pointer network baseline). The no-rule part of the task is just copying, no? It is not clear why simple pointer networks cannot perfectly solver this, since an LSTM can with the constraint on sequence lengths being similarly distributed between training and testing. It is unclear why DNN underperforms an inner product similarity metric.  Overall, this is not a bad submission for a workshop paper, but I would have liked clearer explanation of the model and some indication of what further experiments could be performed to test it.The paper proposes a twist on the sequence-to-sequence learning paradigm in which an external rule-set specifies how to generate responses for inputs matching the rules. Furthermore, a recurrent encoder-decoder network with attention mechanism over the rule-set is presented as a solution to the proposed problem. The paper demonstrates that the architecture can learn to transduce sequences using the rule-set and that it is somewhat able to generalize to the case when new rules are entered into the rule-set (which causes a drop from 90% to 69% accuracy).  The rules consist of a pattern and result, the  pattern contains a string to be matched to the input and optionally a wildcard. The result contains a new string to be written and optionally a symbol to be replaced with all words matched to the wildcard.  The model merges three concepts: the attention mechanism of Bahdanau et al ""Neural machine translation by jointly learning to align and translate"", the external memory from Weston et al ""Memory networks"" and the ability to indicate individual entries in the input sequence from Vinyals et al., ""Pointer networks"". The main novelty of the paper is the task which promises to blend the power of recurrent neural network transducers with hand-engineered rule sets.   The model description is hard to comprehend. It would be helpful to expand it and write equations for all model parts. Some technical problems in the presentation of the model: - Symbols in equations are reused (e.g. the ""e"" in equation (1) and (2) refers to different functions), and are not used consistently (eq. (2) uses E to denote the rule-set, while equation (3) uses scriptR). - It would be helpful to label signals in Fig. 2 by symbols used in eq. (2).  - The relationship between Fig 1. and Fig 2. is not clear. Maybe a box should be drawn in Fig. 1 indicating which part of the model is presented in Fig. 2?  The chosen task (rewriting strings using auxiliary set of rules) seems to be difficult to solve with recurrent neural networks and the proposed architecture also struggles with it. The paper list various tricks to make training feasible, including extensive use of pointer networks to support the wildcard match in rules and introduce a pre-training step with extra supervision indicating which words match with the wildcard.  Pros and cons: + good idea to embed some form of human knowledge into neural transducers - the proposed task seems very artificial and limited - the proposed model essentially emulates an algorithm to solve string matching with wildcard capture, and requires extensive supervision to be trained. It is hard to see how the model can generalize to other tasks  Minor comments: for a input sequence -> for an input sequence ""a certain indicate vector..."" -> rewrite, the sentence is incomprehensible ""T(.)is used to transfer examples to their corresponding rules"" -> did you mean to transform examples with rules? Please clarify ""SRSS (...) has difficulty and scalability to extend to various..."" -> I presume you mean that the proposed task has the potential to mimic real-world tasks? Please clarify the sentence.The paper proposes a novel use of the external memory for recording rules about how inputs should be processed. A rule consists of an input and output sequences pair with option to have a common variable in them. Given an input sequence, the model first finds an applicable rule from its memory, and uses it to extract a substring from the input using a pointer network. Then, a output sequence is generated by an RNN conditioned on the target rule and the extracted substring. This way, it is possible to generalize to unseen rules during testing, although the performance was not as good as the seen rules.  The model description Section 2 was missing lot of details, especially the part about the decoder. Some symbols in equations 1, 2 was not defined in the text, which made it hard to follow what is exactly happening. The subsection ""hybrid encoder-decoder"" definitely needs more explaining or equations. It would be nice to have another figure about the decoder similar to figure 2.  Although the task considered is artificial, I think it is good start and exposing a flaw in current seq2seq architectures. However, it feels like the proposed model is tailored to this specific task, so might have a problem generalizing to different types of rules.   An idea of using the memory as a repository of learned skills is different from recent papers about external memory, and definitely an interesting research direction. However, the paper needs more clear and detailed description of the model to be accepted.",1,5190
"A few points about the idea:  (a) Depending on the dataset, and minibatch size, E(s) but not be very stable. This you can see in the peaks of the training loss. E(s) should really be over the dataset. If that is expensive than maybe computing a moving average (even if W changes from step to step, it should not drastically change). Regardless, the instability introduced by this normalization scheme is somewhat worrisome.  (b) The fact that hurts on the test set, leading to lower score is also a bit worrisome. IMHO what is going one is that the algorithm is to greedy, overfitting the current minibatch early on training, making me think again about E(s) not being estimated properly. Regardless, this points that some important detail is missing from its current form.Appears to be an unthorough set of experiments attempting to keep network weight matrices length preserving. There have been many papers along these lines e.g. arxiv.org/abs/1602.07714 or indeed explicitly length-preserving weights like in arxiv.org/abs/1511.06464  In any case, this looks like very little work on what could be a promising idea. It's just hard to tell from so little data.  I'm not sure what the standards are for the workshop papers, I guess they are lower than the conference's by definition. Perhaps this is acceptable.Interesting idea. Although not totally novel as pointed out in other comments, perhaps this new form of scale preservation is better or more efficient, although the results from what appears to be a *single experiment*, on MNIST, makes it hard to pass much judgement.  Intuitively, since rescaling is a hard constraint, gradient descent might have quite some trouble adjusting towards the convergence; which might be why in the end the unnormalized model gets a better test score.  The reasoning is interesting but it is not backed up by enough empirical evidence.",0,5191
"Inspired by the Nesterov's accelerated gradient, the paper proposes a new variant of Adam algorithm with a modified momentum term.  The contribution is incremental but might be of some interest if a broader benchmarking would be provided.  Pros: i) The proposed modification is easy to implement ii) Figure 1 shows an acceleration sufficient to consider the algorithm for further investigations  Cons: i) The empirical validation is rather poor. The algorithm should be tested in situations where the original Adam shows  state-of-the-art performance. Then we would better see how much we can gain with Nadam.  ii) Figure 1 can be an artifact of the chosen hyper-parameter values. A more detailed hyper-parameter selection should be provided for Adam and Nadam. One can fix some hyper-parameters and then select two others which seem to be the most important. Then make a contour plot with x-axis: hyper-parameter1 , y-axis: hyperparameter2, z: number of epochs to reach MSE 0.01 of the provided example. Alternatively, one can run hyper-parameter optimization. - This paper investigates a variant of the Adam optimization algorithm, where the first moment estimate (momentum) is replaced with a Nesterov momentum. The resulting algorithm, coined Nadam, performs well on a nontrivial task (convolutional autoencoder). - The paper is well written and easy to read. - It should be mentioned that the proposed method is computationally slightly more expensive than Adam. However this is not a big deal for models with weight-sharing (e.g. CNNs and RNNs). - The preliminary result looks promising  Some suggestions/questions: - Section 4 (experiment) mentions that only the learning rate is tuned for Adam/Nadam, but the hyper-parameter 'mu' is set to the atypical value of .975, which suggests it is (somewhat) tuned to this problem particular. Its value might be arbitrary, but it might be better if this was either set to its Adam default (.9) or tuned with the learning rate. - It would be useful to derive an upper bound on the magnitude of the weight update. Adam's simple known upper bound is an advantage and would be nice to have for Nadam as well. - Section 3 mentions ""It often helps to gradually increase or decrease mu over time"". Increasing mu_t over time makes intuitive sense (to counter the typically decreasing signal-to-noise ratio of gradients over time), but in which situations would it help to decrease mu over time? Is there published work that investigates particular schedules for mu_t?  Overall a very exciting potential improvement on Adam.",0,5193
"For the problem of anomaly detection in time series robot data, the paper evaluates a generative time series model. For this they propose to use variational inference (VI) with recurrent networks, so called Stochastic Recurrent Networks (STORNs) [Bayer & Osendorfer, 2014].  Positive points -	Important problem. -	Interesting application of VI and STORNs to robot data -	Quantitative evaluation on off-line data and qualitative evaluation on on-line data  Weaknesses: 1.	Clarity: -	The paper is not self contained, in the sense that all formulas (1),(2),(3) are not sufficiently explained, both symbols and content, to understand them form this paper. -	Abbreviations are not introduced before they are used, e.g. “VI”, page 1 2.	Experimental setup: -	Of the 10 waypoints, how many are actually traversed in each experiment? 3.	Experimental evaluation: -	The main goal of the paper, *on-line* anomaly detection is not evaluated quantitatively, only qualitatively, why? -	The off-line detection [Figure 1] could have compared to Milacski et al, 2015. -	The experimental results are not discussed sufficiently; it is not clear what conclusions are drawn from them. -	An important aspect for anomaly detection is, how likely the learned model (in this case the model + threshold) would transfer to an unknown set (test set). Unfortunately, the paper only presents results of the ROC curve which sort of is upper bound on the performance. The paper should determine the threshold for given false positive rate on the training set, and see how this generalizes to a unseen test set. -	This line of work seems to suffer from common training and test sets. It would thus be great if the authors would release the corresponding data and annotations.    Despite limited novelty the paper would make an interesting workshop paper if the experimental evaluation would have been carried out carefully.  I strongly encourage the authors to release their dataset with annotations to allow reproducible research. For the problem of anomaly detection in robot time series, the authors propose a generative approach with amortized variational inference and recurrent networks, in a similar fashion to Stochastic Recurrent Networks (STORNs) [Bayer & Osendorfer, 2014].  The paper novelty of this paper is the application: variational autoencoders (VAE) for anomaly detection, and in particular robot time series. Although the motivation of this paper is well explained and well written enough to convince me of the importance of this application, the description of the algorithm and experimental procedure lacks details and clarity which hold back the paper's potential quality.  Pros: - motivation well written and well explained - interesting application and approach to the problem  Cons: - lack of clarity/details:  - the newly introduced variables h_g and h_p in Eq (1) hide the actual dependencies between z_{1:T} and x_{1:T};  - the way the approximate posterior is parametrized in Eq (2) is not described at all (although the ""step-wise lower bound"" restricts the class of approximate posteriors);  - the anomaly detection algorithm from the trained VAE is not described well enough both for off-line and on-line (an equation might clarify), only the way to find the threshold is slightly described.  Releasing data + code and revising the paper accordingly would be very helpful in understanding the paper more.",1,5194
"This paper aims to address the problem of learning energy-based density models that have intractable partition functions. A core idea of the proposed approach is to use a *separate model* (Model B) to generate equilibrium samples from the energy-based model we wish to learn (Model A). If an oracle could provide a perfectly matched Model B for each given set of parameters for Model A, then learning via Monte Carlo estimates of the gradients would be straightforward.   This idea of having paired models is similar in spirit to well known previous approaches for learning generative models and density estimation (e.g. Helmholtz machines, Generative Adversarial Nets as noted), but it differs in several ways and there could be interesting ideas to explore in this direction. However, I have some concerns about the current proposal.  In particular, simply minimizing the energy of the generated samples (eqn 5) will not necessarily result in a generator (Model B) that follows and matches the equilibrium distribution of the target energy-based model (Model A). Indeed, the generator could obtain a (local) optimum by being degenerate -- with all the samples at the minima of one of the energy modes of Model A. It could also easily completely ignore some modes, or assign them incorrect probability mass, etc. (The toy examples shown do not seem to exhibit this pathology too badly, however it's not clear whether this is due to fortunate initialization and/or the very low dimensional nature of the problem.) In some respects this is analogous to the MCMC mixing failures that the proposed technique aims to circumvent.  I think the general idea presented here may be worth exploring and expanding on further, but in the current form it doesn't seem ready for presentation at ICLR.  As additional points: the clarity of the writing could be substantially improved, and a more challenging but still tractable toy-task (e.g. MNIST) would help to understand whether the potential problems noted with this method arise in more practical situations. On the algorithm side, one can imagine several possible heuristics to test for and guard against the failure modes suggest above.  Pro acceptance: Interesting direction for ideas tackling a broad/significant problem. Con acceptance: Insufficient evidence that proposed method works well on ""realistic"" problems, combined with a mathematically identifiable weakness whose presence is not discussed and has not been properly explored empirically.The paper proposes a new way of training energy-based probabilistic models using two separate deep networks. The first one estimates the energy function (deep energy model), while the goal of the second one (deep directed generative model) is to generate samples that would approximate samples from the deep energy model.  The authors are using an adversarial setting of Goodfellow et.al. to train both models: parameters of deep energy model are trained to discriminate between the real and generated data, whereas parameters of the generative model are trained to align the probability distribution p(x) between the deep energy model and the directed generative model.  One key assumption of the algorithm is that the distributions of the deep energy model and the directed generative model need to be approximately aligned during training. Simple 2-d simulation results show that this is indeed the case but it is not clear at all whether this would hold when modeling complex high-dimensional distributions: it is essentially saying that a simple deep directed generative model can accurately approximate the distribution of the complex deep energy-based model.  I would also encourage the authors to clean up writing/English, as well as weed out various typos in equations. For example, the authors are sometimes using \theta and sometimes using \Theta to mean the same thing.  There is also a typo in Eq 2: sum_i E_{\theta_I}.  I would also spend more time discussing the actual training of the model (Eq. 4), while reducing justification of why training deep generative models is hard (text around Eq. 1).  Finally, it was not clear to me what is the final output of the training procedure. Do we throw away deep energy model and stick with deep directed generative model, or the other way around? Under the assumption that both models are approximately aligned, then they are essentially modelling the same distribution.  I would also encourage the authors to compare to contrastive backprop for training their deep energy model. I suspect it might work much better compared to what the authors are proposing. The paper proposes interesting idea and tries to address a very hard problem common to all energy based or undirected graphical models.  There are two technical issues to be clarified: For Equation 4, wouldn't you need to multiply the det of the jacobian of x=G(z) : |det dz/dx| to be multiplied for every sample z that is drawn? How much more complexity would you have to incur if you had to backprop through this network every time you wanted calculate negative phase expectations?  Another issue is that if you are going to train the second network (similar in spirit to like a nonlinear sigmoid belief network) on the samples of the original energy model, why not just keep those samples around like the methods of persistent contrastive divergence or fast persistent contrastive divergence? If you keep the samples, that would be sort of like learning a non-parametric model of the negatives phase samples.  I think the bigger issues is that the idea is to try to use a directed uniform to multimodal generative model to model the samples from the partition function of the energy model. The idea is that if the directed model can learn a good multimodal representation, then the negative phase would be easy to ""mix"". However, if you could learn a good directed model, why not just learn that on the original training data instead!?  Perhaps to improve the motivation for the model, one can argue that the product of experts being learned is more interesting or more important than a simple directed uniform-multimodal generative model?!  For related works, there are long history of approximate inference models for addressing the negative phase sampling by learning the posterior, e.g. Wake-Sleep algorithm, and Efficient learning of DBMs. The authors could reference them and draw comparisons to those prior works.",1,5195
"This paper introduces a clever idea for unsupervised relation extraction (relation clustering) from text: the best relation r (drawn from a fixed set R) between two entities in a particular textual mention is the relation that best makes it possible to recover one entity from the other. The paper presents a couple of variants on this idea, introduces a model (based on a discrete-state variational autoencoder) that implements this idea, and shows solidly state-of-the-art performance on the NYT corpus.  I think that the core idea is intuitive but non-obvious, and that the paper evaluates it well. I'd like to see more discussion of the model's performance (i.e., error analysis) and more discussion of how the encoder portion of the model is constructed (mostly: what features does it have access to, and how much does this matter?), but the paper seems sufficient as is for a workshop contribution.In this paper, the authors introduce a new method for unsupervised relation extraction from textual data. In their approach, the relations are treated as latent variables in an auto-encoder like model.  The proposed model is composed of two parts: (1) an encoder, which predicts a relation between two entities, given a sentence that contains these two entities. This encoder is a feature rich discriminative classifier. In the paper, the authors propose to use a log-linear model with handcrafted features. As the authors point out, any discriminative model can be used, as long as the relation posteriors and gradients can be easily computed. (2) a decoder, which reconstructs the entities-relation triplet. More precisely, given the relation and one of the entity (and nothing else), the goal of the decoder is to predict the other entity. As for the encoder, many different models can be used as a decoder (e.g. matrix factorization based model). The authors propose to use the RESCAL model.  The encoder and the decoder are then trained jointly, by minimizing the prediction error of one entity (given the other one) and by marginalizing over the relations. The authors performed experiments on the New York Times corpus, showing that their approach outperforms a generative model for unsupervised relation extraction.  I think that the method described in this paper is interesting, elegant and general (as pointed by the authors, many different models could be used as encoder or decoder). The authors demonstrate on a large dataset (2 millions entity pairs), that their approach is competitive with existing generative models for this task. The paper is clear and well written. I would have liked to see examples of extracted relations and/or an error analysis of the model.This paper proposes approaching unsupervised, open-domain relation extraction via an encoder-decoder model, trained end-to-end. The encoder is never described very carefully, but is basically a conventional, human-designed categorical features soft-max classifier, from what I can tell. The decoder is then asked to predict one entity given the relation and other entity in a continuous space model: the entities are embedded, and the relations are represented as a matrix. Three models are investigated. The work outperforms previous work by Yao et al. (2011, 2012).  I think the general approach is novel, and interesting - the encoder-decoder factorization, and the way the decoder is trained by masking and asking it to predict one entity is novel, good, and opens up a space for others to try things. As such, I think it is an okay workshop contribution, worthy of acceptance.  Some aspects of the paper are not so good:  - The description of the experiment is very vague: re they using the same corpus and filtering as the Yao et al. and subsequent papers. I suspect not, since they say they end up with about 2 million entity pairs, whereas Yao et al. report 2.5 million. Why aren't they using the same corpus? What has been left out? You can't tell. What forms of preprocessing are provided. You can't tell.   - No qualitative output from the experiments is given. Other papers in this area typically show examples of the relation clusters induced, and the sets of entity arguments they pick up. Here we are completely in the dark except for seeing an F1 number.  - The paper seems to limit itself in comparisons to 2011-era work, and barely considers work since then. That is, looking at just work involving Limin Yao (their chosen point of comparison), this paper:      o Shows a comparison to the results of Yao et al. (2011), but doesn't really explain why their results for the methods of Yao et al. (2011) are 10 F1 points below the methods reported by Yao et al. on a ""similar"" corpus, except for the annotation ""(our feats)"", where the ""feats"" used are never explained, unlike in Yao et al. 2011.      o Shows a comparison to one method (HAC) used as a baseline in Yao et al. (2012), but doesn't cover the main result of that paper which does 6 F1 points better, presumably meaning that it is about equal to the results of this paper - the sense clustering of that paper could be argued to be quite analogous to what the encoder-decoder of this paper should be motivated to do to perform well.      o Except for a token cite at the beginning, there is no discussion of the ""universal schema"" work of Riedel, Yao, McCallum and Marlin (2013), which many would now regard as the standard go-to comparison for this line of work. Unfortunately, they use a different evaluation metric, so I can't do an easy comparison - but the authors of this paper could have - but they too show results signficantly above those of Yao et al. 2011.",0,5196
"The natural gradient has long been known to offer very interesting convergence properties (notably invariance to parametrization and convergence in fewer steps) compared to traditional gradient descent. However, natural gradient descent has not seen a wide adoption mainly due to its high computational cost. The update is essentially the same as in SGD but the gradient has to be multiplied (at each time step) by the inverse Fisher matrix which is a NxN matrix for a model with N parameters.     One important method to lower the computational cost associated with computing the natural gradient is the online adaptive natural gradient update which allows the computation of an on-line estimate of the inverted Fisher matrix (thus avoiding the need to estimate a full metric matrix at each step, and a full NxN matrix inversion at each step).  The present approach presents an adaptive online natural gradient algorithm for the Score Matching loss instead of the usual KL-divergence.  Compared to the KL-divergence natural gradient, a score matching gradient may be applied in settings where the model has an intractable partition function and where the traditional natural gradient method cannot by applied.  Aside a few typos, the paper is quite clear.  The contribution is new.  On the down side, the experiments are very limited (one synthetic dataset), but they do demonstrate that the approach can be practical in at least some cases.  This paper shows how natural gradient updates can be computed using a metric derived from the score matching divergence. The same technique could be applied to other divergences. The theory results are very nice. The experimental results are for a toy problem only.  The paper clearly presents its ideas. There are some minor language issues, but they don't interfere with comprehension.  As the authors touch on in the conclusion, this technique would need to be combined with other techniques to be useful for large problems (eg block diagonal approximations to the inverse metric). Out of the box, the algorithm requires storing and manipulating the dense inverse of the metric, whose size is quadratic in the number of parameters.  Some specific comments:  the framework of natural gradient -> the natural gradient framework and a two-layer model with the analytically -> and two-layer models with analytically  as the online learning algorithm such that $$, where -> for an online learning algorithm where $$, and where  The right side of the inline equation before equation 4 should be $A^{-1} - \epsilon A^{-1} B A^{-1}$.  maybe ""N variables"" -> ""N input dimensions""?  How was the learning rate eta chosen for SGD and ANG? You shoudl do a grid search? A more compelling experimental section would also include a comparison to quasi-Newton methods. RMSProp would be a natural choice (or ANG+momentum vs. ADAM).  I wonder if the MPF objective might also lead to a good metric. MPF consists of a Taylor series approximation to the KL divergence, so the corresponding metric might be very similar to the Fisher information, but without the intractable normalization constant. The paper presents a new algorithm based on information geometric principles according to the theory of information geometry. in particular the authors introduce a new metric over the space of probability distributions, derived from the Hyvarinen divergence measure. the derived gradient algorithm represents an alternative to the natural gradient evaluated with respect to the fisher information matrix. The authors test the proposed approach for the training of a simple toy neural network and show its superiority compared to standard gradient descent.  I think the paper is a good starting point for future works in the training of neural networks and also for other problems where natural gradient has been successfully applied in the past. The paper is well written, and I suggest to accept it for the conference.  Some comments:  Page 1, second paragraph: you write ""When we estimate the parameter \xi with a diverge D, its parameter space has the Riemannian metric matrix..."" it this this is not well written: the geometry comes from the choice of a specific metric, or from the choice of a diverge function, but it's not directly associated to the estimation procedure in my opinion. Please explain better what you mean.  Page 1, last line: to be more clear I would write ""is composed of the derivatives of the log-likelihood differentiated with respect to.."" similarly i would sat that the fisher information metric is composed of the derivative of the log likelihood.  Page 2: add a space before ""const"", i.e. ""+ const""  Page 2: third paragraph: ""it requires the average over the unnormalized statistical model such that"", it's not feel written, especially you should change the ""such that"" same paragraph: ""the approximate metric over THE input data""  Page 2: forth paragraph: ""the inversion of THE matrix""  Page 2, end of Section 2, can you be more precise when you express the dependence of the complexity on N. is this the number of inputs in the network?  Page 2, Section 3: ""Of the proposed methods"" remove S  Page 2, Section 3: ""N-dimensional probability variable"" this is not properly defined. i would say N-dimensional sample space, or N-dimensional random variable.  Page 3: ""In terms of step number"" I would say ""in terms of number of steps""  Page 3: Figure: how did you choose the step size for the gradient update? Figure: why didn't you add also natural gradient with the fisher information matrix? you say this method is better, however you lack the comparison.  Page 3: Table: what about the time complexity of the algorithm per iteration? install SGD slower?  Page 4 maybe remove A from 4th reference (A Philip David) ",1,5197
"Summary: This paper proposed an denoising auto-encoder like dictionary learning method for denoising recommender system data.  Novelty: Instead of using a feed forward network like in autoencoder, the proposed method uses dictionary learning based on optimization. Similar optimization techniques have been used for representation learning, but not for data denoising.  Concern: No quantitative results and comparison. Given the similarity, it is interesting to see how well denoising autoencoder does for this task.The paper proposed to use dictionary learning on recommender systems.  The approach is based on an existing approach on alternating minimization.   Since there is no experiment results demonstrating the effectiveness of the approach, it is hard to evaluate the effectiveness of the proposed algorithm. The mainstream of recommendation algorithms tried to find an implicit and linear representation for the ratings. This work suggest that a deep, non-linear representation could handle the noise properly.   Cons: There are few discussions of intuitions of the reason of introducing neural networks and no any empirical experiments.   ",0,5198
"The authors introduce a new method for embedding latent features in a model for automatically predicting triplet relationships within knowledge databases. The authors demonstrate substantial improvement in terms of the MRR (mean reciprocal rank) and Hits@10 on the FB15k-237 corpus.  Pros: The gains appear to be substantial on the single data set they evaluated.  Cons: The authors could provide more details about the model and training methods. Additionally, the authors could provide experiments across additional corpora to demonstrate robustness in their results. This paper aims to develop better models for knowledge base completion.  It proposes to represent relations in vector space, and use the dot product of the vectors of two relation r and r' to capture their similarity which are both observed for an entity pair (s, o). This model is directly compared with model N of (Riedel et al, 2013), which uses one single parameter for each relation pair (r, r').  The authors also add two weighting schemes in scoring: either uniform or a softmax function over all dot product scores.  Experiments on FB15k-237 show that their model outperforms model N, either as a single model, or combined with two other latent feature models (D and E).  * In general, I think this paper is clearly-written, but the model is kind of too incremental - substituting a parameter w_{r, r'} with the inner product of two vecotrs is a natural and intuitive idea.  * I don't totally understand that why selective weighting approach makes sense - since the scoring function is already a sum of scores, why do we need to add another weight proportional to that score?  * The notions of  ""latent feature models"" and ""observed feature models"" seem vague to me.  * Table 1: people usually don't use \odot to represent ""dot product"". This paper propose an approach to utilize relation embedding for automatic knowledge base completion. The novel part of this approach is to use a normalized weight for the similarity between two relation embedding. This work can be viewed as a further extension of (Riedel et al., 2013).   There is one part in the experiment, which I think is confusing. In the column t of both MRR and HITS@10, why adding O_s to D+E actually hurts the performance comparing to D+E+O_u? Based on my understanding, Model O_s is expected to work better than Model O_u. Since utilizing embeddings is the claimed contribution of this paper, I think it definitely needs further investigation, other than simply said it is caused by overfitting.  Overall, I think the contribution of this paper is incremental and the experiment is not fully convincing about the benefit of using relation embedding ",1,5199
"This submission presents an end-to-end neural model with attention mechanisms that is able to answer natural language questions against a knowledge base, after being trained by gradient descent on a set of question / answer pairs. One key idea is the hierarchical combination of so-called ""executors"", each performing one step of the query.  This looks pretty cool and should definitely be accepted for the workshop track (interesting idea and promising results). I wonder if this model could be extended to answer questions that require combining answers from multiple rows? (ex: ""what is the average duration of olympic games?"") Right now it seems a bit limited since it can only find answers that can be found in a single row.  Minor remarks: - DNN_0 does not appear to be actually deep (it is made of a single tanh layer) - F_T is not defined in eq. 1First, it does not clearly state what problem end-to-end neural network training of querying is solving, and presumes that it desirable.  It is not clear that this approach is really a viable option for much larger and more complex queries.  The paper also does not explicitly say which queries can be handled by the approach and which cannot (I doubt you can do SQL statements of arbitrary depth).  The synthetic dataset is not convincing from an NLP perspective, because the core difficulty in language understanding is handling the linguistic variation, of which there is presumably little of in the synthetic dataset.  It's unclear whether solving this dataset will actually help with real datasets like WikiTableQuestions.  Furthermore, the details of how the natural language for the QA task was generated is not clear.  From Table 1, I can't tell how much linguistic variation there is, which has a strong influence on how difficult the task is.  The comparison with SEMPRE is unsatisfying.  There are no details on how SEMPRE was used.  SEMPRE is a semantic parsing framework which does not specify the features, the grammar, etc., which have a huge effect on performance.  It's like saying that you used a neural network without specifying the architecture.  Also, is SEMPRE not working well because it needs to consider too many hypotheses?  Presumably if computation weren't the issue, SEMPRE would be better since the underlying function that one is trying to learn is actually a logical one, so SEMPRE would be a better fit.  The first sentence is a bit off: there is much work on question answering on knowledge bases prior to the cited works.  Even without going all the way back to classic AI systems such as LUNAR and CHAT-80, it would be good to mention classic statistical semantic parsing methods (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005), etc.  The last sentence of the first paragraph makes very little sense: ""This approach, however, is greatly hindered by the fact that traditional semantic parsing mostly involves rule-based features and symbolic manipulation, leaving only a handful of tunable parameters to cater to the great flexibility of natural language.""  One might argue that traditional semantic parsing suffers from combinatorial explosion.  There are many parameters in traditional semantic parsers, not just ""a handful"".",1,5200
"Quality: This paper is interesting, but novelty is a bit lacking and many of the experiments and claims are vague.  Clarity: Paper is unclear to read, particularly the results which are all quoted on different test sets. In addition, the comparison to Human performance seems really biased.  Originality: This work is very similar to H. Sak's CTC papers but now predicts characters. Many of the speedup ideas have also been tried in the literature as well. To me, this is more of an engineering paper that puts together different components already explored in the literature individually. In addition, there are some References are missing which i've noted below.  Significance of Work: Interesting approach for CTC with LVCSR task, putting together many different research ideas into one unified paper  Pros: * Interesting approach for CTC with LVCSR task, putting together many different research ideas into one unified paper Cons:  * A lot of references in the paper are missing    a) for speeding up training with GPUs - cite Frank Seide's Interspeech 2014 paper, Amazon has a paper at Interspeech 2015    b) Hasim Sak's ICASSP 2015 paper should be cited on the first page when you say end-to-end since your work is very similar to this    c) H. Sak's Interspeech 2015 paper also does data augmentation and should be cited * Many vague points in the paper:    a) Notion of end-to-end is unclear? Why is your method end-to-end if you are still using a separate acoustic and language model? To me this paper is just CTC predicting characters    b) Given intuition as to why sequence-wise norm is better than regular batch norm    c) The tables in the paper cannot be compared because they are all on different train/test sets, making things really confusing. Why are the numbers in Table 1    and 2 on different training sets    d) Why was it difficult to introduce a delay in emitting the label your system (like Sak 2015)    e) Your Human performance is based on two workers transcribing, this seems extremely biasedThis paper reviews the authors' experience in building large-scale character-based CTC RNN speech recognition systems for English and Mandarin, combining several existing optimization and robustness tricks and a couple of new variants.  Some practitioners are likely to find this paper useful as an addition to the growing understanding of what works and what doesn't.  On the other hand, the paper is lacking in comparisons and is unclear at times, and some of the results seem questionable.  Quality: It is probably technically correct but I have some reservations -- see below.  Clarity: Could be much clearer.  For example: - What is really meant by end-to-end? - The input acoustic features are not totally clear.  Is 20ms the size of the analysis window, the frame skip, or both?  By ""spectrogram"" do you really mean ""spectrum""?  If ""spectrogram"", then more info is needed on the spectrogram parameters. - Fully define batch normalization.  What is the function B(.)?  Significance: Some practitioners are likely to find this paper useful as an addition to the growing understanding of what works and what doesn't.  However, since there is little analysis, this limits the usefulness since it is not clear why certain techniques worked better for the authors than for others in prior work, and vice versa.  Pros: - It is useful to see how a variety of common techniques are affected by variables such as data set size and noisy vs. clean test data, on a larger scale than is typical in ASR papers. - The speed section provides a useful data point as more groups try to scale up their systems.  Cons: - There are multiple result tables on different data sets that are not comparable to each other.  It would be much more helpful to keep the data sets the same across tables, and to include more of the standard benchmarks, in particular the commonly used Switchboard. - The paper needs a clearer presentation of what exactly is novel vs. not (sequence normalization?  SortaGrad?), and which conclusions are similar to vs. different from what's been found before (and, when different, why). - The human WERs are surprisingly high; e.g. prior work has reported human WERs of around 1% for WSJ (see Lippmann, Speech Communication 1997).  How many total turkers were used?  How was their quality ensured?  How was the label error measured? - The paper describes what worked and what didn't, but the usefulness of the results is limited without a bit more analysis as to why.  For example, the authors report that they did not have success with delaying the output as done in prior work, and it is not clear why. - Citations are missing at times.  For example Sec. 3.3 should cite prior work on 2D convolution for speech (e.g. Abdel-hamid et al. Interspeech 2013, Toth ICASSP 2014). Significance A useful paper on building end-to-end large scale speech recognition system for English and Mandarin. Describes many techniques and ideas in one place.  Clarity The paper assumes significant prior background knowledge on building end-to-end speech recognition system especially with RNNs, CTC training etc.  Novelty There exists a very similar paper on arXiv.org - http://arxiv.org/abs/1512.02595 by the same set of authors. This prior art is not cited but describes almost identical techniques and experiments used in this paper. What is confusing to a reader who has read the prior work are the discrepancies in experimental results - sometimes the new results are better while sometimes they are worse. It is not clear what new techniques are being introduced in this paper and why the results differ.  Pros Useful paper describing several techniques and ideas for end-to-end speech recognition.  Cons Lacks novelty with respect to earlier work of the authors and is missing a lot of citations. Assumes a lot prior background.",1,5201
"The paper introduces a synchronous parallel stochastic gradient descent algorithm and compares its performance on different tasks with a reference asynchronous SGD. The behavior of the two algorithms are compared for different configurations (number of parallel machines - tasks).  The paper addresses an important problem: defining efficient distributed algorithms for large scale deep architectures.  Its contribution is to provide experimental results on different problem types and these results will certainly be interesting for a large community. The comparison of synchronous – asynchronous optimization methods is also a topic of interest by itself and this paper contributes in this direction.  It is not clear in the paper if the parameter servers in the comparison have the same configuration. The algorithm setting (gradient step policy, etc) could certainly be adapted to a specific implementation (synchronous or asynchronous). It seems that the same parameter adaptation method is used for both the synchronous and asynchronous methods. Does it provide a fair comparison? These parameters might influence the performance as much as the 2 different implementations do. How robust is the comparison wrt this aspect? E.g. could the performance order be reversed with another version of SGD?  There is no indication on the distribution of work to the backup workers.  How do you distribute data to these specific workers and how different is it from the other workers?   There is no discussion on mixed synchronous - asynchronous implementations. This could generalize the idea of backup workers. Could this be an interesting option and why?  The passage on the overlap of the gradient computation for the different layers by the end of section is not clear for me and could be made more precise.   Pro: comparison of two different options (synchronous vs asynchronous) for parallelizing SGD. Experiments on different configurations.  Cons: not clear (for me) if the SGD algorithm parameters setting itself is not as important as the option choice (sync. vs async) for the performance.  - Assuming that all nodes run at about the same speeds, using the Sync-SGD instead of the Async-SGD seems a reasonable thing that one could try. And it is interesting to see actually that Sync-SGD performs better than Async-SGD.  - I'm wondering if the authors have not used the gradient clipping in the RNN experiments as well. If the provided results are with clipping, I'm also wondering how the method would perform without the clipping in RNN.  - In the DRAW experiments, the authors only mention the convergence speed, but not provide the accuracies.  - The Async-SGD seems to be a simple one in the class of Async-SGD methods. It would be interesting to see comparisons to more advanced Async-SGD methods as well.  - It would have been interesting to see how the Sync-SGD is affected by increasing the response time of the slowest worker.The paper discusses a very important problem which deserves more attention from the academic community. Large-scale deep learning is important for industrial applications. The paper proposes a slight modification to classical sync-SGD where instead of waiting on all threads, the central controller only waits for a majority fraction of it, and proceeds as if it has received information from all threads. This is motivated from the observation that in multi-worker systems, typically, there are only a few outlying slow workers which negatively impact the overall system's speed (Pareto principle). Ignoring these workers should thus give a huge speedup.  As with any systems machine learning paper, details about the actual system used would have been very useful, i.e. more information about the hardware used.  The results in the paper are impressive, which makes this paper well worth talking about, and thus deserves to be in the workshop proceedings.",1,5202
"It's interesting to see that embedding additional annotations such as part-of-speech and NER tags helps the performance. However, the evaluation in the paper makes it impossible to compare to previous work. They use the same training data but use a different test set. Why not use the same test set?  Also, the standard metric for summarization is Rouge recall  (see the DUC challenge) but the authors chose F1. They also quote previous work but Rush et al. used recall - so it does not sound right to put both results in the same table.Overall, this paper considers a fairly straightforward application of the seq2seq model to abstractive sentence summarization, with most novel work only hinted at or mostly ignored (almost surely due to the short page limit, but it would be very interesting to pursue in a longer paper). The empirical results are carefully described, and based on the authors' response, seem to be comparable to those of Rush et al., and overall therefore represent a significant boost in accuracy over the Rush model.    Some finer points:  Using the Large Vocabulary ""Trick"" speeds up training but hurts the abstractive ability of the model. Since the latter is the core focus of this model, it seems worth it to fix this issue. A sampling approach to the full softmax should represent a better solution than the LVT heuristic or the heuristic of extending the target vocabulary with 1-nearest neighbours.  In the words-lvt2k-(2|5)sent model, it is not clear why using 2 sentences is more accurate than 1, but using 5 sentences is less accurate than 2 (do you reverse the input in the encoder?). It would be beneficial to investigate the reason for this by visualizing and analyzing the attention heat maps for the 2 vs 5 sentence models.  It was nice to see that the authors did also consider a more novel hierarchical model based on Li et al. 2015, but it is unfortunate that this approach did not seem to yield better results. Could it be that this approach could actually benefit from using more than two sentences? It's not clear whether this was tried.  It would be useful to see the ""src-copy rate"" of the gold training data to be able to meaningfully interpret that metric. ",1,5204
"Authors present novel and interesting results on empirical Rademacher complexity (ERC) of deep neural networks, building on recent work of Neyshabur et al.  Specifically they present a new bound on ERC for deep convolutional networks and demonstrate that their bound is tighter than that of Neyshabur et al.  They also provide a deep representation based bound on ERC which is then used to motivate a novel regularization approach that aims to orthogonalize representation for distinct data samples.  Few aspects in which I feel the paper could be improved: i) Authors seem to be claiming that models with lower ERC are better.  This is how use of smaller kernels in CNNs in justified.  However, I think a key missing component of the discussion is absolute error (and not just generalization error) for models in a given model family, this has to be taken into account.  This would then perhaps explain the apparent discrepancy of why ERC increases as network depth increases and yet those models empirically do quite well. ii) The novel regularizer based on coherence (orthogonality) of representation is nice but a little bit counter intuitive for elements that belong to the same class (for a classification network such as ones used in MNIST).  Would it make sense to consider the class membership of data points?  Any insights into this would strengthen the paper I believe.  Overall I think this is a good paper, fairly clearly written and easy to follow.  Should be published and discussed.This paper studies the capacity control for Convolutional Neural Networks with bounded Frobinius norm using Rademacher Complexities.  Pros: 1- Studying the capacity of CNNs is interesting and insightful. The discussions about the effect of filter size and other structural hyper-parameters on the capacity of these networks is very interesting and helpful.  2- A regularizer that penalizes the correlation on the representation layer and the connections to Rademacher complexities are both interesting.  Cons (and suggestions):  1- The paper is 4 page long, almost 2 pages of which are abstract, introduction, background and previous works (up to equation 5) and the last page is dedicated to references so the main part of the paper is only about one page and I think authors could have clarified the discussions and experiments much better.  2- Two theorems are presented but without any proof or even a sketch of the proof.  3- While the section 3 is suggesting a regularization term based on the Rademacher complexity, the experiments are not related in anyway to the discussions on section 2 and there is a bit of disconnection here.  4- The suggested regularizer in interesting to study but calculating the gradient for this regularizer seems very time consuming. A short discussion on the computational complexity of using this regularizer could be helpful here.  5- The discussion on the ResNet ignores the fact that ResNet uses skip-layers which is not included in this analysis. ",0,5205
"The authors presented a GPU kernel trick to speed up computations for RNNs, key points: 1] w/ minibatch size of 4, they claim 30x speedup compared to standard GEMM kernels. This is very impressive, such a simple engineering trick can yield 30x speedup!  2] One of the arguments to use smaller minibatch sizes is due to memory (i.e., only 12G/24G in modern GPUs). For example, the authors demonstrated very deep RNNs in their paper. The memory argument isn't very strong since you can always cache the activations (quite easily, cheaply and transparently) to CPU memory using DMA. Additionally, a lot of groups also run CPU models for RNNs which have can easily have >128G of RAM per CPU (CPUs are not that much slower than GPUs in the RNN GEMM type ops esp w/ AVX512 + FMA).  3] If the memory argument doesn't hold, the other reason to use a smaller minibatch is if it gives better convergence properties (whether theoretical or empirical). In theory, smaller minibatches should give better convergence, but in practice that may not necessarily be true due to the high variance of the gradients in RNNs. The authors didn't show whether the smaller minibatches did indeed give better empirical performance, if large minibatches are required to get state-of-the-art performance, then this would invalidate a lot of motivations to use smaller minibatches.  Comment on the very deep residual RNNs: The authors also claimed that residual RNNs (i.e., skip connections) make possible optimizations of deep RNNs (i.e., see Table 1); compared w/o the skip connections, the deep (48 layers) of RNNs are difficult to converge. However, the authors did not compare their models to GRUs or LSTMs which are generally regarded as much better than RNNs, and stacking such deep RNNs may not be necessary. For example, many acoustic model and end-to-end speech models use only 3-4 layers for RNNs because deeper networks don't seem to help (see Sak et. al 2014; Bahdanau et. al 2015; and Chan et. al, 2015 for their acoustic models / end-to-end attention speech papers). The authors also didn't show that the deep RNNs out perform a ""shallow RNN"" (i.e., assuming the same dev/train sets are used, the Deep Speech 2 paper showed much lower WERs, suggesting that the deep residual RNNs are not necessary). However, part of their motivation is that w/ this kernel trick, we can explore much deeper RNNs.  Side impl details: The authors left out much detail on the implementation of their kernel. This would make it hard for open source groups to implement and/or replicate their work. Would be really nice/cool if the authors were willing to open source their kernel to Theano / Torch / TensorFlow etc... This is a good paper, adequate for the ICLR workshop track. Some comments:  -Decreasing the batch size whilst maintaining a decent throughput in terms of examples / s is a crucial component of minibatch-style optimization methods. Thus, this work is important and should motivate the community towards that direction. -Do speed ups hold when decreasing the number of RNNs? Or decreasing their size? -In the data parallelism experiments with multiple GPUs, do the authors use synchronous SGD? If so, the effective batch size increases. Do they observe a degradation of training speed / epoch of data seen? -Are the authors going to release the source code? Given the amount of code sharing and deep learning platforms, it seems like the major contribution to the community would be to do so. -Please define SMImportant work in dealing with bottlenecks in training of the recurrent layers of an RNN, which allows for high GPU usage down to small mini-batch sizes of 4. I'm not familiar with other hand written assembler software pipelines and optimized implementations of a global barrier, making it difficult to gauge baseline comparisons to Nervana and CUBLAS (being the one small con), but the gains are impressive and should be reported. 30X on recurrent layers, 10x at system level and 45% of peak theoretical throughput for NVIDIA TitanX GPU.  One question, the implementation is focused on the recurrent computation of a vanilla RNN, but more and more we see GRUs or LSTMs in end to end system, which have more complicated recurrent dependencies. For something like a GRU where reset gates computed from one recurrent computation is applied as a pointwise multiplication on another recurrent computation, do you forsee any problems in your current pipeline strategy and optimized global barrier?  On page 3 you mention performance is much better at layer sizes 2048 or 2560, suggesting advantages of persistent implement as model become deeper and thinner (do you mean wider? as the comparison is to 1152?). ",0,5206
"This paper presents an interesting application of embedding learning algorithms to the Healthcare domain. It introduces a model that can jointly learn embeddings of two healthcare concepts: medical codes and patient visits. The set of all medical codes is analogous to words or the vocabulary of a language, and a patient visit is defined as a subset of all medical codes, i.e. a bag-of-words.  The paper is clear and well written. While the model used in this paper is not original, it is a good start for learning representations in the healthcare domain.  Major issues: 1- I find the model used for learning visit representations a bit problematic. The prediction given a specific visit representation v_t is provided with several (in fact 2*w) different (bag-of-word) targets simultaneously, each for a different neighbouring visit. I believe this would cause some kind of ""unlearning"", where the model is asked to predict different things for the same input.  A better model could be to use a different softmax for each neighbouring visit target, or aggregate those targets (e.g. average or AND) and ask the model to provide one single prediction per input.  2- There is some important information missing. For example, I cannot find a definition of the demographic information d_t. In fact, using this information should be defined as input to the function f_V.  3- Figure 2 has 4 different sub-plots, all with the same x-axis but with different ranges. It is not clear what the authors are trying to show here, and why not combining all of them in one plot.  4- Claims about better interpretability of the representations are not supported by experimental result. This paper presents a simple two-layer neural network for modeling bag-of-word representations with temporal dynamics, applicable to sequences of related documents. The end-to-end learning algorithm consists in optimising the sum of two losses, the cross-entropy for co-occurrences of words in one document, and the cross-entropy for predicting words in a document appearing at a different time.  Instead of words, the authors use medical codes (with a vocabulary of about 29k codes) in consecutive patient records, and demonstrate the applicability of their model to predicting medical codes records, using a set of 3.3M visits for 550k patients, and demonstrate that their model outperforms non-temporal models such as stacked-autoencoders, skip-grams or Glove vectors (as well as a plain sum of one-hot representations). When predicting the codes of the next visit, they achieve a recall of about 0.76 to 0.77 at 30 codes.  The paper is well written but some areas remain unclear.  1) Why does loss function (1) and figure 1 suggest that codes from visit V_t are used interchangeably to predict codes from visists V_{t-w}, V_{t-w+1}, ..., V_{t-1}, V_{t+1}, ..., V_{t+w}? The application is to predict V_{t+1} from V_t.  2) What are the demographic data used by the authors (vector d_t)?  3) If I am not mistaken, the fact the one-hot vectors perform as well as the other methods seems to suggest that there is a strong stationarity in the consecutive visits, on which the authors should elaborate. Similarly, the performance of skip-grams is surprisingly bad compared to the other methods.     ",0,5207
"The authors propose a deep Gaussian process (GP) model with a *linear* covariance function (eq. 7) and non-linear transformations between the GPs. The use of a linear covariance function connects the model to Bayesian neural networks (single layer with no non-linearities). The use of a non-linear transformation between the GPs corresponds to the non-linearity between a Bayesian neural network's layers. The GP output dimensions are correlated (through the use of matrix Gaussian distributions), and a sparse inducing point approximation is used to make the approximation efficient (following [Damianou and Lawrence]). The work offers an approach to connect Bayesian neural networks to deep GPs (with a certain imposed structure), following the line of work of [Gal and Ghahramani].  The paper is clear (to someone coming from the GP community at least), but seems to be mostly a composition of several existing works. The ideas discussed are interesting, and the assessment is sufficient for a workshop submission, but I would suggest to extend it for a conference submission (see below).   Some comments for the authors: * For a conference submission, I would suggest to compare the model intrinsically with several changes, to see where the improvement comes from: - with no output dependence (V=I) - with no non-linearity between the layers (sigma=I and eg RBF covariance function instead of a linear one - the usual method with deep GPs) - by optimising over \tilde{A} and \tilde{B} instead of putting a variational distribution over these (the usual approach in sparse GPs is to optimise the locations of the inducing points \tilde{A} and optimise / solve analytically for \tilde{B}) * Extrinsically, I would suggest to compare to the results of [Bui et al., see below]. Bui et al. collected results from many sources and evaluated various methods for inference in Bayesian neural networks and GPs. * The model is not non-parametric. First, a linear covariance function is of finite-rank. Second, even without a finite-rank covariance function, the sparse input approximation makes this into a parametric approximation. You might want to look into the work of [Titsias] in this regard. Lastly, you lose the marginalisation property with multiple layers.  Minor comments: * You actually use Multiplicative Gaussian Noise rather than ""dropout posteriors"" * What number of inducing points was used in the experiments? * You might want to cite [Gal and Turner, see below] which also approximate the Gaussian process by placing a Gaussian posterior distribution over the weight matrices. * The words ""let's"", ""GP's"", ""its'"" are misspelt multiple times * H is not defined in eq. preceding eq. 5 * The sentence ""This corresponds to samples from the marginal..."" is very long and difficult to parse.  Pros: * The experiments are of good quality for a workshop paper * The idea of using inducing points with neural networks is intriguing (I've been working on it myself) * The paper is interesting Cons: * The assessment compares apples and oranges (the deep GP approximation is quite unrelated to the VI and dropout models compared). * Time complexity of the model is O(K**3 + M**3) with K hidden units and M inducing points. * the structure is simplified with diagonalisation assumptions * The model is not non-parametric  References: Bui et al., ""Deep Gaussian Processes for Regression using Approximate Expectation Propagation"" http://arxiv.org/abs/1602.04133  Gal and Turner, ""Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs"" http://jmlr.org/proceedings/papers/v37/galb15.htmlThis is an interesting paper, which includes two key themes: firstly, how to improve the definition and inference in Bayesian Neural Networks. Secondly, how the employed modeling/inference choices make the proposed approach a specific case of a deep Gaussian process. The key idea employed to achieve the above, is to use the matrix variate Gaussian as a distribution over the whole weight matrix together with the variational reparametrization trick.   SUMMARY OF ASSESSMENT:  Overall I enjoyed reading this paper. I have some concerns regarding the clarity and some claimed properties of the approach (see below), but overall it is conveying interesting ideas and the contained amount of novel work -including experiments- is larger than most workshop submissions. Therefore, the paper is well suited for the workshop.    DETAILED ASSESSMENT:   The research area of Bayesian neural networks and deep Gaussian processes are currently attracting a lot of interest, which adds to the significance of this paper. The ideas behind the proposed framework are close to those of Gal & Ghahramani 2015. For this reason I expected a more thorough discussion on the qualitative difference of the approaches.   The use of the matrix variate Gaussian in the context of Bayesian NNs is a novel element in this paper, and results in the interesting property of correlations in the hidden units. The arguments in favour of its use are convincing, and the relation to deep GPs follows. However, I am not convinced that there is enough support to claim that the proposed model is non-parametric. It is true that the marginalization property of the Gaussian (and matrix variate) gives rise to a GP-like equation, but the proposed model does not inherently seem to be a ""process"", from the functional support point of view (referring to the mapping between the layers).  The inclusion of pseudo-data is novel and interesting in the context of NNs. However, it'd be nice if the authors could expand on their role. Is their role purely computational, i.e. in terms of low-rank approximation to the covariance? Furthermore, in the deep GP [Damianou and Lawrence] the pseudo-data are variational parameters and only affect the approximation and not the model, therefore allowing the approach to remain non-parametric. It is not clear to me that this is the case in this paper: the pseudo-data do seem to change the model, as there is no underlying process (like the Gaussian process).  Also, can you please expand on the number of the pseudo data typically used and how their inclusion affects optimization/complexity in practice? How does that compare to the mean-field approach?  One other place where clarification would be good is the dropout posteriors (below eq. 7), which are only mentioned but not explained.  The experiments are performed in comparison with related methods in the literature and are convincing. I also recommend to the authors to have a look at a recent paper by Bui et al., ""Deep Gaussian Processes for Regression using Approximate Expectation Propagation"".   Typos: - ""presented in Gal & ..."" -> ""presented in (Gal & ...)"". Same typo in other citations too. - Some typos with apostrophes: lets', GPs', its', ... - Some refs need capitalization: bayesian -> Bayesian, gaussian -> Gaussian ... ",1,5208
"This paper studies the connection between different types of regularization for neural network training. The authors found that these regularizations can be expressed as different functionals of the singular values of the Jacobian matrix. The result is interesting and could be presented as a workshop paper. There are a few minor remarks to be taken into account.  1.	In the paragraph after Eq. (1), the notation epsilon is mentioned before its actual usage in the first paragraph on the 2nd page. It could be defined after that. 2.	It is not clear why VAT is maximizing the maximum eigenvalue of J^TJ (minimizing the negative value of the maximum eigenvalue of J^TJ ) while the others are performing the minimization. Clarification on this would be helpful. This paper proposes a regularization based on the distance of predictions for similar inputs. This kind of regularization has been widely used in the 2000s for graph-based semi-supervised learning but may also be applied to fully supervised learning.  The connection with the penalty on the Jacobian is known but the implications of that regularization when using adversarial examples is interesting and new to me.  My main concern is about regularizations which explicitly enforce constraints on the output of the function. There are cases where we want the output of a function to change quickly as a function of the input and I believe enforcing the same smoothness across the entire space will likely be too strong or too weak.  Due to this concern, I'm slightly leaning towards rejection but, as this is pure personal preference, I am fine with the paper being accepted.The regularizer examined in this paper was previously presented in a more general form in ""Learning with Pseudo-Ensembles"" by Bachman et al. (NIPS 2014). See Eq. 2 on page 3 of: http://papers.nips.cc/paper/5487-learning-with-pseudo-ensembles.pdf. Bachman et al. consider controlling variation in any observable property of a model's behaviour w.r.t. some distribution over perturbations of the model and/or its inputs.  In my experience, penalizing first-order variation in euclidean space is often only marginally useful. It suffers from a strong bias towards shrinking all network outputs towards a constant value. It may be better to penalize stochastic approximations of higher-order derivatives.  In the semi-supervised classification setting, penalizing variation in distribution space using KL divergence works well. The KL divergence becomes robust to relatively large changes in the raw (i.e. pre softmax) output once the predictions become confident, so it suffers less from the tendency to shrink all outputs towards a constant value. Penalizing worst-case KL divergence over a ball of fixed radius was tried in the VAT paper, and penalizing expected-case KL divergence was tried in the paper by Bachman et al. Penalizing worst-case behaviour seems to be a more effective regularizer, but comes with a higher computational cost.  As you develop this work further, you should consider looking at the literature on ""stochastic progamming"" (https://en.wikipedia.org/wiki/Stochastic_programming) and ""robust optimization"" (https://en.wikipedia.org/wiki/Robust_optimization). Stochastic programming is about solving an optimization problem in expectation w.r.t. some distribution over misspecification of the problem, and robust optimization is about solving an optimization problem in the worst case w.r.t. some bounds over misspecification of the problem. A lot of mathematically rigorous work has been done in these fields, which may help with your formal analyses.  ****************** Review summary ****************** 1. The regularizer described in this paper is not particularly novel.  2. Developing a stronger formal understanding of connections between different practical instantiations of the underlying concept (i.e. robustness to model/input perturbation) is worthwhile. This paper provides a decent step in that direction.",0,5209
"This paper addresses the task of generating natural language answers to simple factoid questions. It presents an end-to-end neural network model that can search and represent related factors to a natural language question in a knowledge base, and decode these factors using a recurrent neural network with the attention mechanism.  This paper is very clear and easy to understand. The task of generating natural language answers is new as far as I know. This paper addressed this task using an end-to-end neural network with trainable question interpreter, knowledge base enquirer, and answer decoder. The question interpreter and the knowledge base enquirer can be found in previous work. But it is somewhat novel to propose the answer decoder that can generate both KB words (i.e. words represent factors retrieved from a knowledge base) and common words (i.e. words used to connect KB words to form a natural language answer). The paper also validated the effectiveness of training all the three components in an end-to-end way.  I am curious about the choice of K_Q for the knowledge base enquirer. How long does it take to train a model? It seems that it might be slow especially when K_Q is large. In addition, it would be great if the authors can extend the model to multiple factoid questions.This paper introduces an end-to-end neural network model, in which natural language answers to simple factoid questions can be generated. The model consists of three sub-models, namely Interpreter, Enquirer and Answerer, along with an external knowledge base. The experimental results show that the proposed framework can achieve slightly above 50% of accuracy on a specific dataset, when generating answers based on the facts in the knowledge base.  The paper was written clearly in most of the places. I have a few minor comments and questions here.  1. Fig 1: it'll be better to add Q, H_Q, r_Q in the diagram to make it more clear and consistent with the text description. 2. Last sentence on Page 3: ""correct patterns"" means ""correct answers"" or just correct pattern/type but not necessarily correct at the lexical level? 3. Will the dataset collected and processed in the work be shared with the research community?  This paper proposes to do QA but also to learn to generate answers on top of retrieving the correct answer. The paper is strong both in the retrieval part than in the generation. The fact that the model can switch from knowledge to language words is rather neat. I think this will make a nice paper for the ICLR workshop session.  Few questions: - Will the dataset be released? - Since not all models are generating answers, how is the accuracy computed in Section 3? By comparing the objects of the candidate triple? - For the generation of answer, is the z variable controlling the switch between language and KB words learned directly end-to-end or is there a pre-training or an intermediate supervision of any sort?",1,5210
"The authors propose an approach for the on-line maximization of the variational lower bound. The new method is based on iterating over the data and solving individual optimization problems between the current posterior approximation and the product of that posterior approximation and the likelihood function for the current data point. The advantages of the proposed approach with respect to other variational techniques is that it does not require to use learning rates or compute complicated expectations with respect to the variational approximation.  Quality:  The proposed approach is not validated in any form of experiments. It is not clear how well it is going to work since variational Bayes is known to under-estimate variance and its application in an on-line manner could make more significant this problem because of consecutive under-estimation of variances at each iteration. Another problem is that there is no guarantee that the proposed approach is going to converge to any local minimizer of the original variational bound. In fact, by looking at equation 5, the update for the variance produces increasingly small variances. This means that the proposed approach would converge to a point mass at the mean of the posterior approximation q.  The mixture of Gaussians in Section 3 does not seem to be the correct approach. The correct approach would be to compute the product of all these Gaussians to obtain a final Gaussian approximation (accounting for the prior being repeated multiple times). The correct approach is given in  Expectation propagation as a way of life Andrew Gelman, Aki Vehtari, Pasi Jylänki, Christian Robert, Nicolas Chopin, John P. Cunningham http://arxiv.org/abs/1412.4869  Clarity:  The work needs to be improved for clarity. It is not clear how equation 4 is obtained. The equation above equation 4 seems to come from performing a Laplace approximation. The authors should clarify this possible connexion with the Laplace approximation.  Originality:  The approach proposed seems to be original up to my knowledge.  Significance:  It is not clear how significant the proposed method is since one can use stochastic optimization to optimize the variational lower bound. The approach for training neural networks fast by splitting the data seems to be wrong.Manuscript describes variational Bayesian (VB) treatment of weights in neural networks and online learning for them.  Similar ideas have been studied recently, for instance in  http://jmlr.org/proceedings/papers/v37/blundell15.pdf  but relationship to existing work is not presented clearly. Instead, using VB for network weights is presented as something novel.  There is no clear theoretical contribution or any experiments.  There is one crucial error as well: Bottom of page 3 writes that ""...distribution of the whole ensemble is a mix of..."" whereas the equation on the top of page 3 is a product rather than a mixture.  This paper might be of interest to the author, covering similar ideas: https://www.hiit.fi/u/ahonkela/papers/ica2003.pdfThis paper proposes a method for online updates of a variational approximation of the posterior over neural network weights. No experimental evaluation is provided. The presentation is intelligible, but far from clear.  The idea of using a recursive variational Bayes approximation for streaming data was proposed in Broderick et al.'s SDA-Bayes paper (http://papers.nips.cc/paper/4980-streaming-variational-bayes). But as another reviewer noted, online variational inference has been around since at least Sato's 2001 paper on online model selection with variational Bayes, and in a sense since the 1998 Neal and Hinton paper on incremental EM.  There have been plenty of papers about variational inference for neural networks, for example, Graves's Practical Inference for Neural Networks (2011) or Hinton's original 1993 variational inference/MDL paper (http://dl.acm.org/citation.cfm?id=168306).  The idea of using the variational distribution's variance to control step size is interesting. It's sort of related to recent papers that use trust regions/prox algorithms to optimize variational approximations (Theis&Hoffman, 2015; Khan et al., 2015).  However, that doesn't mean it will work. With no experimental validation, it's impossible to say whether this is anything more than a cute idea.",0,5211
"The paper is about introducing a notion of (soft) source-side coverage into neural MT models. The idea makes sense and is shown to produce reasonable gains in BLEU.   This version of the paper is extremely difficult to understand. I had to google for the uncompressed arxiv version to get any kind of confidence that I understood the paper. I would recommend that people read the original paper -- it is quite interesting.  I think the authors should have tried to actually write an extended abstract that conveys the key points, rather than trying to fit the entire formal description of their approach into the 3-page format.   Below are just a few of the notational issues in this version:  $auxs$ --> maybe $\psi$?  \alpha_{i,j} in Eq. (1) is not defined.  \phi(h_j) is used in the equation, but then \phi_i(h_j) is defined immediately thereafter.  Which should it be?  What is the ""decoding state"" s_i? This is not defined in the paper.  The equation in Section 2.1 uses \alpha_{i,j} but below that the authors write ""Here we only employ \alpha_{i-1}..."" -- this seems to be a mismatch. Or if it's not a mismatch, I don't understand what it means.   There is also no description of the experimental setup -- only some tables and plots are shown.  I think the work is interesting and compelling but I am hesitant to recommend acceptance of this paper as an ICLR workshop paper.  I would prefer that the authors submit this as a conference paper to another venue, like ACL, CoNLL, EMNLP, or COLING.  This paper would be a good fit for one of these NLP conferences.  This paper extends attention-based neural machine translation (NNMT) with a coverage model.  In the standard attention model, for each target word a subset of relevant source words are ""selected"" as context vector.  In principle, it can happen that source words are used several times or not all.  Therefore, the introduction of the notion of source word coverage in an NNMT attention model is an interesting idea (a coverage model is used in standard PB SMT).  I don't agree with the statement that learning the coverage information by back-prop is potentially weak and one should add ""linguistic information"". In that case, one could question the whole idea to do NNMT - in such a model every ""decision"" is purely statistical without any linguistic information.  The description of the used coverage model itself is very complicated to understand. Given the space constraints, it seems a bad idea to first present a general model (Eqn 1) and than to use a much simpler one, which is insufficiently explained. I wasn't able to understand how the coverage model was calculated, how the fertility probabilities were obtained (Eqn 2+3), etc.  Finally, the results are not analyzed - the authors just provide two figures and a table.  There is no information on what data the system was trained on nor the actual language pair !!  Also, I'm surprised that the BLEU score of the NNMT system decreases substantially with the length of the sentences (Figure 1 left).  This is in contrast to results by Bahdanau et al. who show that the attention model does prevent this decrease (plot RNN search-50 in Figure 2 in their paper) !  This raises some doubts on the experimental results ...  why the attention coverage vector beta is uniformly initialized ? I expect it to be zero (nothing covered)  - you use notation without defining it, e.g.     - what is d in "".. is a vector (d>1) ..""     - s_i and h_j ; a small figure would be very helpful !  Several sentences are difficult to understand and I spotted a couple of stupid errors (e.g. ""predefined constantto denoting""). Please proof-read the paper !",1,5212
"The authors propose an approach for learning the structure of Sum-Product Networks that extends the original LearnSPN algorithm to cases involving missing data and mixed discrete/continuous feature sets. The original LearnSPN works by recursively co-clustering variables and instances into approximately independent sets and similar instances.   The proposed algorithm MiniSPN differs from LearnSPN in a few ways. First by using a 2-way Chi squared test of independence, applied only to rows where the variable pairs are not missing, as their ""independence oracle"" rather than the G-test. Continuous data is also binned lazily for each batch of instances by binarizing around the median. The main contribution seems to come from simplifying the mixture model step into a greedier (presumably this is where the computational speedup comes from) approach that discards the cluster penalty prior and uses hard-EM.  They present two sets of experiments. The first is on data from the Knowledge Graph, and shows their algorithm to achieve better fit and runtime than the alternatives. I am confused by the claim that they could not test the original LearnSPN on this data because of missing data problems, as their fix for missing data seems easily applicable to LearnSPN as well. Experiments on the standard datasets show their method to be competitive with the original LearnSPN but much faster.  I have some concerns about the work. The contributions of the work and their importance are not sufficiently analyzed / discussed. The rationale for replacing the the G-test with Chi-squared is not discussed, and likelihood ratio testing (G-test) should generally be optimal. The extension to handle missing data is very simple and applicable to any of the competing algorithms. The contribution of the work seems to be the large speed increase from the simpler clustering sub-routine and the lazy binning of continuous values.  This paper proposes a novel algorithm for SPN, which is both faster and more general. The original SPN algorithm is a EM procedure starting from a fully connected graph and where edges are removed if the weights are zero (see Algorithm 1 in Poon and Domingos, 2011).  According to the authors, the original algorithm (called LearnSPN) requires a careful hyper-parameters search in order to work. Their algorithm, MiniSPN, is avoiding this problem and is more general.  ""In the variable partition step"" (which I think is the E-step), their algorithm do ""the two-way Chi-square test of independence"" over of non-missing variables. Overall along with other tricks, this is suppose to make the E-step more greedy (and thus faster and maybe more robust?)/  They compare their approach to learnSPN on 2 datasets from the Knowledge Graph People collection. On the first task, because of the missing data they don not compare to learnSPN, but as mention by the other reviewer, they could have use the same trick on LearnSPN in order to make it work on this task. On the second task, the performance of MiniSPN and LearnSPN are basically the same, but MiniSPN has an impressive speed up of 500x.  Overall, despite some promising results (very fast with same performance), the lack of analysis of their algorithm and the impact of their contribution make the paper a bit weak. Considering this paper is about an algorithm I m surprised that they never explicitly write it down nor make a light analysis of its convergence rate, guarantees and so on. Most modifications to the original algorithm are not motivated. It is hard to understand their design choices.  This paper presents some tweaks to an existing sum product network (SPN) structure learning algorithm, LearnSPN. The original algorithm works by recursively partitioning data either according to variables or instances. If partitioning by instances, instances are clustered and aggregated by a sum node (mixture assumption). If partitioning by variables, the aim is to find independent subsets of variables on the given set of instances, and then to aggregate with a product node (independence assumption).  The contribution of the paper is to make some minor tweaks to the LearnSPN algorithm: 1. Discretize continuous variables using a binary threshold on the median value when performing independence tests. 2. Clustering is done more crudely, with a simple guess-and-check approach, where a clustering is proposed, then accepted or rejected based on a validation set. 3. To handle missing data in the variable partitioning step that tests pairwise independence, only consider rows that have both variables present.  Each of these contributions is very minor, and they seem like the first thing to try rather than the result of a long thought-out consideration of the design space. The main source of interest is that the method is orders of magnitude faster than LearnSPN and performs better than an alternative “Pareto” algorithm that is 1-5x slower.  Overall, the strengths are that the method seems reasonable and practical. The weaknesses are that the originality is low, and there is not much in the way of technical contributions.",1,5213
"The paper proposes a simplified version of an attention mechanism. The proposed mechanism summarizes the input sequence into a context by taking a weighted sum of its elements, where the weights are computed by a feed-forward network. The resulting context is further used by the same feedforward network.  I do not think it is correct to say that the proposed approach actually handles long-term dependencies. It does solve the considered toy tasks, but it performs almost as well as a simple baseline that just sums all the inputs. As mentioned in the paper, the information about positions of the input elements relative to each other is completely discarded by the proposed method. Thus, I do not think that it is fair to compare it with RNN, as the latter are much more generic.  While the approach may be suitable for some applications, I think that the presenting it as a method to handle long-term dependencies is wrong.Capturing long-term dependencies is difficult when a problem requires temporal positions of input symbols to be exploited. From its definition, the proposed model assumes temporal independence among the input symbols (Eq. (1) and h_t = f(x_t)), and it's not supposed to capture well, if not at all, any interesting long-term dependencies in the input sequence.   This is not to say that the proposed approach will not be useful at all. Rather, it should be framed differently in a very different context. In this regard, the evaluation in this paper is quite meaningless, as both of the tasks (addition and multiplication) are commutative and in fact easier to model if a network is a priori oblivious to any temporal ordering of the input sequence.   In short, the argument for the proposed approach is wrong, and therefore, the evaluation is rather meaningless.  The paper introduces a simplified attention mechanism by replacing the recurrent attention model with a feed forward one. It is shown that this is sufficient for some of the pathological long term problems used to evaluate the long term capabilities of RNNs. The results are convincing in the sense that performance is greatly improved and that problems targeted by the model can be solved.  I would have liked a more elaborate discussion of the weaknesses of the approach. Clearly, cases where attention requires  While this clearly limits the power of the model, as more some patterns cannot be detected. The feed forward attention assumes that an optimal attention value can be estimated by the model without the context: this is clearly not the case for a large set of data sets, such as in NLP.  The only other major flaw, the lack of experiments that show that the method generalises to more complicated problems is not an issue for a workshop track IMHO.  Overall, I like the idea very much! I believe that this assumption holds for a huge variety of data sets, and that variations of the model can overcome it to some degree, e.g. by using a model that has a limited ""attention window"" spanning more than a single time step–e.g. using smaller RNNs or CNNs.  Therefore I would like to see the paper accepted to the workshop track so that the authors get a chance to discuss this ongoing work with other researchers. I am confident that this work can result in a method that is generally applicable to a wide range of problems.   Minors  - Wrong citation way, use \citep and \cite correctly. (e.g. ""and exploding gradient problem Pascanu et al. (2012);"" - References are used as nouns - Use “Equation (1)” instead of “Equation 1""",1,5214
"This paper proposes an alternative to a paragraph vector model, using a single vector to jointly predict a word and its ngram context.  The model works well when applied to a sentiment analysis task, slightly outperforming other approaches.  Overall, the model proposed seems reasonable although not particularly novel. However the paper in its current form suffers from a lack of analysis and motivation. For instance it is not clear to me how the predictive power of a given context causes the PV model to learn insufficient document representations (paragraph 2, Model section).    Similarly the argument that ngram features cannot be used for the Paragraph Vector model doesn't quite make sense. Clearly in that formulation it would also be possible to use ngram context (excluding the word to be predicted). For both model formulations p(word, context | vector) and p(word | context, vector) it would have been good to explore ngram features to make the comparison more complete.  I marginally lean towards accepting the paper on the understanding that this is a workshop and in the hope that the authors would extend their analysis in a subsequent publication.The authors propose a model to learn embeddings for documents that does not require supervision.  Pros: - the model is simple  - good results on the IMDB dataset  Cons: - the large number of n-grams can create sparsity issues in the presence of many rarely occurring n-grams; negative sampling can also be very noisy when choosing from such a large set of candidates. - to what extent do the document embeddings actually preserve the word order in the document? how do sentence embeddings look in this method and how do they compare with other methods such as skip-thought vectors? - lack of analysis and verification of robustness of the model on multiple different datasets  I marginally recommend the paper for acceptance to the workshop, also in view of the longer and more thorough version of the paper that is already available.",1,5215
